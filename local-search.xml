<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>一些奇怪的CSS方法</title>
    <link href="/posts/css-hacky/"/>
    <url>/posts/css-hacky/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>实习期间遇到了一些页面上的需求，虽然这些需求可能也没有那么重要，但还是希望越接近设计稿越好。毕竟设计稿的效果看着真不错，还是由衷希望这些效果也能在页面上呈现出来。计划在这里整理一些不太好实现的样式，以做记录，<del>也希望以后回来看看的时候能吐槽下这实现的是什么鬼东西</del>。</p><h1 id="布局"><a href="#布局" class="headerlink" title="布局"></a>布局</h1><h2 id="非边缘的滚动条"><a href="#非边缘的滚动条" class="headerlink" title="非边缘的滚动条"></a>非边缘的滚动条</h2><p>这里是指滚动条没有位于卡片的最边缘，而是左右两侧都有一定的空白。如果两侧的空白完全相等的话，确实看起来效果会更好。不过通常来讲，应该不会有人在意滚动条的位置。</p><p>实现也很简单（或者说过于简单）。只需要在右侧同时加上<code>margin</code>和<code>padding</code>就可以了。可以发现，在盒模型中，scrollbar是出现在border里面的。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs CSS"><span class="hljs-selector-class">.scroller-demo__container</span> &#123;<br>    <span class="hljs-attribute">margin-right</span>: <span class="hljs-number">4px</span>;<br>    <span class="hljs-attribute">padding-right</span>: <span class="hljs-number">4px</span>;<br>    <span class="hljs-attribute">overflow-y</span>: auto;<br>&#125;<br></code></pre></td></tr></table></figure><div id="scroller-demo" class="scroller-demo">    <div class="scroller-demo__container">        <div class="scroller-demo__content">            rolling girl 🎵        </div>    </div></div><style>.scroller-demo {    width: 100%;    height: 200px;    margin-bottom: 12px;    border: 2px #66CCFF solid;}.scroller-demo__container {    margin-left: 24px;    margin-right: 4px;    padding-right: 4px;    height: 100%;    overflow-y: auto;    border: 0px #F00 solid;    transition: border-width 1s;}.scroller-demo__container:hover {    border-width: 2px;}.scroller-demo__container::-webkit-scrollbar {    width: 16px;}.scroller-demo__container::-webkit-scrollbar-thumb {    background: #66CCFFAA;    border-radius: 16px;}.scroller-demo__content {    padding: 12px;    height: 1000px;    background: #66CCFF;    color: #FFF;}</style><h1 id="样式"><a href="#样式" class="headerlink" title="样式"></a>样式</h1><h2 id="毛玻璃"><a href="#毛玻璃" class="headerlink" title="毛玻璃"></a>毛玻璃</h2><p>虽说这个博客主题中header部分就有毛玻璃效果，但一直没有关注过这个的实现方式（甚至以为需要用JS），但实际上CSS就可以实现。</p><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/CSS/backdrop-filter">https://developer.mozilla.org/zh-CN/docs/Web/CSS/backdrop-filter</a></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs CSS"><span class="hljs-selector-class">.filter-demo</span> &#123;<br>    <span class="hljs-attribute">background-color</span>: <span class="hljs-built_in">rgba</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.5</span>);<br>    backdrop-<span class="hljs-attribute">filter</span>: <span class="hljs-built_in">blur</span>(<span class="hljs-number">10px</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>需要特殊注意的是，这个元素一定要有一点透明，否则这个元素就会完全挡住背景，也就完全看不到被高斯模糊的背景了。效果还是不错的，但如果有动画的话，可能会遇到一个小问题。</p><div id="filter-demo" class="filter-demo">    <div class="filter-demo__background">        <div class="filter-demo__container">            <div class="filter-demo__glasscard"></div>        </div>    </div></div><style>.filter-demo {    width: 100%;    height: 200px;    margin-bottom: 12px;}.filter-demo__background {    background-image: url("https://tackoil.github.io/img/bg/natsumi23.jpg");    background-size: cover;    height: 200px;    display: flex;    align-items: center;    justify-content: center;}.filter-demo__container {    display: flex;    align-items: center;    justify-content: center;    opacity: 100%;    border: 2px #66CCFF solid;    width: 100px;    height: 80px;    transition: opacity 1s;}.filter-demo__glasscard {    width: 90px;    height: 70px;    background-color: rgba(0, 0, 0, 0.5);    backdrop-filter: blur(10px);}.filter-demo:hover .filter-demo__container {    opacity: 0;}</style><p>可以发现当黑色方块的透明度恢复到100%时，模糊效果才出现。这是由于<code>opacity</code> 和<code>backdrop-filter</code> 不在同一个元素上导致的，所以使用时需要多留意。</p>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Repeater - 分析与设计</title>
    <link href="/posts/repeater/"/>
    <url>/posts/repeater/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>从最开始想写一个群机器人，到现在也有陆续一个月了。现在这个bot的最好记录是在一个非常水的群里复读了一周后才被发现。至于为什么想要写这样一个bot，可能也是突然意识到自己经常在群里聊天也只是复读其他的话。</p><blockquote><p>其实 Repeater 本身翻译过来是 <strong>中继器</strong> 的意思。</p></blockquote><h1 id="群聊消息分析"><a href="#群聊消息分析" class="headerlink" title="群聊消息分析"></a>群聊消息分析</h1><h2 id="复读定义"><a href="#复读定义" class="headerlink" title="复读定义"></a>复读定义</h2><div class="note note-info">            <p>🔁 由多人或单人连续发送相同内容的行为</p>          </div><ul><li>发送者：可能是同一人，也可能是多个人</li><li>发送时间：连续发送，小概率不连续发送</li><li>发送内容：完全相同的消息（<del>暂时不考虑劣质复读的情况</del>）</li></ul><h3 id="可复读消息"><a href="#可复读消息" class="headerlink" title="可复读消息"></a>可复读消息</h3><div class="note note-info">            <p>🔁 重复该消息不会被认为是“不合常理的（unreasonable）”，被认为是人类能做出的行为。</p>          </div><p>例如：？、好、yyds、😰、不亏、草、哈哈哈、笑死、开摆！、呜呜</p><p>或者更长的例子：</p><ul><li>居然不复读</li><li>一睁眼怎么又周一了</li><li>i hope🙏</li><li>群友能不能学会然后合宿做给我吃</li><li>我有[Face: 流泪][Face: 流泪][Face: 流泪][Face: 流泪][Face: 流泪]</li><li>……</li></ul><h2 id="基础数据分析"><a href="#基础数据分析" class="headerlink" title="基础数据分析"></a>基础数据分析</h2><h3 id="群消息统计"><a href="#群消息统计" class="headerlink" title="群消息统计"></a>群消息统计</h3><h4 id="群消息中连续重复的消息数量的统计"><a href="#群消息中连续重复的消息数量的统计" class="headerlink" title="群消息中连续重复的消息数量的统计"></a>群消息中连续重复的消息数量的统计</h4><p><img src="length_distribution.png" alt="群消息中连续重复的消息数量的统计"></p><h4 id="群消息的时间分布统计"><a href="#群消息的时间分布统计" class="headerlink" title="群消息的时间分布统计"></a>群消息的时间分布统计</h4><blockquote><p>时间单位是分钟，以北京时间（GMT+8）为标准</p></blockquote><p><img src="tf.png" alt="群消息的时间分布统计.png"></p><h3 id="可复读消息统计"><a href="#可复读消息统计" class="headerlink" title="可复读消息统计"></a>可复读消息统计</h3><ul><li>当前进入可复读消息集合的规则：100条消息的滑动窗口中出现不少于3次</li><li>所有进入过可复读消息集合的消息：758条</li><li>标注后的有价值的复读消息：<ul><li>Golden（在多个群出现过）10条：确实、😄、？、乐、好似喵、呜呜、好、听不懂，说人话、草、[图片]</li><li>Silver（只在一个群出现过，但成功引发过<strong>连续</strong>的复读事件）116条</li></ul></li></ul><blockquote><p>以下特征的评价标准将以上述<em>10+116=</em>126条作为分类任务的正例。</p></blockquote><h2 id="可复读消息的特征"><a href="#可复读消息的特征" class="headerlink" title="可复读消息的特征"></a>可复读消息的特征</h2><h3 id="时间滑动窗口中的频次（TF模式）"><a href="#时间滑动窗口中的频次（TF模式）" class="headerlink" title="时间滑动窗口中的频次（TF模式）"></a>时间滑动窗口中的频次（TF模式）</h3><p>使用 <strong>时间滑动窗口中的频次</strong> 对可复读消息的分布进行建模，包含以下参数：</p><ul><li>滑动窗口的时间大小：$T$</li><li>滑动窗口中的最小命中频次：$F$</li></ul><blockquote><p>由不知道哪个时间单位更好，所以分为“秒级别”与“分钟级别”</p><p>一票否决方案 表示模型如果不能召回所有的 Golden消息，则该模型的 F1 分数为0。没有该限制则称为 标准方案</p></blockquote><div class="table-container"><table><thead><tr><th>级别</th><th>方案</th><th>T</th><th>F</th><th>F1</th></tr></thead><tbody><tr><td>秒级别</td><td>一票否决</td><td>140秒</td><td>3次</td><td>0.2550</td></tr><tr><td>秒级别</td><td>标准</td><td>10秒</td><td>3次</td><td>0.3333</td></tr><tr><td>分钟级别</td><td>一票否决</td><td>600秒（10分钟）</td><td>3次</td><td>0.2214</td></tr><tr><td>分钟级别</td><td>标准</td><td>600秒（10分钟）</td><td>4次</td><td>0.2673</td></tr></tbody></table></div><p>TF模式一票否决方案下较难预测的Golden消息（8）：确实、呜呜、乐、好、😄、 听不懂，说人话、好似喵、[图片]</p><h3 id="消息滑动窗口中的频次（WF模式）"><a href="#消息滑动窗口中的频次（WF模式）" class="headerlink" title="消息滑动窗口中的频次（WF模式）"></a>消息滑动窗口中的频次（WF模式）</h3><p>不考虑消息的发送时间，使用 <strong>基于消息数量的滑动窗口中的频次</strong> 对可复读消息的分布进行建模，包含以下参数：</p><ul><li>滑动窗口的消息数量：$W$</li><li>滑动窗口中的最小命中频次：$F$</li></ul><div class="table-container"><table><thead><tr><th>方案</th><th>W</th><th>F</th><th>F1</th></tr></thead><tbody><tr><td>一票否决</td><td>80</td><td>4</td><td>0.1114</td></tr><tr><td>标准</td><td>40</td><td>23</td><td>0.2126</td></tr></tbody></table></div><ul><li>WF模式一票否决方案下较难预测的Golden消息（2）：听不懂，说人话、好似喵</li></ul><h3 id="被连续发送的次数（F模式）"><a href="#被连续发送的次数（F模式）" class="headerlink" title="被连续发送的次数（F模式）"></a>被连续发送的次数（F模式）</h3><p>使用 <strong>连续的相同消息的次数</strong> 对可复读消息的分布进行建模，包含以下参数：</p><ul><li>连续发送的最小次数：$F$</li></ul><div class="table-container"><table><thead><tr><th>方案</th><th>F</th><th>F1</th></tr></thead><tbody><tr><td>一票否决</td><td>-</td><td>0</td></tr><tr><td>标准</td><td>3</td><td>0.5508</td></tr></tbody></table></div><p><img src="repeat_send_heat_map_f1.png" alt="连续发送次数"></p><h3 id="消息文字长度"><a href="#消息文字长度" class="headerlink" title="消息文字长度"></a>消息文字长度</h3><p>把图片、转发、at等特殊消息视为长度为1的消息。统计可复读消息中占所有消息的比例。</p><p><img src="message_length_prob.png" alt="消息文字长度分布"></p><p>可以发现好像并没有什么严格的规律，概率都不到0.5%</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ol><li>绝大部分可复读消息都在所有消息中连续多次出现。</li><li>基于时间的滑动窗口方案（TF模式）比基于数量的滑动窗口方案（WF模式）性能更高，但两方案相比连续发送次数方案（F模式）存在 <em>准确率</em> 较低的情况。</li><li>无法通过消息文字长度预测是否为可复读消息。</li></ol><h2 id="复读行为的特征"><a href="#复读行为的特征" class="headerlink" title="复读行为的特征"></a>复读行为的特征</h2><h3 id="复读是否发生的概率"><a href="#复读是否发生的概率" class="headerlink" title="复读是否发生的概率"></a>复读是否发生的概率</h3><h4 id="基础概率复读（p模式）"><a href="#基础概率复读（p模式）" class="headerlink" title="基础概率复读（p模式）"></a>基础概率复读（p模式）</h4><p>设消息列表中 一个可复读消息 后的消息是 与前一个相同的消息 的概率为 $p=0.1198$ </p><p>随机执行测试（10000次）结果如下：</p><div class="table-container"><table><thead><tr><th>最小值</th><th>最大值</th><th>平均值</th><th>中位数</th></tr></thead><tbody><tr><td>0.7105</td><td>0.7443</td><td>0.7281</td><td>0.7281</td></tr></tbody></table></div><h4 id="指数累进概率复读（pd模式）"><a href="#指数累进概率复读（pd模式）" class="headerlink" title="指数累进概率复读（pd模式）"></a>指数累进概率复读（pd模式）</h4><p>设消息列表中 一个可复读消息 后的第一个消息是同一个可复读消息的概率为 $p$。前$k-1$个消息都是同一个可复读消息的情况下，第$k$个消息是同一个可复读消息的概率为$p \cdot d^{k-1}$。先统计在不同的$k$ 下的概率情况。</p><div class="table-container"><table><thead><tr><th>k</th><th>条件概率</th><th>k</th><th>条件概率</th><th>k</th><th>条件概率</th></tr></thead><tbody><tr><td>1</td><td>0.0737</td><td>5</td><td>0.5965</td><td>9</td><td>0.5556</td></tr><tr><td>2</td><td>0.4276</td><td>6</td><td>0.4706</td><td>10</td><td>0.6</td></tr><tr><td>3</td><td>0.4436</td><td>7</td><td>0.6875</td><td>11</td><td>0.6667</td></tr><tr><td>4</td><td>0.5</td><td>8</td><td>0.8182</td><td>12</td><td>1</td></tr></tbody></table></div><p>用指数函数拟合这一概率，得到</p><script type="math/tex; mode=display">p=0.3430,\ d=1.0888</script><p>随机执行测试（10000次）结果如下：</p><div class="table-container"><table><thead><tr><th>最小值</th><th>0.6021</th></tr></thead><tbody><tr><td>最大值</td><td>0.6397</td></tr><tr><td>平均值</td><td>0.6195</td></tr><tr><td>中位数</td><td>0.6195</td></tr></tbody></table></div><p><img src="opti.png" alt="指数累进概率拟合"></p><h4 id="负指数累进复读模式（q模式）"><a href="#负指数累进复读模式（q模式）" class="headerlink" title="负指数累进复读模式（q模式）"></a>负指数累进复读模式（q模式）</h4><p>设消息列表中 一个可复读消息 后的第一个消息是同一个可复读消息的概率为 $1-q$。前$k-1$个消息都是同一个可复读消息的情况下，第$k$个消息是同一个可复读消息的概率为$1-q^k$。拟合这一概率，得到</p><script type="math/tex; mode=display">q=0.8624</script><p>随机执行测试（10000次）结果如下：</p><div class="table-container"><table><thead><tr><th>最小值</th><th>0.7595</th></tr></thead><tbody><tr><td>最大值</td><td>0.7904</td></tr><tr><td>平均值</td><td>0.7751</td></tr><tr><td>中位数</td><td>0.7751</td></tr></tbody></table></div><p><img src="opti_p.png" alt="负指数累进概率拟合"></p><h3 id="复读间隔时间的分布"><a href="#复读间隔时间的分布" class="headerlink" title="复读间隔时间的分布"></a>复读间隔时间的分布</h3><p>TBD</p><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><ol><li>负指数累进复读模式（q模式）可能更加符合实际复读行为</li><li>不同的累进复读模式的效果差距很大，可能存在更好的累进复读方案。</li><li>基础概率复读的阈值应较低</li></ol><h2 id="进一步分析计划"><a href="#进一步分析计划" class="headerlink" title="进一步分析计划"></a>进一步分析计划</h2><ol><li>目前的分析都是从实验的角度进行的。不知是否可以进一步使用统计学知识从建模的角度进行分析。</li><li>现有方案对可复读消息的判断性能较低。</li></ol><h1 id="Roadmap"><a href="#Roadmap" class="headerlink" title="Roadmap"></a>Roadmap</h1><ul><li>[x]  基础概率复读</li><li>[x]  冷却时间</li><li>[x]  引入Mongo</li><li>[x]  配置支持</li><li>[x]  多群支持（可发布）</li><li>[x]  支持图片复读</li><li>[x]  关键词排除</li><li>[x]  累进复读概率</li><li>[ ]  随机丢弃已保存消息（idea by 719）</li><li>[ ]  多人复读判断引入</li></ul>]]></content>
    
    
    <categories>
      
      <category>吐槽</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>负外边距与非语义三栏布局</title>
    <link href="/posts/three-columns/"/>
    <url>/posts/three-columns/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><script src="https://unpkg.com/vue@next"></script><link rel="stylesheet" href="https://unpkg.com/element-plus/dist/index.css" /><script src="https://unpkg.com/element-plus"></script><style>    :root {    --el-color-primary: #3ea09e;    }    @media (min-width: 1530px) {        .show {            display: flex;        }        .result {            width: 300px;            margin: 0 16px;        }        .controller {            flex: 1;            margin: 0 8px;        }    }    @media (max-width: 1530px){        .show {            width: 100%;        }        .result {            width: 100%;            margin-bottom: 12px;        }        .controller {            width: 100%;            margin-top: 12px;        }    }    @media (min-width: 425px){        .controller-item {            margin: 16px 4px;            flex-direction: row;            align-items: center;        }        .controller-item .controller-label {            flex: 1;        }        .controller-item .elcom {            flex: 2;            margin-left: 12px;        }    }    @media (max-width: 425px){        .controller-item {            margin: 24px 16px;            flex-direction: column;            align-items: left;        }        .controller-item .controller-label {            flex: 1;            font-size: 12px;        }        .controller-item .elcom {            flex: 2;            margin-top: 8px;            margin-left: 0;        }    }    .result {        box-shadow: 0 0 5px 5px rgba(0, 0, 0, 0.2);    }    .controller-item {        display: flex;    }</style><h1 id="圣杯布局"><a href="#圣杯布局" class="headerlink" title="圣杯布局"></a>圣杯布局</h1><div id="holy-grail" class="playground">    <div class="show">        <div class="result">            <div style="height: 300px" :style="hgContainer">                <div style="background-color: #F5FAF0DD; height: 150px; text-align: center; " :style="hgCenter"></div>                <div style="background-color: #6DAFA4DD; height: 80px; width: 50px; text-align: center;" :style="hgLeft"></div>                <div style="background-color: #276562DD; height: 80px; width: 100px; text-align: center; " :style="hgRight"></div>            </div>        </div>                <div class="controller">            <div class="controller-item">                <div class="controller-label"> container:padding-left </div>                <el-slider class="elcom" size="small" v-model="hgContainerRaw.paddingLeft" :min="0" :max="200" :marks="{50: '50px'}" ></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> container:padding-right </div>                <el-slider class="elcom" size="small" v-model="hgContainerRaw.paddingRight" :min="0" :max="200" :marks="{100: '100px'}"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> center:float </div>                <div class="elcom">                    <el-radio-group v-model="hgCenterRaw.float" size="small">                        <el-radio-button label="left" >left</el-radio-button>                        <el-radio-button label="right">right</el-radio-button>                        <el-radio-button label="none">none</el-radio-button>                    </el-radio-group>                </div>            </div>            <div class="controller-item">                <div class="controller-label"> center:width </div>                <el-slider class="elcom" size="small" v-model="hgCenterRaw.width" :min="0" :max="100" :marks="{100: '100%'}"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> left:float </div>                <div class="elcom">                    <el-radio-group v-model="hgLeftRaw.float" size="small">                        <el-radio-button label="left" >left</el-radio-button>                        <el-radio-button label="right">right</el-radio-button>                        <el-radio-button label="none">none</el-radio-button>                    </el-radio-group>                </div>            </div>            <div class="controller-item">                <div class="controller-label"> left:position </div>                <div class="elcom">                    <el-radio-group v-model="hgLeftRaw.position" size="small">                        <el-radio-button label="static" >static</el-radio-button>                        <el-radio-button label="relative">relative</el-radio-button>                        <el-radio-button label="absolute">absolute</el-radio-button>                        <el-radio-button label="fixed">fixed</el-radio-button>                    </el-radio-group>                </div>            </div>            <div class="controller-item">                <div class="controller-label"> left:margin-left </div>                <el-slider class="elcom" size="small" v-model="hgLeftRaw.marginLeft" :min="-100" :max="100" :marks="lmlmarks"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> left:right </div>                <el-slider class="elcom" size="small" v-model="hgLeftRaw.right" :min="0" :max="200" :marks="{50: '50px'}"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> right:float </div>                <div class="elcom">                    <el-radio-group v-model="hgRightRaw.float" size="small">                        <el-radio-button label="left" >left</el-radio-button>                        <el-radio-button label="right">right</el-radio-button>                        <el-radio-button label="none">none</el-radio-button>                    </el-radio-group>                </div>            </div>            <div class="controller-item">                <div class="controller-label"> right:position </div>                <div class="elcom">                    <el-radio-group v-model="hgRightRaw.position" size="small">                        <el-radio-button label="static" >static</el-radio-button>                        <el-radio-button label="relative">relative</el-radio-button>                        <el-radio-button label="absolute">absolute</el-radio-button>                        <el-radio-button label="fixed">fixed</el-radio-button>                    </el-radio-group>                </div>            </div>            <div class="controller-item">                <div class="controller-label"> right:left </div>                <el-slider class="elcom" size="small" v-model="hgRightRaw.left" :min="0" :max="200" :marks="{100: '100px'}"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> right:margin-left </div>                <el-slider class="elcom" size="small" v-model="hgRightRaw.marginLeft" :min="-100" :max="200" :marks="rmrmarks"></el-slider>            </div>        </div>        </div></div><script>    const HolyGrailDemo = {        data() {            const lmlmarks = {};            lmlmarks[-100] = '-100%';            const rmrmarks = {};            rmrmarks[-100] = '-100px';            return {                lmlmarks,                rmrmarks,                hgContainerRaw: {                    paddingLeft: 50,                    paddingRight: 100,                },                hgCenterRaw: {                    float: 'left',                    width: 100,                },                hgLeftRaw: {                    float: 'left',                    marginLeft: -100,                    position: 'relative',                    right: 50,                },                hgRightRaw: {                    float: 'left',                    marginLeft: -100,                    position: 'relative',                    left: 100,                }            }        },        computed: {            hgContainer(){                return {                    paddingLeft: `${this.hgContainerRaw.paddingLeft}px`,                    paddingRight: `${this.hgContainerRaw.paddingRight}px`                }            },            hgCenter(){                return {                    float: this.hgCenterRaw.float,                    width: `${this.hgCenterRaw.width}%`                }            },            hgLeft(){                return {                    float: this.hgLeftRaw.float,                    position: this.hgLeftRaw.position,                    marginLeft: `${this.hgLeftRaw.marginLeft}%`,                    right: `${this.hgLeftRaw.right}px`,                }            },            hgRight(){                return {                    float: this.hgRightRaw.float,                    marginLeft: `${this.hgRightRaw.marginLeft}px`,                    position: this.hgRightRaw.position,                    left: `${this.hgRightRaw.left}px`                }            }        }    }    const app = Vue.createApp(HolyGrailDemo);    app.use(ElementPlus);    app.mount('#holy-grail');</script><h1 id="双飞翼布局"><a href="#双飞翼布局" class="headerlink" title="双飞翼布局"></a>双飞翼布局</h1><div id="flying-wing" class="playground">    <div class="show">        <div class="result">            <div style="min-height: 300px" :style="fwContainer">                <div style="background-color: #F5FAF0DD; height: 150px;" :style="fwCenter">                    <div style="background-color: #DDDDDDDD; height: 120px; text-align: center;" :style="fwCenterInner">                    </div>                </div>                <div style="background-color: #6DAFA4DD; height: 80px; width: 50px; text-align: center;" :style="fwLeft"></div>                <div style="background-color: #276562DD; height: 80px; width: 100px; text-align: center;" :style="fwRight"></div>            </div>        </div>                <div class="controller">            <div class="controller-item">                <div class="controller-label"> center:float </div>                <div class="elcom">                    <el-radio-group v-model="fwCenterRaw.float" size="small">                        <el-radio-button label="left" >left</el-radio-button>                        <el-radio-button label="right">right</el-radio-button>                        <el-radio-button label="none">none</el-radio-button>                    </el-radio-group>                </div>            </div>            <div class="controller-item">                <div class="controller-label"> center:width </div>                <el-slider class="elcom" size="small" v-model="fwCenterRaw.width" :min="0" :max="100" :marks="{100: '100%'}"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> center-inner:margin-left </div>                <el-slider class="elcom" size="small" v-model="fwCenterInnerRaw.marginLeft" :min="-100" :max="100" :marks="{50: '50px'}"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> center-inner:margin-right </div>                <el-slider class="elcom" size="small" v-model="fwCenterInnerRaw.marginRight" :min="-100" :max="200" :marks="{100 : '100px'}"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> left:float </div>                <div class="elcom">                    <el-radio-group v-model="fwLeftRaw.float" size="small">                        <el-radio-button label="left" >left</el-radio-button>                        <el-radio-button label="right">right</el-radio-button>                        <el-radio-button label="none">none</el-radio-button>                    </el-radio-group>                </div>            </div>            <div class="controller-item">                <div class="controller-label"> left:margin-left </div>                <el-slider class="elcom" size="small" v-model="fwLeftRaw.marginLeft" :min="-100" :max="100" :marks="lmlmarks"></el-slider>            </div>            <div class="controller-item">                <div class="controller-label"> right:float </div>                <div class="elcom">                    <el-radio-group v-model="fwRightRaw.float" size="small">                        <el-radio-button label="left" >left</el-radio-button>                        <el-radio-button label="right">right</el-radio-button>                        <el-radio-button label="none">none</el-radio-button>                    </el-radio-group>                </div>            </div>            <div class="controller-item">                <div class="controller-label"> right:margin-left </div>                <el-slider class="elcom" size="small" v-model="fwRightRaw.marginLeft" :min="-100" :max="200" :marks="rmrmarks"></el-slider>            </div>        </div>        </div></div><script>     const FlyingWingDemo = {        data() {            const lmlmarks = {};            lmlmarks[-100] = '-100%';            const rmrmarks = {};            rmrmarks[-100] = '-100px';            return {                lmlmarks,                rmrmarks,                fwContainer: {},                fwCenterRaw: {                    float: 'left',                    width: 100,                },                fwCenterInnerRaw: {                    marginLeft: 50,                    marginRight: 100                },                fwLeftRaw: {                    float: 'left',                    marginLeft: -100,                },                fwRightRaw: {                    float: 'left',                    marginLeft: -100,                }            }        },        computed: {            fwCenter(){                return {                    float: this.fwCenterRaw.float,                    width: `${this.fwCenterRaw.width}%`                }            },            fwCenterInner(){                return {                    marginLeft: `${this.fwCenterInnerRaw.marginLeft}px`,                    marginRight: `${this.fwCenterInnerRaw.marginRight}px`                }            },            fwLeft(){                return {                    float: this.fwLeftRaw.float,                    marginLeft: `${this.fwLeftRaw.marginLeft}%`,                }            },            fwRight(){                return {                    float: this.fwRightRaw.float,                    marginLeft: `${this.fwRightRaw.marginLeft}px`                }            }        }    }    const app1 = Vue.createApp(FlyingWingDemo);    app1.use(ElementPlus);    app1.mount('#flying-wing');</script>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
      <category>CSS</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Axios 中关于 Promise 的使用与理解</title>
    <link href="/posts/axios-promise/"/>
    <url>/posts/axios-promise/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><script src="https://unpkg.com/axios/dist/axios.min.js"></script><p>最近在看了 Axios 的源码。在已有 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/XMLHttpRequest"><code>XMLHttpRequest</code></a> 的情况下，Axios 做了哪些改进吸引大家使用？抱着这样的想法尝试理解与阅读 Axios 仓库。</p><p><a href="https://github.com/axios/axios">Link: axios/axios</a><br><img src="https://opengraph.githubassets.com/ddb54613cedd4ca9c7c32776111dc36f7622b78611708ca86fa850b692919aec/axios/axios" alt=""></p><span id="more"></span><h1 id="Axios-的基本组成"><a href="#Axios-的基本组成" class="headerlink" title="Axios 的基本组成"></a>Axios 的基本组成</h1><blockquote><p>这里的<strong>基本组成</strong>是指 <code>lib</code> 文件夹下的内容。其他文件夹主要是用来存放示例文件、测试文件、开源相关说明文件等。</p></blockquote><p>首先，观察目录结构。每个模块的作用可以通过文件夹的名字了解个大概。整个结构也非常整洁，还是很适合初学者（比如笔者）阅读的。</p><ul><li><code>adapters/</code> 为提供 HTTP 请求的适配器。用以支持 node.js 与 浏览器发送 HTTP 请求。</li><li><code>cancel/</code> 负责实现请求取消的功能。包含了将要废除的 Cancel Token 模式的实现。</li><li><code>core/</code> 为核心内容，主要负责加载配置文件、处理拦截器、发送请求等功能。</li><li><code>defaults/</code> 存放一些默认的配置文件。</li><li><code>env/</code> 自动生成，主要用来标识版本。</li><li><code>helpers/</code> 各种各样的帮助文件，显然里面有一些功能已经成为 JavaScript 的内置功能了，交给 Babel 也未尝不可。</li><li><code>axios.js</code> 入口文件。除了暴露 API 以外，还有一小部分逻辑用来实现非实例化直接使用的功能。</li><li><code>utils.js</code> 一些小工具函数。感觉也有不少其实可以直接调用 JavaScript 的内置函数。主要还是减少工作环境的影响。</li></ul><p>Axios 既然被称之为<strong>基于Promise的网络请求库</strong>，其对于 JavaScript 中 <code>Promise</code> 的理解也应该是非常透彻的。本文主要针对笔者所认为两处对 <code>Promise</code> 特性运用较为充分的两部分进行介绍。</p><p>在开始下文之前，还需要介绍一下 Axios 是通过 <code>config</code> 控制的。这个 <code>config</code> 除了 <code>url</code> 以外，还包含 <code>headers</code>、<code>proxy</code> 等其他配置。可以认为 Axios 实例如果传入相同的 <code>config</code>，就会执行完全一致的操作。</p><h1 id="Axios-中关于拦截器的实现"><a href="#Axios-中关于拦截器的实现" class="headerlink" title="Axios 中关于拦截器的实现"></a>Axios 中关于拦截器的实现</h1><h2 id="Axios-中的拦截器"><a href="#Axios-中的拦截器" class="headerlink" title="Axios 中的拦截器"></a>Axios 中的拦截器</h2><p>拦截器的作用是在<strong>发送请求前</strong>与<strong>收到响应后</strong>分别对请求与响应进行处理，从而实现一些便捷操作。例如 JWT 权限验证、响应数据的分析等。拦截器使用如下方式定义：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-comment">// 添加请求拦截器</span><br>axios.<span class="hljs-property">interceptors</span>.<span class="hljs-property">request</span>.<span class="hljs-title function_">use</span>(<span class="hljs-keyword">function</span> (<span class="hljs-params">config</span>) &#123;<br>    <span class="hljs-comment">// 在发送请求之前做些什么</span><br>    <span class="hljs-keyword">return</span> config; <span class="hljs-comment">// 一定要返回 config，很重要</span><br>  &#125;, <span class="hljs-keyword">function</span> (<span class="hljs-params">error</span>) &#123;<br>    <span class="hljs-comment">// 对请求错误做些什么</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-title class_">Promise</span>.<span class="hljs-title function_">reject</span>(error);<br>  &#125;);<br><br><span class="hljs-comment">// 添加响应拦截器</span><br>axios.<span class="hljs-property">interceptors</span>.<span class="hljs-property">response</span>.<span class="hljs-title function_">use</span>(<span class="hljs-keyword">function</span> (<span class="hljs-params">response</span>) &#123;<br>    <span class="hljs-comment">// 2xx 范围内的状态码都会触发该函数。</span><br>    <span class="hljs-comment">// 对响应数据做点什么</span><br>    <span class="hljs-keyword">return</span> response; <span class="hljs-comment">// 一定要返回 response，很重要</span><br>  &#125;, <span class="hljs-keyword">function</span> (<span class="hljs-params">error</span>) &#123;<br>    <span class="hljs-comment">// 超出 2xx 范围的状态码都会触发该函数。</span><br>    <span class="hljs-comment">// 对响应错误做点什么</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-title class_">Promise</span>.<span class="hljs-title function_">reject</span>(error);<br>  &#125;);<br></code></pre></td></tr></table></figure><p>对照 <code>core/InterceptorManager.js</code>，可以发现还包含了一个 <code>options</code> 选项，为拦截器提供特殊功能。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-title class_">InterceptorManager</span>.<span class="hljs-property"><span class="hljs-keyword">prototype</span></span>.<span class="hljs-property">use</span> = <span class="hljs-keyword">function</span> <span class="hljs-title function_">use</span>(<span class="hljs-params">fulfilled, rejected, options</span>) &#123;<br>  <span class="hljs-variable language_">this</span>.<span class="hljs-property">handlers</span>.<span class="hljs-title function_">push</span>(&#123; <span class="hljs-comment">// 存放拦截器的数组</span><br>    <span class="hljs-attr">fulfilled</span>: fulfilled,<br>    <span class="hljs-attr">rejected</span>: rejected,<br>    <span class="hljs-attr">synchronous</span>: options ? options.<span class="hljs-property">synchronous</span> : <span class="hljs-literal">false</span>,<br>    <span class="hljs-attr">runWhen</span>: options ? options.<span class="hljs-property">runWhen</span> : <span class="hljs-literal">null</span><br>  &#125;);<br>  <span class="hljs-keyword">return</span> <span class="hljs-variable language_">this</span>.<span class="hljs-property">handlers</span>.<span class="hljs-property">length</span> - <span class="hljs-number">1</span>; <span class="hljs-comment">// 返回 index 以进行拦截器的移除</span><br>&#125;;<br></code></pre></td></tr></table></figure><p>了解了这些，就可以仔细看看 Axios 是如何实现拦截器的。</p><p>首先，对请求拦截器与响应拦截器进行处理。判断是否包含异步拦截器、跳过部分拦截器。主要注意的是，请求拦截器中后绑定的放在队列的前面（<code>unshift</code>），响应拦截器则相反（<code>push</code>）。其中，Axios 默认所有请求拦截器都是<strong>异步的</strong>，即<code>interceptor.synchronous = false;</code>。</p><p>拦截器列表由偶数个函数构成，每<strong>两个函数</strong>一组。分别表示一个拦截器的执行函数与错误处理函数。类似于 <a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/then"><code>Promise.prototype.then()</code></a> 的入参。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-comment">// core/Axios.js</span><br>  <span class="hljs-comment">// filter out skipped interceptors</span><br>  <span class="hljs-keyword">var</span> requestInterceptorChain = [];<br>  <span class="hljs-keyword">var</span> synchronousRequestInterceptors = <span class="hljs-literal">true</span>;<br>  <span class="hljs-variable language_">this</span>.<span class="hljs-property">interceptors</span>.<span class="hljs-property">request</span>.<span class="hljs-title function_">forEach</span>(<span class="hljs-keyword">function</span> <span class="hljs-title function_">unshiftRequestInterceptors</span>(<span class="hljs-params">interceptor</span>) &#123;<br>    <span class="hljs-keyword">if</span> (<span class="hljs-keyword">typeof</span> interceptor.<span class="hljs-property">runWhen</span> === <span class="hljs-string">&#x27;function&#x27;</span> &amp;&amp; interceptor.<span class="hljs-title function_">runWhen</span>(config) === <span class="hljs-literal">false</span>) &#123; <span class="hljs-comment">// 过滤</span><br>      <span class="hljs-keyword">return</span>;<br>    &#125;<br><br>    synchronousRequestInterceptors = synchronousRequestInterceptors &amp;&amp; interceptor.<span class="hljs-property">synchronous</span>; <span class="hljs-comment">// 判断是否都为同步</span><br><br>    requestInterceptorChain.<span class="hljs-title function_">unshift</span>(interceptor.<span class="hljs-property">fulfilled</span>, interceptor.<span class="hljs-property">rejected</span>); <span class="hljs-comment">// 注意顺序：unshift</span><br>  &#125;);<br><br>  <span class="hljs-keyword">var</span> responseInterceptorChain = [];<br>  <span class="hljs-variable language_">this</span>.<span class="hljs-property">interceptors</span>.<span class="hljs-property">response</span>.<span class="hljs-title function_">forEach</span>(<span class="hljs-keyword">function</span> <span class="hljs-title function_">pushResponseInterceptors</span>(<span class="hljs-params">interceptor</span>) &#123;<br>    responseInterceptorChain.<span class="hljs-title function_">push</span>(interceptor.<span class="hljs-property">fulfilled</span>, interceptor.<span class="hljs-property">rejected</span>); <span class="hljs-comment">// 注意顺序：push</span><br>  &#125;);<br></code></pre></td></tr></table></figure><p>之后，Axios 针对异步请求拦截器与同步请求拦截器分情况进行了处理。其中如果所有请求拦截器都是同步的，则循环执行更新 <code>config</code> 即可。下面介绍包含异步的拦截器链进行介绍。</p><ol><li>把主请求（指除去拦截器以外的请求）与一个<strong>空元素</strong>放在列表中间。然后将请求拦截器列表放在前面，把响应拦截器列表放在后面。</li><li>调用 <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/resolve"><code>Promise.resolve()</code></a> 函数，生成一个 <code>Promise</code> 对象。</li><li>将拦截器列表中的元素两个一组，放入 <a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/then"><code>then</code></a> 中，并将返回的新 <code>Promise</code> 替代原始的 <code>Promise</code>。</li><li>返回最终得到的 <code>Promise</code> 链。</li></ol><p>将上述 Promise 链具体化，如下图所示。</p><p><img src="./promise-chain.jpg" alt="Promise 链"></p><blockquote><p>您可能发现了，这里面<strong>请求拦截器</strong>的 <code>onRejected</code> 好像有些诡异。是的，笔者也是写到这里才发现这个问题，现已提交 <a href="https://github.com/axios/axios/issues/4537">issue</a>。</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-comment">// core/Axios.js</span><br>  <span class="hljs-keyword">if</span> (!synchronousRequestInterceptors) &#123;<br>    <span class="hljs-keyword">var</span> chain = [dispatchRequest, <span class="hljs-literal">undefined</span>];<br><br>    <span class="hljs-title class_">Array</span>.<span class="hljs-property"><span class="hljs-keyword">prototype</span></span>.<span class="hljs-property">unshift</span>.<span class="hljs-title function_">apply</span>(chain, requestInterceptorChain);<br>    chain = chain.<span class="hljs-title function_">concat</span>(responseInterceptorChain);<br><br>    promise = <span class="hljs-title class_">Promise</span>.<span class="hljs-title function_">resolve</span>(config);<br>    <span class="hljs-keyword">while</span> (chain.<span class="hljs-property">length</span>) &#123;<br>      promise = promise.<span class="hljs-title function_">then</span>(chain.<span class="hljs-title function_">shift</span>(), chain.<span class="hljs-title function_">shift</span>());<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> promise;<br>  &#125;<br></code></pre></td></tr></table></figure><p>Promise 链中所有在主请求前的 Promise 传递修改 config，在主请求后的 Promise 传递并修改 response。这样就完成了对请求和响应的拦截。那么 <code>Promise.resolve()</code> 与 <code>Promise.prototype.then()</code> 是如何实现这个可以把同步请求也包装进 Promise 链的功能的？</p><h2 id="Promise-resolve"><a href="#Promise-resolve" class="headerlink" title="Promise.resolve()"></a>Promise.resolve()</h2><p>参考 MDN 中关于 <code>Promise.resolve()</code> 的描述，可以发现其作用就是返回一个新的 <code>Promise</code>。但根据入参的类型不同行为有一定区别。</p><ol><li>如果是另一个 <code>Promise</code>，则返回这个 <code>Promise</code></li><li>如果是一个含有 <code>then</code> 方法的对象，则会尝试执行该 <code>then</code> 方法，根据执行结果返回对应状态的 <code>Promise</code>。（类似于<code>new Promise(then)</code>）<strong>这里会递归执行所有嵌套的 <code>then</code>，直到 <code>fuilfilled</code> 或者 <code>rejected</code></strong>。</li><li>其他情况则返回一个新的 <code>Promise</code> ，并且这个 <code>Promise</code> 将处于 <code>fulfilled</code> 状态，<code>fulfilled</code> 值为入参。</li></ol><p>可以发现这个函数非常适合将一个对象包装成 Promise，并作为 Promise 链的开头使用。</p><p>如果对上述描述还有困惑的话，可以参考下面的简易 Polyfill。<del>（不过都有 <code>Promise</code> 了，还能没有 <code>Promise.resolve()</code> 么）</del></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-title class_">Promise</span>.<span class="hljs-property">resolve</span> = <span class="hljs-keyword">function</span>(<span class="hljs-params">value</span>)&#123;<br>  <span class="hljs-keyword">if</span>(value <span class="hljs-keyword">instanceof</span> <span class="hljs-title class_">Promise</span>)&#123;<br>    <span class="hljs-keyword">return</span> value; <span class="hljs-comment">// 情况1，入参为 Promise，则直接返回该 Promise</span><br>  &#125;<br>  <span class="hljs-keyword">if</span>(value &amp;&amp; <span class="hljs-keyword">typeof</span> value.<span class="hljs-property">then</span> === <span class="hljs-string">&#x27;function&#x27;</span>)&#123;<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Promise</span>(value.<span class="hljs-property">then</span>); <span class="hljs-comment">// 情况2，入参对象含有 then 方法，则执行该 then 方法</span><br>  &#125;<br>  <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Promise</span>(<span class="hljs-keyword">function</span>(<span class="hljs-params">resolve</span>)&#123;<br>    <span class="hljs-title function_">resolve</span>(value); <span class="hljs-comment">// 情况3，返回 fulfilled 的 Promise</span><br>  &#125;)<br>&#125;<br></code></pre></td></tr></table></figure><p>对于这种含有 <code>then</code> 方法的对象，可以称之为 <code>thenable</code>。可以认为是一个 <strong>弱化版</strong> 的 <code>Promise</code>，但没有JavaScript解释器的 <strong>微任务</strong> 的保障。为了加深对 <code>thenable</code> 的印象，来看下面这个例子。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">var</span> p_thenable = &#123;<br>  <span class="hljs-attr">then</span>: <span class="hljs-keyword">function</span>(<span class="hljs-params">resolve_outter</span>)&#123;<br>    <span class="hljs-title function_">resolve_outter</span>(&#123;<br>      <span class="hljs-attr">then</span>: <span class="hljs-keyword">function</span>(<span class="hljs-params">resolve_middle</span>)&#123;<br>        <span class="hljs-title function_">resolve_middle</span>(&#123;<br>          <span class="hljs-attr">then</span>: <span class="hljs-keyword">function</span>(<span class="hljs-params">resolve_inner</span>)&#123;<br>            <span class="hljs-comment">// throw Error(&quot;Rejected in Inner thenable.&quot;);</span><br>            <span class="hljs-title function_">resolve_inner</span>(<span class="hljs-string">&quot;Fulfilled in Inner thenable.&quot;</span>);<br>          &#125;<br>        &#125;);<br>      &#125;<br>    &#125;)<br>  &#125;<br>&#125;<br><span class="hljs-keyword">var</span> p_promise = <span class="hljs-title class_">Promise</span>.<span class="hljs-title function_">resolve</span>(p_thenable);<br>p_promise.<span class="hljs-title function_">then</span>(<span class="hljs-keyword">function</span> <span class="hljs-title function_">onFulfilled</span>(<span class="hljs-params">value</span>) &#123;<br>  <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-string">&quot;FULFILLED with:&quot;</span>, value);<br>&#125;, <span class="hljs-keyword">function</span> <span class="hljs-title function_">onRejected</span>(<span class="hljs-params">err</span>)&#123;<br>  <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-string">&quot;REJECTED with:&quot;</span>, err);<br>&#125;)<br></code></pre></td></tr></table></figure><p>在注释和取消注释 <code>throw</code> 语句之后，console 中分别输出以下内容。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">FULFILLED <span class="hljs-keyword">with</span>: Fulfilled <span class="hljs-keyword">in</span> <span class="hljs-keyword">Inner</span> thenable.<br>REJECTED <span class="hljs-keyword">with</span>: Error: Rejected <span class="hljs-keyword">in</span> <span class="hljs-keyword">Inner</span> thenable.<br></code></pre></td></tr></table></figure><p>如果根据上文给出的 Polyfill 理解 <code>Promise.resolve()</code> 的话，输出应该都是 <code>FULFILLED with: &#123;then: function ...&#125;</code>，因为只调用了两次 <code>then</code>，第三个对象应该会作为 <code>fulfilled</code> 的值传给 <code>value</code>。这就说明 <code>Promise.resolve()</code> 会尝试解析 <code>thenable</code> 的最终结果，并把最终结果包装成 <code>Promise</code> 进行返回。</p><p>因此如果使用 <code>thenable</code> 特性的时候，<strong>不要将自身作为 <code>fulfilled</code> 的值</strong>，否则解释器会死循环。（不过应该没有多人想用 <code>thenable</code> 吧。）</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">let</span> thenable = &#123;<br>  <span class="hljs-attr">then</span>: <span class="hljs-function">(<span class="hljs-params">resolve, reject</span>) =&gt;</span> &#123;<br>    <span class="hljs-title function_">resolve</span>(thenable)<br>  &#125;<br>&#125;<br><span class="hljs-title class_">Promise</span>.<span class="hljs-title function_">resolve</span>(thenable)  <span class="hljs-comment">// Will lead to infinite recursion.</span><br></code></pre></td></tr></table></figure><blockquote><p>PS: 其实尝试执行的话，发现上面 <code>Promise.resolve()</code> 的 Polyfill 也是正确的。可能是因为 <code>Promise</code> 构造函数在入参（执行器）执行过程中调用的 <code>resolve()</code> 与 <code>Promise.resolve()</code> 的功能相同。</p></blockquote><h2 id="Promise-prototype-then"><a href="#Promise-prototype-then" class="headerlink" title="Promise.prototype.then()"></a>Promise.prototype.then()</h2><p>如果使用过 Axios 的话，那对于 <code>Promise.prototype.then()</code> 应该不陌生了。其作为构建 <code>Promise</code> 链的胶水，传入两个函数分别处理<strong>前一个</strong> <code>Promise</code> 处于 <code>fulfilled</code> 或者 <code>rejected</code> 状态下的执行内容。然而正因为<code>then</code> <strong>返回值</strong>的特性，才使得其可以作为构建 <code>Promise</code> 链的胶水使用。</p><blockquote><p>下文中的执行函数指的是 <code>onFulfilled</code> 或 <code>onRejected</code>。</p></blockquote><ol><li>当执行函数返回一个值，则 <code>then</code> 函数返回一个 <code>fulfilled</code> 状态的 <code>Promise</code>，<code>fulfilled</code> 值为执行函数的返回值</li><li>当执行函数不返回任何值，则 <code>then</code> 函数返回一个 <code>fulfilled</code> 状态的 <code>Promise</code>, <code>fulfilled</code> 值为 <code>undefined</code></li><li>当执行函数抛出一个错误，则 <code>then</code> 函数返回一个 <code>rejected</code> 状态的 <code>Promise</code>，<code>rejected</code> 值为该错误。</li><li>当执行函数返回一个  <code>Promise</code>，则 <code>then</code> 函数返回该 <code>Promise</code>。</li></ol><p>可以发现 <code>Promise.prototype.then()</code> 也有包装非 <code>Promise</code> 的作用。那么 <code>Promise.prototype.then()</code> 会和 <code>Promise.resolve()</code> 一样尝试处理 <code>thenable</code> 么？答案是会的。就不在下文举例说明啦。 </p><h1 id="Axios-中关于取消的实现"><a href="#Axios-中关于取消的实现" class="headerlink" title="Axios 中关于取消的实现"></a>Axios 中关于取消的实现</h1><h2 id="Axios-中的-CancelToken"><a href="#Axios-中的-CancelToken" class="headerlink" title="Axios 中的 CancelToken"></a>Axios 中的 CancelToken</h2><p>其实当本文撰写过程中，Axios 已经声明将 <code>CancelToken</code> 废弃（Deprecated）。不过源码中还包含这部分功能的实现，还是值得一看的。</p><p>先简单介绍一下怎么使用 CancelToken 取消一个正在执行的请求。使用 <code>CancelToken.source()</code> 工厂函数获得一个 token 操作对象 <code>source</code>。将 token 传入 config 中即可控制该请求是否需要取消。取消时执行 <code>source.cancel()</code> 即可。 </p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">const</span> <span class="hljs-title class_">CancelToken</span> = axios.<span class="hljs-property">CancelToken</span>;<br><span class="hljs-keyword">const</span> source = <span class="hljs-title class_">CancelToken</span>.<span class="hljs-title function_">source</span>();<br>axios.<span class="hljs-title function_">post</span>(<span class="hljs-string">&#x27;/user/12345&#x27;</span>, &#123;<br>  <span class="hljs-attr">name</span>: <span class="hljs-string">&#x27;new name&#x27;</span><br>&#125;, &#123;<br>  <span class="hljs-attr">cancelToken</span>: source.<span class="hljs-property">token</span><br>&#125;)<br><br><span class="hljs-comment">// cancel the request (the message parameter is optional)</span><br>source.<span class="hljs-title function_">cancel</span>(<span class="hljs-string">&#x27;Operation canceled by the user.&#x27;</span>);<br></code></pre></td></tr></table></figure><p>不过，工厂函数方案其实是对 CancelToken 的一个封装。工厂函数如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-comment">// lib/cancel/CancelToken.js</span><br><span class="hljs-title class_">CancelToken</span>.<span class="hljs-property">source</span> = <span class="hljs-keyword">function</span> <span class="hljs-title function_">source</span>(<span class="hljs-params"></span>) &#123;<br>  <span class="hljs-keyword">var</span> cancel;<br>  <span class="hljs-keyword">var</span> token = <span class="hljs-keyword">new</span> <span class="hljs-title class_">CancelToken</span>(<span class="hljs-keyword">function</span> <span class="hljs-title function_">executor</span>(<span class="hljs-params">c</span>) &#123;<br>    cancel = c;<br>  &#125;);<br>  <span class="hljs-keyword">return</span> &#123;<br>    <span class="hljs-attr">token</span>: token,<br>    <span class="hljs-attr">cancel</span>: cancel<br>  &#125;;<br>&#125;;<br></code></pre></td></tr></table></figure><p>发现 <code>cancel</code> 函数通过 <code>CancelToken</code> 构造函数执行入参函数的方式暴露的，<del>可以说是非常扭曲了</del>。不过概括起来，CancelToken 对象为用户提供了两个元素：<code>token</code> 与 <code>cancel</code> 函数。</p><p>先观察 <code>cancel</code> 函数，执行该函数会给 <code>reason</code> 赋值，并传入 <code>resolvePromise</code> 函数。通过判断 <code>reason</code> 是否已经存在来判断 <code>cancel</code> 函数是否已经被执行。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-comment">// lib/cancel/CancelToken.js</span><br>  <span class="hljs-title function_">executor</span>(<span class="hljs-keyword">function</span> <span class="hljs-title function_">cancel</span>(<span class="hljs-params">message</span>) &#123;<br>    <span class="hljs-keyword">if</span> (token.<span class="hljs-property">reason</span>) &#123;<br>      <span class="hljs-comment">// Cancellation has already been requested</span><br>      <span class="hljs-keyword">return</span>;<br>    &#125;<br><br>    token.<span class="hljs-property">reason</span> = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Cancel</span>(message);<br>    <span class="hljs-title function_">resolvePromise</span>(token.<span class="hljs-property">reason</span>);<br>  &#125;);<br></code></pre></td></tr></table></figure><p>这个 <code>resolvePromise</code> 是用来 resolve 一个 <code>Promise</code> 的。<del>（和没说一样）</del> 具体而言，使用外部变量将 <code>Promise</code> 中的执行器入参暴露出来，从而实现在任意位置控制这个 Promise 的状态。后面会详细介绍 <code>Promise</code> 构造器的工作方式。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-comment">// lib/cancel/CancelToken.js</span><br>  <span class="hljs-keyword">var</span> resolvePromise;<br><br>  <span class="hljs-variable language_">this</span>.<span class="hljs-property">promise</span> = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Promise</span>(<span class="hljs-keyword">function</span> <span class="hljs-title function_">promiseExecutor</span>(<span class="hljs-params">resolve</span>) &#123;<br>    resolvePromise = resolve;<br>  &#125;);<br></code></pre></td></tr></table></figure><p>有了这个“扳机”，怎样实现请求的取消呢？先看看这个“扳机”后面都链接了什么内容。可以发现 Axios 在这个“扳机”后面调用了 <code>Promise.prototype.then()</code> 函数。由于“扳机”一直处于 <code>pending</code> 状态，<code>then</code> 的入参函数也将不会执行。阅读后可以发现这个函数主要执行与该 <code>token</code> 绑定的每个函数。这属于设计模式中的 <strong>观察者模式</strong> 。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-comment">// lib/cancel/CancelToken.js</span><br>  <span class="hljs-variable language_">this</span>.<span class="hljs-property">promise</span>.<span class="hljs-title function_">then</span>(<span class="hljs-keyword">function</span>(<span class="hljs-params">cancel</span>) &#123;<br>    <span class="hljs-keyword">if</span> (!token.<span class="hljs-property">_listeners</span>) <span class="hljs-keyword">return</span>;<br><br>    <span class="hljs-keyword">var</span> i;<br>    <span class="hljs-keyword">var</span> l = token.<span class="hljs-property">_listeners</span>.<span class="hljs-property">length</span>;<br><br>    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; l; i++) &#123;<br>      token.<span class="hljs-property">_listeners</span>[i](cancel);<br>    &#125;<br>    token.<span class="hljs-property">_listeners</span> = <span class="hljs-literal">null</span>;<br>  &#125;);<br></code></pre></td></tr></table></figure><p>后面就不过多展示源码了，主要包含以下几部分内容：</p><ul><li>实现订阅函数与取消订阅函数</li><li>在请求发送前、请求发送后判断是否已经执行取消操作</li><li>与 <code>http</code> 或 <code>xhr</code> 联动，实现取消功能。</li></ul><p>不过还有一个额外的问题，在提交历史和其他介绍文章中也没有看到合适的解释：</p><blockquote><p>在 <code>lib/cancel/CancelToken.js</code> 中，<code>CancelToken</code> 构造函数内。在上述执行 <code>then</code> 函数的操作后，又新写了一个 <code>then</code> 方法。不知道这个 <code>then</code> 方法是用来做什么的。</p></blockquote><p>如果有了解的同学希望不吝赐教。</p><h2 id="Promise-构造器"><a href="#Promise-构造器" class="headerlink" title="Promise() 构造器"></a>Promise() 构造器</h2><p>这部分在 MDN 上的中文文档有我参与翻译的部分。当时也主要是因为原始版本与英文版本相距甚远，于是按照英文版本的文档进行了完善。笔者认为阅读过该文档，应该会对 <a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/Promise">Promise() 构造器</a> 及其入参函数有进一步的理解。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">new</span> <span class="hljs-title class_">Promise</span>(executor)<br></code></pre></td></tr></table></figure><p>入参函数 <code>executor</code> 决定了这个新 Promise 的行为是怎样的。<code>executor</code> 有两个参数构成，当然这两个参数传不传都不会报错，但如果连 <code>onFulfilled</code> 都不传的话，这个新 Promise 将会永久处于 <code>pendding</code> 状态。</p><blockquote><p>这里使用与 MDN 文档上不同的入参命名方式，主要是考虑到与上文的一致性。</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">executor</span>(<span class="hljs-params">onFulfilled, onRejected</span>)&#123;<br>  <span class="hljs-comment">// 通常是一些异步操作，同步操作也可以</span><br>&#125;<br></code></pre></td></tr></table></figure><p>如何改变这个 Promise 的状态呢？有以下三种情况：</p><ul><li>调用 <code>onFulfilled(value)</code>，新 Promise 为 <code>fulfilled</code>，<code>fulfilled</code> 值为 <code>value</code>。</li><li>调用 <code>onRejected(reason)</code>，新 Promise 为 <code>rejected</code>，<code>rejected</code> 值为 <code>reason</code>。</li><li><code>onFulfilled</code> 函数出现异常，新 Promise 为 <code>rejected</code>，<code>rejected</code> 值为该异常。</li></ul><p>此外 <code>executor</code> 自身的返回值将会被忽略。所以 <code>Promise()</code> 构造器的主要作用是将一个异步操作包装成 Promise。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>本文主要还是笔者个人的想法居多，以学习的角度详细介绍了 Axios 库中两个与 Promise 应用相关的功能，从而深入理解 Promise 的工作原理和部分函数的实现。希望以后还可以产出这样的文章与各位共勉。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><ul><li>其实写这篇文章的时候主要参考的是 <a href="https://axios-http.com/zh/docs/intro">官方文档</a> 与 <a href="https://developer.mozilla.org/zh-CN/">MDN 文档</a> 。下面列出来的主要是学习 Axios 源码时所参考的文章。</li><li>另外，还有一些关于用语的内容，也写在这里了。针对 “fulfilled/rejected with …” 这种表达，笔者采用了 fulfilled/rejected 的值为 … 。都指该 Promise 后接 then 的两个入参函数（onFulfilled/onRejected）的入参。（感觉不太好翻译这个概念。）</li></ul></blockquote><ul><li><a href="https://juejin.cn/post/6844903824583294984">Axios 源码解析 - 掘金</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
      <category>JavaScript</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2021年12月21日-冬至</title>
    <link href="/posts/2021-winter-solstice/"/>
    <url>/posts/2021-winter-solstice/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>“正经人谁写日记啊。”—— 姜文 《邪不压正》</p></blockquote><p>相比元旦或者新年亦或是圣诞节，本人更喜欢冬至一点。虽说冬天还没有过去，甚至可能更加寒冷。但黑夜越来越短，白天越来越长，地球又绕着太阳走了一圈，更喜欢这样的日子作为新年一些。</p><span id="more"></span><p>虽然这些矫揉造作的话越来越不想写了，但想了想这个仪式感还是稍微继承一下比较好。</p><p>原本想借用巧克力的比喻来作为一年以来的概括，不过现在的巧克力也不会有不稳定的味道了吧。只剩下又苦又甜的味道。但去吃100%的巧克力无异于是在折磨自己，就和去喝美式咖啡、啤酒亦或是茶叶一样。口腔和嗓子里只有或苦或酸的味道，与回味无穷完全不沾边。</p><p>这可能也是我觉得这一年也不算坏的原因吧。给不出很高的评价，但也给不出很低的评价。</p><p>翻了一下手机里的照片，这一年里经历的事情也真是够多的，即使翻了下相册也有这种繁忙的感觉。有些经历倒是可以用新奇来形容，但有些就只能用称之为成长了。当然，成长也不是不好吧，只是觉得肩膀更酸痛了。</p><p>想起一个人坐着来时两个人的公交。想起无力的自己坐在木屋烧烤。想起送我出门的 Mentor 手里拿着的烟。想起丢掉的那些炒糊的黑胡椒。想起深夜心悸时的她打来的QQ电话。想起欢歌笑语的病房里一个又一个吊瓶。想起又一次打开腾讯会议的电脑。想起朝阳站排了长队的公交。</p><p>也许考试成绩没那么重要。也许长远的计划不会很长远。也许最好的结果也没那么好。也许无力的感觉还是那么无力。也许运气比努力更有效。也许新冠比计划更迅速。也许失望比期望更迫切。也许未来比现在更好。</p><p>也不知道自己有没有在这些经历里学到些什么。有变得更从容么？有变得更果断么？有变得更可靠么？有变得更坚强么？可能也没有吧，还是不敢开启对话，还是会错过重要的机会，还是搞砸过一件又一件事，还是想把她的手放在我的头上。这样下去就不会长大了吧。</p><p>但身边的世界越来越现实，学校这个泡泡也总会破碎。自己真的明白接下来应该怎么走么？真的能自己解决一切么？大概也还是走一步算一步吧。</p><p>也许大部分人都在走一步算一步，可能也不是逃避现实，只是真的没有方法。虽然听起来有种得过且过的感觉，仿佛成为了小时候讨厌的样子。但也许这就是大部分人能给出的答案了。</p><p>虽然未来就像迷雾雾一样，但至少和她在一起的未来就像迷雾中的光。即使想一下都会很开心，开心地走进迷雾中。</p><p>纯黑的巧克力也没有那么糟糕，虽然很苦，但巧克力的香味也变得更浓了。美式咖啡也没有那么难喝，虽然还是苦涩的味道，但闻起来还是很香的。茶水也没有那么老气，虽然还是不习惯，但觉得普洱也挺好喝的。</p><p>好像也没那么糟糕嘛，虽然也还蛮糟糕的。</p>]]></content>
    
    
    <categories>
      
      <category>吐槽</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2021a10-静态词向量的评估</title>
    <link href="/posts/2021a10/"/>
    <url>/posts/2021a10/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>（把每双周周报在BLOG上整理一下，以防止自己做的PPT最后都想不起来是在说什么。）</p></blockquote><p>词向量的评估是个人比较感兴趣的方向，好像也是比较冷门的方向。虽然这一领域目前没有公认方案，但实际上由于大规模语言模型的迅速发展，可以将上下文进行融合进词的动态词表征被广泛的使用，静态词表征的相关研究一直处在冷门状态。</p><span id="more"></span><h1 id="词向量及其评估"><a href="#词向量及其评估" class="headerlink" title="词向量及其评估"></a>词向量及其评估</h1><h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>词向量（Word Embedding/Vector）又或者翻译成词嵌入，对于大部分从事相关工作的人来说应该不是一个陌生的概念。其主要作用是将自然语言中的词用一个向量表示，并且希望这个向量的维数可以小一点，还能再包含一些语义信息的话就更好了。比如这个经典的例子：</p><script type="math/tex; mode=display">\text{king} - \text{man} + \text{women} \approx \text{queen}</script><p>我们希望训练出来的词向量就可以包含上述的信息，如果词向量能携带更多这种信息的话，那应该对后续的模型有更大帮助。</p><p>当然，随着 BERT/GPT 等语言模型的高速发展，它们可以生成包含上下文信息的词向量。由于在预训练语言模型是使用了大量的数据，这些基于 BERTology 的语言模型有着非常好的性能，现已作为很多后续任务的特征提取部分。</p><p>文本主要讨论的是前者，也就是<strong>静态词向量</strong>。它不随词汇所在句子而改变其表征。</p><h2 id="词向量的评估"><a href="#词向量的评估" class="headerlink" title="词向量的评估"></a>词向量的评估</h2><p>词向量的评估一直是一个比较棘手的问题，因为词向量没有黄金标准，无法通过简单的分类或者回归任务对其进行建模，从而无法给出一个公认评估方式。但换种角度，一个好的词向量应该可以在多种任务上都能取得不错的表现。由此，评价一个词向量的方法主要分为两种：外在评估（Extrinsic Evaluation）与内在评估（Intrinsic Evaluation）。</p><h3 id="外在评估"><a href="#外在评估" class="headerlink" title="外在评估"></a>外在评估</h3><p>外在评估就是将词向量在多种下游任务中进行实验，通过与其他词向量的实验结果进行比较，从而得出哪个模型的性能更好。这个思路是相对更加直接的。</p><p><img src="./exval.png" alt="外在评估流程简图"></p><p>但其显然存在很多问题：</p><ul><li>实验步骤繁琐，需要大量实验才能证明某一词向量的优越性。</li><li>需要基线进行对比，无法为单一词向量给出评价。</li><li>干扰变量过多，在实验中控制变量会非常困难。</li></ul><p>因此，内在评估就显得格外重要。</p><h3 id="内在评估"><a href="#内在评估" class="headerlink" title="内在评估"></a>内在评估</h3><p>内在评估就是直接测试词向量之间的句法和语义联系。概念倒是很直接，但如何测试句法联系和语义联系就变得非常困难。目前内在评估主要分为：相关度比较与类比推理两种。</p><h4 id="相关度比较"><a href="#相关度比较" class="headerlink" title="相关度比较"></a>相关度比较</h4><p>相关度比较是指给出一组包含若干个词语对的测试集合，每个词语对有若干个人类评分作为标准答案。在评价某一词向量方案时，计算两向量的相似度与人类评分的相关性即可。</p><p>但显然，相关度比较方案最大的问题就是，该方案需要大量的人力资源，想获得大量的标注数据成本过高。另外，评价两个词是否相似存在严重的主观性，两个词的相似性在不同的语言环境中也有不同。</p><h4 id="类比推理"><a href="#类比推理" class="headerlink" title="类比推理"></a>类比推理</h4><p>类比推理是指让模型进行类比推理，比较模型给出的答案与人类标准答案，从而判断模型的性能。我们再用前面的例子进行解释：通过让模型计算 $\text{man}:\text{king}::\text{women}:?$ 。如果模型给出的向量计算结果与 $\text{queen}$ 这个向量相近的话，那么就认为词向量是正确的。通过若干道这种题目，判断词向量质量。</p><p><img src="./inval.png" alt="内在评估的简要对比"></p><h1 id="部分词向量评估方案"><a href="#部分词向量评估方案" class="headerlink" title="部分词向量评估方案"></a>部分词向量评估方案</h1><h2 id="类比推理评估"><a href="#类比推理评估" class="headerlink" title="类比推理评估"></a>类比推理评估</h2><p>论文原文：Efficient Estimation of Word Representations in Vector Space<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mikolov T ,  Chen K ,  Corrado G , et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013.">[1]</span></a></sup></p><p>该方案与 Word2Vector 在同一篇文章中被提出，用以评估 Word2Vector 的性能。该方案中的题目大概分为两部分，一部分是世界知识：国家与首都、城市与州、男性与女性等等；而另一部分则是英文语法，例如过去时态、名词单复数等等。</p><p><img src="./ca.png" alt="类比推理"></p><p>但该数据集中的词汇过于局限于英文环境，因此在其他语言中需要另外设计题目，迁移性较差。</p><h2 id="引入词形学的类比推理评估"><a href="#引入词形学的类比推理评估" class="headerlink" title="引入词形学的类比推理评估"></a>引入词形学的类比推理评估</h2><p>论文原文：Analogical Reasoning on Chinese Morphological and Semantic Relations<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Li S ,  Zhao Z ,  Hu R , et al. Analogical Reasoning on Chinese Morphological and Semantic Relations[C]// Meeting of the Association for Computational Linguistics. 2018.">[2]</span></a></sup></p><p>这是一个针对中文词向量数据集研究，针对中文中的叠词等词形学变化，在上文类比推理的基础上加入了这部分测试内容，构成如下测试集合。可以发现这种测试数据更为符合中文的词向量情况。</p><p><img src="./ca8.png" alt="引入词形学的类比推理"></p><p>该方法和其他类比推理的方法一样，其局限性在于仅针对中文环境，在其他语言中需要另外设计题目，迁移性较差。</p><h2 id="分类模块度"><a href="#分类模块度" class="headerlink" title="分类模块度"></a>分类模块度</h2><p>论文原文：Evaluating Word Embeddings with Categorical Modularity<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Casacuberta S ,  Halevy K ,  Blasi D E . Evaluating Word Embeddings with Categorical Modularity[C]// Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021.">[3]</span></a></sup></p><p>该方法为静态词向量的内在评估提出一种新的方法。该方法首先选定若干类别，每个类别内有若干单词。从词向量中获取这些单词对应的向量，将单词相连构成网络。最后根据类别将网络划分成多个社区，计算其模块度。</p><p>这篇文章中的另一个亮点是使用了神经生物学的相关研究，这篇文章中所选的类别来自Binder Categories<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Binder J R ,  Conant L L ,  Humphries C J , et al. Toward a brain-based componential semantic representation[J]. Cognitive Neuropsychology, 2016:1-45.">[4]</span></a></sup>。该分类方法根据分析人脑对不同词汇的反应进行分类，本文使用这种该方法与无监督的社区划分做对比，得出该方法的效果更好。</p><p><img src="./binder_categories.png" alt="Binder Categories"></p><p>该方法虽然用实验展示了一种非常新颖的词向量评估方法，但仍存在一部分问题，或者说研究不充分的地方。</p><ol><li><strong>未考虑一词多义的情况：</strong>在大部分语言中，一个词通常包含多个意思。即使在Binder Categories中，部分词汇仍然有其他含义，使得该词汇从一个类别跳到另一个类别。</li><li><strong>无法扩展至其他语言：</strong>虽然文中通过机器翻译的方法将模块度评估方法扩展至其他语言，但经过本人阅读词表后发现，该词表中包含与文化关系密切的词汇。这些词汇在部分国家中可能无法翻译成非常恰当合适的词汇。</li><li><strong>实验未考虑深度学习模型：</strong>文中针对词向量评估方法的验证仅停留在SVM等传统方法上。针对深度学习模型，尤其是大规模预训练语言模型的验证则更加实际。</li></ol><h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><h2 id="针对类比推理方案的质疑"><a href="#针对类比推理方案的质疑" class="headerlink" title="针对类比推理方案的质疑"></a>针对类比推理方案的质疑</h2><p>论文原文：The (too Many) Problems of Analogical Reasoning with Word Vectors<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rogers A ,  Drozd A ,  Li B . The (too Many) Problems of Analogical Reasoning with Word Vectors[C]// Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017). 2017.">[5]</span></a></sup></p><p>近来这篇文章质疑类比推理方法评价词向量存在一些问题，而这些问题被有意无意忽略了，导致类比推理评价方法的说服力大大减弱。类比推理方法相当于找到下式中最接近 $b’$ 的向量所表示的词汇。</p><script type="math/tex; mode=display">a - a' + b \approx b'</script><p>这篇文章通过大量实验，发现如下几个问题。</p><ol><li>$a-a’$ 无法对 $b$ 形成足够的偏移。导致事实上最接近 $a - a’ + b$ 的向量应该是 $b$ 而不是 $b’$。这其实是与我们的期望相违背的。</li><li>$a,a’,b,b’$ 间的相似度会影响预测准确率。当这四个向量越接近，预测的准确率越高。这一说明类比推理其实并没有起到作用。</li><li>在候选集合里排除 $a,a’,b$ 会导致错误。例如下面的表达式，答案应该为$\text{white}$，但该向量已经被排除，所以永远无法得到正确的答案。</li></ol><script type="math/tex; mode=display">\text{snow} : \text{white} = \text{sugar} : ?</script><p>由此说明类比推理方法虽然看起来很合理，但实际上仍然存在巨大缺陷。这也导致了静态词向量的内在评价陷入了瓶颈。</p><p>END</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mikolov T ,  Chen K ,  Corrado G , et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Li S ,  Zhao Z ,  Hu R , et al. Analogical Reasoning on Chinese Morphological and Semantic Relations[C]// Meeting of the Association for Computational Linguistics. 2018.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Casacuberta S ,  Halevy K ,  Blasi D E . Evaluating Word Embeddings with Categorical Modularity[C]// Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Binder J R ,  Conant L L ,  Humphries C J , et al. Toward a brain-based componential semantic representation[J]. Cognitive Neuropsychology, 2016:1-45.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rogers A ,  Drozd A ,  Li B . The (too Many) Problems of Analogical Reasoning with Word Vectors[C]// Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017). 2017.<a href="#fnref:5" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
      <category>周报</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>动画OP与ED手法观察2</title>
    <link href="/posts/oped-view2/"/>
    <url>/posts/oped-view2/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><div class="note note-secondary">            <p><strong>发布于</strong>：檐枫动漫社<br><strong>作者</strong>：Santoin<br><strong>日期</strong>：2021年10月5日<br><strong>原文链接</strong>：<a href="https://mp.weixin.qq.com/s/hSSzfYa2Vwdcr7KRp1GiTA">【投稿】动画OP与ED手法观察 - 二 - 檐枫动漫社</a></p>          </div><p>みなさんこんばんは。</p><p>因为众所周知的原因，再加上本人确实有点懒，最近已经很少坐在电脑前等着动画的更新然后即刻收看。更多的可能是挑自己有时间的时候把之前没看的一口气补完。虽说补番的话，很容易只看一遍 OPED，之后就跳过了，但有些动画的 OPED 仍然给我留下了非常深刻的印象。</p><p>虽然在下面列举的动画的 OPED 中，有一些可能没法讨论其手法，但在这里也一并介绍，更多是因为个人很喜欢其创意或者设计想法。</p><p>当然，我也回顾了一下在三年前写的 <a href="https://tackoil.github.io/2019/10/26/oped-view/">《动画OP与ED手法观察》</a>，感觉这三年的总结可能没有没有那么多丰富的手法，更多的是一次分享。希望能把这次对各种各样的 OPED 介绍给大家。</p><span id="more"></span><h1 id="动画OP与ED手法观察2"><a href="#动画OP与ED手法观察2" class="headerlink" title="动画OP与ED手法观察2"></a>动画OP与ED手法观察2</h1><p>众所周知，动画的 OPED 不仅仅是放 Staff 表的地方，也是展现动画内容并与主题曲结合的重要位置。做好这两点也是一个优秀的动画 OPED 所必备的工作。</p><h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><p>如何做好上面两点，对于不同的监督而言可能各有想法。有的会把重心放在如何淋漓尽致地展现动画的精彩之处。</p><p><img src="./wondereggpriority.png" alt="WONDER EGG PRIORITY - OP"></p><p>《奇蛋物语》这部动画刚上映的时候，不少人应该会被其非常精细的动画所吸引。（当然后面看起来实在做不动了，<del>感觉原画都要进医院了</del>）但至少这个 OP 透露着监督和团队的决心。无论是截图中这个吃酸奶的细节，还是后面的大户爱跳着碰天梯的动作，都是夸张到离谱的精细。</p><p>虽然没有单独买曲子作为 OP，直接改编了日本创作自 1965 年的经典毕业歌曲《巣立ちの歌》，但与整部动画的主题仍然比较契合。<del>（可以说是非常省钱了）</del></p><p>手法上除了前面极其离谱的精细感之外，也使用了一些结合实景的技术：包括奇蛋出现在桥上的渲染、以及大户爱走在林间小道的背景。使用这样的技术可以更好的贴近生活场景，再结合精细的作画就可以给观众带来真实感和代入感。</p><p>当然，希望各位动画从业者，和各位画师不要这么拼命。注意身体啊。</p><hr><p><img src="./eizouken.png" alt="映像研には手を出すな！ - OP"></p><p>如果说《奇蛋物语》是通过非常精细的作画吸引观众，将真实感贯穿 OP 始终。那么《映像研》就是另一个极端———把不真实感贯穿始终。</p><p>高饱和、扭曲、元素拼贴、奇怪的舞蹈。可以说这部 OP 几乎完全脱离常规，仿佛像在一堆奇幻的元素符号中梦游。但这些元素符号也恰恰体现了动画中天马行空的想象这一特点。动画中大量出现剧情世界与想象世界的反复横跳，所以笔者觉得非常符合动画的风格。感觉这篇 OP 的作者真的很有勇气。</p><blockquote><p>ps 我觉得 OPED 选 rap 的都是天才。<br>pss 这个截图里的元素居然有制作方的 LOGO。</p></blockquote><h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><p>选 rap 这件事，也是促使我写这篇文章的一个重要契机，也就是下面这张截图。</p><p><img src="./doramaid.jpg" alt="小林さんちのメイドラゴンS - OP"></p><p>《龙女仆》第二季的 OP 还是挺另笔者眼前一亮的。笔者是先听了 OP 曲后看的动画，这就导致在看动画时便携带着一个问题：如何把一首 rap 做成一个正常的动画 OP？这篇动画给出的答案还是很令我满意的，这部 OP 绘制了每一位角色非常细致的 rap 动作来填充这一部分，并比较细致的对了口型，从而使这一段不会显得无趣。</p><p>当然，还是很有《日常》既视感的。但相比《日常》来说，每个人物的动作更加细致和丰富，镜头拉近的过程中人物也有小幅度的动作，大概这就是技术的进步吧。（石原立也：我致敬我自己）</p><p><img src="./nichijou.png" alt="日常 - OP"></p><hr><p><img src="./oddtaxi.png" alt="ODD TAXI - OP"></p><blockquote><p>好像这部动画的 OP 也有 rap 诶。</p></blockquote><p>《奇巧计程车》这部动画在叙事上有一点像日剧，笔者认为还算是一部不错的作品。OP 选用了一首比较爵士风格的曲子，确实会有一种在深夜出租车的广播里听到这样一首歌的感觉。作画部分，在衬词部分让所有角色在这里做一些有摇摆感的动作，给人一种比较放松的感觉。</p><p>另外就是这部 OP 都使用了类似水彩画或者简笔画的笔触，再加上出场角色都是动物拟人，因此看起来就更像是绘本，也减少了动物拟人在真实人类城市的违和感。</p><h2 id="3"><a href="#3" class="headerlink" title="3"></a>3</h2><p>当然，除了做好基本工作，如果能让人在 OP 或者 ED 令人眼前一亮，也可以说是巨大成功了。当然，这样的眼前一亮可能不依赖于精湛的作画或者精巧的剧情，可能只需要一些创意。</p><p><img src="./saga2.png" alt="ゾンビランドサガ リベンジ - OP"></p><p>《佐贺》第二季的 OP 非常巧妙地把 MG 元素和动画角色融合在一起。个人觉得更加高超的地方在于其还巧妙地把 3D 元素和 2D 元素融合在了一起。整个 OP 一直在 3D 与 2D 中反复切换，并使用 3D 的透视变化去构成 2D 元素，非常巧妙。<del>（尤其是最后的 MAPPA）</del></p><p>虽说使用公交牌作为 MG 元素已经不算罕见了（比如《街角魔族》），但这里也非常成功地做出了自己的风格和特色，这还是很难得的。</p><hr><p><img src="./horimiya.png" alt="ホリミヤ - OP"></p><p>《堀与宫村》这部动画的 OP，最突出的地方就是这个图片间的切割的移动了。与普通的图片流不同的是，屏幕中的多个区块可能会显示同一区域的不同画面。例如走廊拐角、向日葵花田、还有截图中的这件教室。此外这间教室虽然同时出现了4个人，但每个人在不同的区块中，所处的季节也各不相同。用各种错觉给观众眼见一亮的感觉。</p><p>另外很巧妙的事，利用这个移动的区块，把 Staff 表藏在其中。不知道 NCOP 还会不会有 Staff 的这个白色背景框。</p><hr><p><img src="./jyujyutsu.png" alt="呪術廻戦 - ED1"></p><p>《咒术回战》的 ED 从“让人眼前一亮”的角度来说，可谓让人眼前一亮（喂）。这种故意很潦草的、将颜色涂出区域外或者图不满的上色风格可能会在某些海报上见到，这里结合 JAZZ 使用显然是更加突出涂鸦的随意感觉。<del>（你看人物动作这么复杂，肯定是有钱没处花了）</del></p><p><img src="./jyujyutsu2.png" alt="呪術廻戦 - ED2"></p><p>而 ED2 则大量使用了这种不稳定的、逆光、对不上焦的镜头，应该是想模拟手机拍摄的效果。结合 ED 里悠仁拿手机录像的镜头来看，应该是表达这样的含义。当然，从制作上也可能是先用手机拍了一段，然后将绘画的素材叠在上面达到这样的效果。但即便如此，这段对手机录像的模拟也充满的真实感。<del>（你看这么光污染，肯定是有钱没处花了）</del></p><hr><p>当然，眼前一亮也并不是需要很多钱。就像前面提到的，运用一些海报中的手法也会让画面变得非常酷。</p><p><img src="./idinvaid.png" alt="ID:INVADED - ED"></p><p>《异度侵入》这部动画的 ED 用了当时非常常用的将人物与风景图重叠起来的海报制作方法，大概检索了一下叫“双重曝光”风格。大概原理也是用一层人物作为风景图的滤色，用一层人物作为风景图的蒙版，最后调下各部分的透明图就可以得到类似的风格了。</p><p>上一篇推送可能更多的都是关于与海报设计风格在 OPED 中的应用。显然这个 ED 就比较符合上一篇推送的话题。</p><h2 id="4"><a href="#4" class="headerlink" title="4"></a>4</h2><p>在结语部分想说的内容可能和上次差不多，这些手法和技术从来也不是限制作品的枷锁，而是帮助作者表达其想法和创意的工具。让人看完喊出 awsl 或许只需要一位屑魔女打一个小节标准的 4/4 拍指挥手势。</p><p><img src="./majou.png" alt="魔女の旅々 - OP"></p><p>三年来，随着技术的提高，各个作品的作画愈发精细和完美。但即使这样，可以发现留给这短短 90 秒的想象空间仍然很多，仍然还可以迸发出不少创意让观众眼前一亮。那么就一起期待未来的 90 秒里，创作者们会带来怎样的惊喜吧。</p><p>大好きだよ、みんな！</p>]]></content>
    
    
    <categories>
      
      <category>吐槽</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>从核酸检测开始</title>
    <link href="/posts/pcrtest/"/>
    <url>/posts/pcrtest/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>仅记录道听途说的笑话，以及联想。</p><span id="more"></span><h1 id="管中窥豹"><a href="#管中窥豹" class="headerlink" title="管中窥豹"></a>管中窥豹</h1><p>实验室开会，可能是抨击我们的时候突然想到了前一天给新研一开会的事情。老师突然抓一个人问了几个问题。</p><blockquote><p>老师：你从哪里回的学校？从 XX 市？<br>同学：是的。<br>老师：<strong>怎么做的核酸？</strong><br>同学：携带48小时内的报告上车，然后……</p></blockquote><p>老师：你看，这才是正常的回答。你们知道昨天开会的那个新生怎么回答的么？</p><blockquote><p>老师：你从哪里来的学校？<br>同学：XX 市。<br>老师：<strong>怎么做的核酸？</strong><br>同学：张开嘴，然后……</p></blockquote><p>老师：昨天我还在想，是我们有代沟么，还是我问的有问题。今天看你这么一回答，就放心多了。</p><p>老师讲完这件事后，实验室传来快活的气息。不过出于个人兴趣，所以我就陷入了这样的一个思考。为什么会有这样的回答？</p><h1 id="盲人摸象"><a href="#盲人摸象" class="headerlink" title="盲人摸象"></a>盲人摸象</h1><p>首先，可以判断一下，这样的对话成立么？如果把第一组问答和第二组问答分开看的话，显然是成立的。这显然涉及到了“<strong>怎么做的核酸？</strong>”这个问题中的“怎么”到底在询问什么。在第一个回答中，回答了返校的疫情防控规定；在第二个回答中，回答了如何进行核酸检测。如果独立来看，显然后者的回答更加准确。</p><p>可以通过根据答案提问题的方式得出上面的推论。</p><blockquote><p>回答：携带48小时内的核酸检测阴性报告上车，……<br>提问：</p><ul><li>疫情期间，返校的流程是什么？</li><li>返校的核酸检测应该怎么做？</li><li>返校的时候，核酸检测的流程是什么？</li></ul></blockquote><p>显然，上面的问题都需要<strong>返校</strong>这个条件。</p><blockquote><p>回答：张开嘴，发出“啊”的声音，……<br>提问：</p><ul><li>如何进行核酸检测？</li><li>核酸检测的流程是什么？</li><li>怎样配合医生进行核酸检测？</li></ul></blockquote><p>而这里的问题就更加符合原始问题。可见在不考虑上下文的情况下，后者的回答更加准确。</p><p>但读完这个故事，大部分人（包括我）的反应也是后者的回答过于荒唐。以普遍理性而论，是因为第一组问答包含了一定的信息，才导致第二组问答的问题的含义发生了变化。但我个人认为，除了第一组问答外，背景知识则是更加重要的原因。</p><h1 id="按图索骥"><a href="#按图索骥" class="headerlink" title="按图索骥"></a>按图索骥</h1><p>在自然语言处理领域中，背景知识是导致模型性能不尽如人意的重要原因。近来很多模型通过大量的训练数据和模型参数，也可以认为其学习了不少背景知识，从而使模型有一定的提高。但人们在平时的对话聊天中，背景知识很可能会自然而然的忽略掉。</p><p>我们可以分析一下这组问答，看看合理回答这简单的两个问题需要哪些背景知识。</p><h2 id="问题：你从哪里来的学校？"><a href="#问题：你从哪里来的学校？" class="headerlink" title="问题：你从哪里来的学校？"></a>问题：你从哪里来的学校？</h2><p>“来学校”，一般说明是第一次在学校，或者说很少在学校。在提问者的理解中，对于答者而言，学校是个新地方，或者不属于自己的地方。</p><p>“从哪里”，显然是表示地点，但并没有明确限定是哪种粒度的地点。本着提供最大信息量的想法，首先排除包含学校的地点：例如太阳系、地球、……、北京、海淀。所以在北京的话，可能会回答在西直门、在望京或者在学校附近。而不在北京就可能会回答所在省或者所在市。</p><p>问题的提问时间：“九月份”。在中国九月是开学的月份。再结合前两点可知，在提问者的角度，回答者应该是从放暑假的状态进入开学状态，可以简单称作：返校或者开学。所以回答“哪里”的时候，一般指的是家乡。这也就有了提问者可以根据自己对答者的理解，给出猜测的答案。</p><h2 id="问题：怎么做的核酸？"><a href="#问题：怎么做的核酸？" class="headerlink" title="问题：怎么做的核酸？"></a>问题：怎么做的核酸？</h2><p>首先，缺失的主语肯定是“你”，也就是答者。</p><p>其次，显然“核酸”是“核酸检测”的简写。可以通过“做核酸检测”这一常用词汇推知。</p><p>“怎么做”，这个就是问题歧义的源头。所以首先想到的就是借用上个问题包含的信息：返校或者开学、还有答者回学校路途的起点，我们假设为 XX 市。把这两条信息加入，问题就变成了“<strong>从 XX 市返校时的核酸检测时怎么做的呢？</strong>”既然带上了这两个修饰词，说明这里的怎么做核酸检测与 XX 市这一地点和返校这一时间点有关。在这两条信息的约束下，与其他不同的核酸检测，就应该是返校疫情防控规定中不同地区进行的不同有效时间的核酸检测了。</p><p>综上可以发现，在人类的日常对话中实际上使用了大量的背景知识。这些背景知识不仅是时间、地点、说话人的身份。甚至还包含时事政策、说话人的家庭环境等等。</p><h1 id="虚构推理"><a href="#虚构推理" class="headerlink" title="虚构推理"></a>虚构推理</h1><blockquote><p>怎么突然不是成语了？</p></blockquote><p>在这部分做一段有意思的思考：</p><ol><li>在已有条件下不变的情况下，如何添加其他的条件，使回答者可以合理地给出这个答案。</li><li>不考虑现有条件，对于同一个问句，通过改变背景条件，可以给出怎样不同的答案？</li></ol><div class="note note-warning">            <p>以下推理仅为猜想，与任何真实事件无关。文章作者不对以下内容的真实性负责。</p>          </div><h2 id="添加新条件"><a href="#添加新条件" class="headerlink" title="添加新条件"></a>添加新条件</h2><p>首先，做一个假设：<strong>回答者总之本着提供更多信息的想法回答问题。</strong>通常来讲，只有满足上述条件的回答才是礼貌、积极的回答。可以举一个例子来解释这个现象。如果一个人问：“你住在哪里？”而回答是“家里”，那么对于提问的人而言这就是几乎没有信息的回答，那么提问者就会觉得被敷衍。</p><p>现在我们明确一下已有条件都有哪些。可能还会有一些其他的已有条件。不过可能干扰不大，就先不列举在这里了。</p><ul><li>时间：9月初</li><li>地点：实验室内</li><li>人物：老师（提问者）、研究生新生（回答者）</li><li>社会背景：刚刚结束一段疫情。导致学校需要根据学生所在地调整返校政策。该政策主要通过控制学生的核酸检测有效时间的方式实现。核酸检测是人类检测新型冠状病毒的无症状感染者与患者的有力手段。通常包含咽拭子采样与鼻拭子采样两种方式。</li></ul><p>好了，下面可以开始了。</p><h3 id="回答者的家庭背景"><a href="#回答者的家庭背景" class="headerlink" title="回答者的家庭背景"></a>回答者的家庭背景</h3><p>这也是我当时想到的第一个方案。首先，为什么一个人会在这样的对话中回答核酸检测的流程？这说明这个人认为核酸检测是比较少见的东西，可能认为大部分人都没有做过核酸检测。因此猜测提问者想知道核酸检测的流程（并且提问句的本意就应该是这样），所以给出了如此回答。</p><p>这里引入的第一个条件就是：<strong>回答者及其所认识的人没有经历过核酸检测</strong>。但这个条件非常苛刻，还是需要添加其他条件对其进行解释。回答者是一名研究生，那么ta一定读过大学。这样ta认识的人就应该包括家人、朋友、同学和网友了。从人的常住地角度分析，家人、朋友大概率在同一常住地，网友虽然分布更广但一般不会聊这一话题，那么这里的同学就要被限定在“没做过核酸检测”这一范围内了。</p><p>如果一个学校里的所有人都没做过核酸检测，说明<strong>ta所在大学的生源大部分都是本地的</strong>，所以这也就是第二个条件。简单查一下，ta可能是来自西藏、青海或者宁夏的同学。</p><p>那么这个故事就完整了。ta从大学以来一直在家附近，考研到学校是第一次做核酸检测。因此ta对这个问题给出了如此回答。（可喜可贺）</p><h3 id="核酸检测的类型"><a href="#核酸检测的类型" class="headerlink" title="核酸检测的类型"></a>核酸检测的类型</h3><p>还有一种可能性就是，答者所强调的是这种核酸检测的流程，即咽拭子采样的核酸检测流程。那么为什么会需要强调这一点呢？说明<strong>回答者至少两种核酸检测方式都做过</strong>，甚至可以说ta可能两种方式都做过很多次。这一点可以作为引入的条件。</p><p>但仅仅引入上面的条件应该是不够的，需要为回答者找到一个使用前一问题中信息的方式。那么还是从家乡的角度入手，如果一个人经常进行核酸检测，要么是因为从事高危行业（航空运输、冷链运输等），要么是因为所在地曾经是中高风险地区。答者作为一名学生，显然是因为其所在学校或者家乡曾经是中高风险地区。更细化一点，<strong>答主的家乡应该是一个做过多轮全民核酸检测筛查的地区。</strong></p><p>这样就有比较完善的动机了。因为回答者家乡是一个经常做核酸检测的地区，所以结合前面提问者询问作者的家乡情况，因此回答者认为提问者知道自己的家乡是经常做核酸检测的地区，所以给出了咽拭子核酸检测的流程作为回答。</p><h2 id="设立新背景"><a href="#设立新背景" class="headerlink" title="设立新背景"></a>设立新背景</h2><p>这部分就是比较天马行空的联想了。下面会给出“怎么做的核酸？”这一问题的答案，然后再解释这些答案的背景条件是什么。</p><h3 id="先手消毒然后……"><a href="#先手消毒然后……" class="headerlink" title="先手消毒然后……"></a>先手消毒然后……</h3><blockquote><p>提问：怎么做的核酸？<br>回答：先手消毒，然后将拭子放入无菌生理盐水中湿润，将拭子越过舌根，在被采集者两侧咽扁桃体稍微用力来回擦拭至少3次，然后再在咽后壁上下擦拭至少3次，将拭子头放入保存液，尾部扔掉，旋紧管盖，手消毒。</p></blockquote><p>以上回答参考自《医疗机构新型冠状病毒核酸检测工作手册》。显然这个回答不是被检测者所需要关心的，说明是<strong>回答者是医务人员</strong>。但问题中包含“做”这一动词，所以回答者不是简单地背诵工作手册，而是在描述自己进行核酸检测时的操作步骤。但如果操作步骤与手册完全一致的话，这个提问就没有动机了，因此这里的操作步骤有自己的特殊性。但显然，和手册不同的操作是不规范或者错误的。不被发现的错误通常不会被作为话题，因此<strong>该核酸检测点可能发生了事故</strong>。</p><h3 id="得网上预约挂号才能做……"><a href="#得网上预约挂号才能做……" class="headerlink" title="得网上预约挂号才能做……"></a>得网上预约挂号才能做……</h3><blockquote><p>提问：怎么做的核酸？<br>回答：得网上预约挂号才能做。挂好号当天去医院就可以了。</p></blockquote><p>可以发现这组问答中的提问者和回答者的身份又有了些不同。首先对于回答者而言，他知道提问者最近一段时间没有做过核酸检测，或者医院的通知并不明确，所以给出了如何去医院进行核酸检测的流程。因此可以猜测两人所处环境应该是非中高风险地区，提问者是一位不怎么流动的居民，但最近需要出门了。</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>当然，这篇文章的出发点就是那个被老师调侃的对话了。然后出于个人兴趣开始分析这段话可能的原因。毕竟人类即使注意力再分散，这种程度的对话也应该不会出问题。所以就自然想到了会不会是实验室内的大家有一个默认共识，而这个共识再那位学生身上并没有。</p><p>不过这些推理都是猜测，或者说是一次思维游戏吧。也是第一次尝试把自己突然蹦出来的想法以有条理的方式写出来，希望有兴趣读完的读者能看懂吧。在推理可能原因的时候，使用了《虚构推理》这一名字，也是突然想到了现在做的事情也有一种虚构推理的感觉。总之写下这堆东西还挺有趣的，以后有什么奇怪的思路也可以记录下来。</p>]]></content>
    
    
    <categories>
      
      <category>吐槽</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>JavaScript 的列表解构赋值执行顺序</title>
    <link href="/posts/js-destructuring-assignment/"/>
    <url>/posts/js-destructuring-assignment/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>这篇文章将简单解释一下 JavaScript 中列表的解构赋值的细节和执行顺序。新特性还是要谨慎使用。</p><span id="more"></span><h1 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h1><p>在有次 leetcode 中的经典题目 <a href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">剑指 Offer 03. 数组中重复的数字</a> 中遇到一个小问题。简单概括，这道题需要从左往右依次将数字放到对应下标上，直到发现两个相同的数字为止。一个很直观的想法就是使用解构赋值以避免使用程序员搬家。完整的程序就大概是长下面这个样子。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">var</span> findRepeatNumber = <span class="hljs-keyword">function</span>(<span class="hljs-params">nums</span>)&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">let</span> i=<span class="hljs-number">0</span>; i&lt;nums.<span class="hljs-property">length</span>; i++)&#123;<br>        <span class="hljs-keyword">while</span>(nums[i] !== i)&#123;<br>            <span class="hljs-comment">// The following line will not work as the expect.</span><br>            [nums[i], nums[nums[i]]] = [nums[nums[i]], nums[i]]<br>            <span class="hljs-keyword">if</span>(nums[i] === nums[nums[i]]) <span class="hljs-keyword">return</span> nums[i];<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;<br>&#125;;<br><br><span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title function_">findRepeatNumber</span>([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>]))<br></code></pre></td></tr></table></figure><p>如果输入进行测试，可以喜闻乐见地发现 TLE 了。这么简单的代码应该不会超时啊。</p><h1 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h1><p>在每次交换后输出 <code>nums</code> 观察结果。第一步的输出显然应该是 <code>[1, 3, 2, 0, 2, 5, 3]</code> ，因为把<code>2</code>移动到了下标<code>2</code>上，和原本那个位置上的数字<code>1</code>进行交换。然而实际上是这样的：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[ <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span> ]<br>[ <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span> ]<br>[ <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span> ]<br>[ <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span> ]<br>...<br></code></pre></td></tr></table></figure><p>发现第一行输出就与期望值严重不符。从输出实在看不出来原因，就调试了一下。</p><p><img src="./jsdebug.png" alt="调试中" title="调试中"></p><p>可以发现调试步骤可以停在左侧列表中的第二项。意味着 JavaScript 会先对左侧第一项<code>nums[i]</code>进行赋值并存储，然后再对第二项<code>nums[nums[i]]</code>进行赋值。可以发现在赋值左侧第二项时，索引<code>nums[i]</code>已经发生了修改，也就导致无法正常工作了。</p><p>结合上面的例子。<code>i=0</code>时，<code>nums[i]=1</code>。<code>nums[nums[i]]=2</code>。因此第0项赋值为<code>2</code>，然后对第2项赋值为1。最终得到上面的结果。（当然，死循环是巧合，并非每次都会触发。）</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>当时觉得这个问题只有 JavaScript 才会遇到，Python是没问题的。后来实验了一下，Python也是按照这种方式进行赋值的。所以还是尽量不要写这种奇怪的语法吧。</p>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
      <category>JavaScript</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>《语言学常识十五讲》笔记</title>
    <link href="/posts/linguistics/"/>
    <url>/posts/linguistics/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>第一次看这种类似于教材的书籍特别想在上面做标注。可能是因为理工科的教材都把知识点列的非常明显。这本书中很多知识并不是以表格或者明显的小节标题的形式展示出来，因此需要一定的笔记。</p><span id="more"></span><div class="note note-secondary">            <p><strong>书名</strong>：《语言学常识十五讲》<br><strong>作者</strong>：沈阳<br><strong>出版社</strong>：北京大学出版社</p>          </div><h1 id="第一讲-人类独有的特性——语言"><a href="#第一讲-人类独有的特性——语言" class="headerlink" title="第一讲 人类独有的特性——语言"></a>第一讲 人类独有的特性——语言</h1><h2 id="认识人类的语言"><a href="#认识人类的语言" class="headerlink" title="认识人类的语言"></a>认识人类的语言</h2><h3 id="屈折语、黏着语、综合语、孤立语（分析性语言）"><a href="#屈折语、黏着语、综合语、孤立语（分析性语言）" class="headerlink" title="屈折语、黏着语、综合语、孤立语（分析性语言）"></a>屈折语、黏着语、综合语、孤立语（分析性语言）</h3><blockquote><p>P6 - P7</p></blockquote><ul><li>屈折语、黏着语、综合语可以总称为：形态语</li><li><strong>屈折语：</strong>主要是句子中某些词本身有丰富的形态变化，例如：德语、俄语</li><li><strong>黏着语：</strong>主要是句子中某些词的形态变化表现为附在词前后的语素，例如：维吾尔语、日语</li><li><strong>多式综合语</strong>：句子的所有成分都通过一个复杂的词来体现</li><li><strong>孤立语</strong>：没有形态变化，例如：汉语<ol><li>汉语不是通过谓词词形变化来表示“时、体、态”等语法意义，而是有一套非常丰富的表示时态的助词（如“了、着、过”等）和表示语气的助词（如“了、的、呢”等）系统。</li><li>汉语没有通过名词词形变化表示的“性、数、格”等语法意义，而是特别突出语序和虚词的作用。</li><li>汉语的各种实词（如名词、动词、形容词）都没有词尾标记，因此词类和句法成分之间不存在形态语那样的一一对应关系。</li></ol></li></ul><h2 id="语言的各种表现形式"><a href="#语言的各种表现形式" class="headerlink" title="语言的各种表现形式"></a>语言的各种表现形式</h2><h3 id="语言和言语的关系"><a href="#语言和言语的关系" class="headerlink" title="语言和言语的关系"></a>语言和言语的关系</h3><blockquote><p>P7 - P8</p></blockquote><ul><li>言语：言语动作（实际说话的过程）和言语作品（说出来和写下来的话）。</li><li>语言：说话时使用的符号工具。</li><li>只好通过“言语”去发现“语言”的踪影。</li><li>“语言”存在于“言语”的两个地方：人们已经说出来的话中、没有说出来的话中（判断某句话的正误）</li></ul><h3 id="口语和书面语"><a href="#口语和书面语" class="headerlink" title="口语和书面语"></a>口语和书面语</h3><blockquote><p>P10</p></blockquote><ul><li>口语是第一性的，书面语是第二性的。</li></ul><h2 id="语言符号和语言结构"><a href="#语言符号和语言结构" class="headerlink" title="语言符号和语言结构"></a>语言符号和语言结构</h2><h3 id="语言的符号特性"><a href="#语言的符号特性" class="headerlink" title="语言的符号特性"></a>语言的符号特性</h3><blockquote><p>P12 - P14</p></blockquote><ul><li><p>能指与所指</p><ul><li>能指：语言符号中能够职称某种意义的声音</li><li>所指：语言符号中由特定声音表示的意义</li></ul></li><li><p>约定的与任意的</p><ul><li>约定的：人为的某些规定或共同认可的某些习惯。（例：烽火台的狼烟）</li><li>任意的：用什么符号表示什么意义是不需要什么道理的。</li></ul></li><li><p>不可变的与可变的</p><ul><li>不可变的：语言符号的变化是非常缓慢的</li><li>可变的：语言符号又每时每刻都在缓慢的发生变化</li></ul></li></ul><h3 id="语言的结构特性"><a href="#语言的结构特性" class="headerlink" title="语言的结构特性"></a>语言的结构特性</h3><blockquote><p>P15 - P18</p></blockquote><ul><li><p>线性的与离散的</p><ul><li>线性的：语言符号的结构必须是按照时间顺序成一条线的样子排列</li><li>离散的：语言符号又可以分解还原成本来的一个一个的符号</li></ul></li><li><p>组合关系与聚合关系</p><ul><li>组合关系：哪个成分在前、哪个成分在后的问题就反映了一种结构特性</li><li>聚合关系：什么成分能替换出现在某个位置上的问题也反应了另一种结构特性</li></ul></li></ul><h1 id="第二讲-研究语言的科学——语言学"><a href="#第二讲-研究语言的科学——语言学" class="headerlink" title="第二讲 研究语言的科学——语言学"></a>第二讲 研究语言的科学——语言学</h1><h2 id="语言学研究的现代思潮"><a href="#语言学研究的现代思潮" class="headerlink" title="语言学研究的现代思潮"></a>语言学研究的现代思潮</h2><blockquote><p>P39</p></blockquote><p>汉语的层次分析比起美国描写语言学的分析程序来，增加了语法关系意义的内容，因此在一定程度上是以形式分析为主但也注意了意义分析的层次分析方法。</p><blockquote><p>P41</p></blockquote><p>在语言各要素当中，只有<strong>句法形式</strong>才可以从人脑全部认识系统中抽象出来作为一个独立系统，或者说只有句法形式才是一个可以穷尽推导和通过优先手段重复使用而实现的系统。而其他部分，尤其是<strong>语义</strong>，往往与人对世界的各种认识即百科知识交织在一起而无法分离出来。</p><blockquote><p>P42</p></blockquote><p>语言生成机制的语法模型：核心部分只包括句法、逻辑、语言三块，又统称“计算系统”；词库、完全语义解释使语法模型的外围部分，又叫做“调节系统”。</p><blockquote><p>P43</p></blockquote><ul><li>原则与参数理论<ul><li>“原则”使普遍适用于各种语言的。</li><li>“参数”则主要用来说明具体语言的差异。</li></ul></li></ul><h1 id="第三讲-语言的物质载体——语音"><a href="#第三讲-语言的物质载体——语音" class="headerlink" title="第三讲 语言的物质载体——语音"></a>第三讲 语言的物质载体——语音</h1><h2 id="语音是语言的物质载体"><a href="#语音是语言的物质载体" class="headerlink" title="语音是语言的物质载体"></a>语音是语言的物质载体</h2><blockquote><p>P46</p></blockquote><p>语音：由人的发音器官发出，用于人与人之间交际和表达一定意义的声音。</p><blockquote><p>P47</p></blockquote><p>世界上任何一种自然语言都是有声语言，语音正是人类语言的物质载体。</p><h2 id="语音的自然属性和社会属性"><a href="#语音的自然属性和社会属性" class="headerlink" title="语音的自然属性和社会属性"></a>语音的自然属性和社会属性</h2><h3 id="语音的心理属性"><a href="#语音的心理属性" class="headerlink" title="语音的心理属性"></a>语音的心理属性</h3><blockquote><p>P59</p></blockquote><ul><li>大脑听觉中枢在进行语音识别时，显然对听觉器官传送过来的声波进行了一定的过滤和筛选，只选择并提取与识别语音有关的特征，而舍弃其他信息。</li></ul><h2 id="“音位”和“音素（音位变体）”"><a href="#“音位”和“音素（音位变体）”" class="headerlink" title="“音位”和“音素（音位变体）”"></a>“音位”和“音素（音位变体）”</h2><h3 id="怎么确定一种语言中的“音位”"><a href="#怎么确定一种语言中的“音位”" class="headerlink" title="怎么确定一种语言中的“音位”"></a>怎么确定一种语言中的“音位”</h3><blockquote><p>P76 - P77</p></blockquote><ul><li>“对立原则”：凡两个音素，如果可以出现在完全相同的语音环境中，并且可以区别语音形式的意义，这两个音素之间就具有一种“对立关系”<ul><li>例：<code>[san⁵⁵]</code> （三）、<code>[ʂan⁵⁵]</code>（山）</li></ul></li><li>“互补原则”：凡两个音素，如果各自有自己的出现环境或者说从不出现在相同的语音环境中，这两个音素之间就具有一种“互补关系”<ul><li>例：<ul><li><code>[a]</code>：<code>[zai]</code>（海hai）、<code>[xan]</code> （涵han）</li><li><code>[A]</code>：<code>[tA]</code> （大da） 、<code>[tɕiA]</code>（家jia）</li><li><code>[ɑ]</code>：<code>[lɑu]</code>（老lao）、<code>[uɑŋ]</code> （王wang）</li></ul></li></ul></li><li>“语音近似原则”：把几个不同的音素归并为一个音位，除了要考虑它们是否处于互补关系之中，还要看语音上是否相似。</li></ul><h1 id="第四讲-语言的书写符号——文字"><a href="#第四讲-语言的书写符号——文字" class="headerlink" title="第四讲 语言的书写符号——文字"></a>第四讲 语言的书写符号——文字</h1><h2 id="文字符号的形式和文字的类型"><a href="#文字符号的形式和文字的类型" class="headerlink" title="文字符号的形式和文字的类型"></a>文字符号的形式和文字的类型</h2><h3 id="什么才是文字的“字”"><a href="#什么才是文字的“字”" class="headerlink" title="什么才是文字的“字”"></a>什么才是文字的“字”</h3><blockquote><p>P94</p></blockquote><ul><li>作为文字的符号应该有这么几个特性：<ul><li>用于书写的符号</li><li>与语言中的某种单位相联系的符号</li><li>可以确定地和成系统地表示语言中的某种单位的符号</li></ul></li></ul><blockquote><p>P97</p></blockquote><p>世界上所有独立发展起来的文字，不论是苏美尔楔形文字、古埃及文字、玛雅文字，还是汉字，情况都是一样，总是现有意符，后来才有借用意符形成的音符；而且所有非独立发展的问题的音符也都是从这些文字中的意符借用转化而来的。</p><h3 id="文字有哪些不同的类型"><a href="#文字有哪些不同的类型" class="headerlink" title="文字有哪些不同的类型"></a>文字有哪些不同的类型</h3><blockquote><p>P99 - P101</p></blockquote><ul><li><p>人类语言的文字首先可以大致分成“表音文字”和“表意文字”两大类。</p><ul><li>表音文字：可以是“音素”（音位），也可以是“音节”。</li><li>表意文字：可以是“语素”、“词语”、“词组”、“句子”</li></ul></li><li><p>“表意文字”也可以叫做“意音文字”</p><ul><li>因为所有表意文字的字符都同事表音，所以表意文字也就是意音文字。</li><li>表意文字的大部分字符是表意的，但也有些是纯粹表音的，所以表意文字也就是意音文字。</li></ul></li></ul><blockquote><p>P102 - P103</p></blockquote><ul><li>关于汉字类型的一些不同意见和错误看法<ul><li><strong>汉字是“语素文字”，汉字是“词语文字”</strong>：在创制汉字的时代，汉语的词绝大多数是单音节的，现代的语素在当时绝大多数确实还是地地道道的“词”。现代汉语中多音节词语在绝对数量上已经占了优势，多数的汉字确实代表的只是语素。</li><li><strong>汉字是一种“语素·音节文字”</strong>：汉字本身说不上是表音文字，而且有的汉字代表不止一个音节，一个音节又往往由很多不同的汉字来代表，并且没有什么规律可循，因此汉字似乎不太符合音节文字的基本条件。</li><li><strong>汉字是一种“拼型文字”</strong>：汉字除了一部分构成成分确实可以是单纯字符以外，大部分的笔画也好，部件也好，本身还不能算是字符，而只是字符的图形构件。</li><li><strong>现代汉字仍然是一种“象形文字”</strong>：不科学。全部字符都是象形和表意的文字自古以来就是不存在的。</li></ul></li></ul><h2 id="古往今来话汉字"><a href="#古往今来话汉字" class="headerlink" title="古往今来话汉字"></a>古往今来话汉字</h2><blockquote><p>P116</p></blockquote><p>可以设想汉字最初的情况应该是一个汉字只有一个读音和表示一个意思，事实上现在也仍还有一字一音一义对应的字。但是随着字符越来越多，意义越来越复杂，汉字的形音义就可能交叉和重叠。</p><h1 id="第五讲-语言的建筑材料——语汇"><a href="#第五讲-语言的建筑材料——语汇" class="headerlink" title="第五讲 语言的建筑材料——语汇"></a>第五讲 语言的建筑材料——语汇</h1><h2 id="语汇是语言的建筑材料"><a href="#语汇是语言的建筑材料" class="headerlink" title="语汇是语言的建筑材料"></a>语汇是语言的建筑材料</h2><h3 id="什么是“语汇”"><a href="#什么是“语汇”" class="headerlink" title="什么是“语汇”"></a>什么是“语汇”</h3><blockquote><p>P123</p></blockquote><ul><li>“词”在语法上有专门的定义，即“最小的、有意义的、能独立使用的语言单位”。</li><li>“语”指那些长度相当于语法上的词组或句子，但意义和用法都相对凝固的语言判断，即所谓“固定词组”或“熟语”。</li></ul><h3 id="语汇有什么样的性质和特点"><a href="#语汇有什么样的性质和特点" class="headerlink" title="语汇有什么样的性质和特点"></a>语汇有什么样的性质和特点</h3><blockquote><p>P124 - P126</p></blockquote><ul><li>语汇在<strong>产生</strong>时既有<strong>任意性</strong>的一面，又有<strong>理据性</strong>的一面<ul><li>任意性：任何语言的词语，特别是意义单一的词，发什么音表什么义在初始的阶段是大多是任意的。</li><li>理据性：语言中也有相当多对的词语，特别是“同源词”和“复合词”，其音义联系也会有一定的理据性。</li></ul></li><li>词汇在<strong>表达</strong>上既有<strong>普遍性</strong>的一面，又有<strong>民族性</strong>的一面<ul><li>普遍性：只要客观事物中有某种概念，就一定会有某个相应的词语来表达它，这方面所有的语言都一样。</li><li>民族性：词语往往反映某个民族对事物的独特认识，在这方面不同的语言就可能有明显的差异。</li></ul></li><li>词汇在<strong>变化</strong>中既有<strong>活跃性</strong>的一面，又有<strong>稳定性</strong>的一面<ul><li>活跃性：社会生活的发展变化，都会很快反映到语汇中。</li><li>稳定性：语汇的变化又不是随心所欲的，它要收到社会约定和语汇系统的严格制约。</li></ul></li></ul><h2 id="词语的“类聚关系”"><a href="#词语的“类聚关系”" class="headerlink" title="词语的“类聚关系”"></a>词语的“类聚关系”</h2><blockquote><p>P129</p></blockquote><ul><li>语汇的类聚系统首先应分成“词”和“语”两大类。</li></ul><blockquote><p>P137</p></blockquote><ul><li>汉语中按“复现率”确定的常用词大致也是3000个出头。</li></ul><h2 id="词语构造的“魔方”"><a href="#词语构造的“魔方”" class="headerlink" title="词语构造的“魔方”"></a>词语构造的“魔方”</h2><h3 id="“语素”和“词”是什么关系"><a href="#“语素”和“词”是什么关系" class="headerlink" title="“语素”和“词”是什么关系"></a>“语素”和“词”是什么关系</h3><blockquote><p>P141</p></blockquote><p>“语素”的定义是“最小的有意义的语言单位”。语素的这种“最小”要从音和义相结合的角度来看。</p><blockquote><p>P145</p></blockquote><ul><li>成词语素/不成词语素：某个语素虽然是语素，但有时也可以直接形成为词，称为成词语素，反之为不成词语素。</li><li>自由语素/不自由（黏着）语素：某种语素既可以单独形成词，又可以单独说出来，称为自由语素，反之为黏着语素。</li><li>定位语素/不定位语素：在最小的合成结构中的位置是固定的，或者总是前置，或者总是后置，称为定位语素，反之为不定位语素。</li><li>实义语素/虚义语素：有实在的词汇意义，或者说本身直接负载了词汇意义的语素，称为实义语素，反之为虚义语素。</li></ul><p>例子：</p><ol><li>笔、灯、走、看、好、大、他、谁、蜻蜓、轰隆、雷达、巧克力</li><li>也、再、与、吗、了、着、被、的、忽然、似的</li><li>机、企、体、民、坚、技、迹、性、式、者、员、无、反、泛、半</li><li>-子、-儿、-头、阿-、第-、老-、-乎乎、-滋滋、-咕隆咚、-里叭唧</li></ol><div class="table-container"><table><thead><tr><th>分组</th><th>是否成词</th><th>是否自由</th><th>是否定位</th><th>是否实义</th></tr></thead><tbody><tr><td>1</td><td>成词语素</td><td>自由语素</td><td>不定位语素</td><td>实义语素</td></tr><tr><td>2</td><td>成词语素</td><td>黏着语素</td><td>定位语素</td><td>虚义语素</td></tr><tr><td>3</td><td>不成词语素</td><td>黏着语素</td><td>不定位语素</td><td>实义语素</td></tr><tr><td>4</td><td>不成词语素</td><td>黏着语素</td><td>定位语素</td><td>虚义语素</td></tr></tbody></table></div><h3 id="“词”有哪些构造形式"><a href="#“词”有哪些构造形式" class="headerlink" title="“词”有哪些构造形式"></a>“词”有哪些构造形式</h3><blockquote><p>P147</p></blockquote><ul><li>成词：单个语素直接形成词，可称为单纯词<ul><li>直接成词：语素不改变形式。（例：一、大、了、着、book、go、and、in）</li><li>转化成词：语素改变形式后成为词。（例：to call、bike、pop、laser、雷达）</li></ul></li><li>构词：语素与语素相互结合而构成词<ul><li>语汇构词：语素和语素经过组合构成一个新词。（例：black-board、hypothesis、火车）</li><li>语法构词：语素和语素的结合并不构成新词，但具有一定的语法作用。（例：readers、realize）</li></ul></li></ul><blockquote><p>P151</p></blockquote><p>汉语语素的语法功能与词类没有必然联系。</p><h3 id="“语”有哪些结构特点"><a href="#“语”有哪些结构特点" class="headerlink" title="“语”有哪些结构特点"></a>“语”有哪些结构特点</h3><ul><li>语的总体的结构特点是结构的固定性和整体性<ul><li>结构固定性：语的结构是一种特定的组合形式，大多不能像普通词组那样随意改变，否则即使改变后的结构形式可以成立，也不再是语的格式了。</li><li>结构整体性：表达的意义不一定就是词语的搭配结果，大多不能简单地像词组那样根据组成成分来分析整体的意义，否则即使解释后的意思能够成立也不再是语的意义了。</li></ul></li><li>语的结构基本上就是词组或句子的结构形式，但各自有特殊格式。</li></ul><h1 id="第六讲-语言的结构规则——语法"><a href="#第六讲-语言的结构规则——语法" class="headerlink" title="第六讲 语言的结构规则——语法"></a>第六讲 语言的结构规则——语法</h1><h2 id="语法是组词造句的规则"><a href="#语法是组词造句的规则" class="headerlink" title="语法是组词造句的规则"></a>语法是组词造句的规则</h2><h3 id="什么是语法"><a href="#什么是语法" class="headerlink" title="什么是语法"></a>什么是语法</h3><blockquote><p>P160</p></blockquote><p>现在政府有语言文字部分专门管语言的使用，语言学家也常常对语言中某些不当用法提出意见，但这种工作的主要目的也并不是限制大家怎么说话，而正是要在大多数人语感的基础上对个别混乱现象进行适当的统一和规范，而规范的结果事实上最后也还要服从社会上大多数人的选择。</p><h3 id="语法与其他语言现象有什么关系"><a href="#语法与其他语言现象有什么关系" class="headerlink" title="语法与其他语言现象有什么关系"></a>语法与其他语言现象有什么关系</h3><blockquote><p>P164</p></blockquote><ul><li>“差点摔倒”和“差点没摔倒”意思一样，都是“没摔倒”</li><li>“差点考上”和“差点没考上”意思就不一样，前句是“没考上”，后句是“考上了”。</li></ul><blockquote><p>P165</p></blockquote><p>修辞现象有时也可能影响到语法，即为了修辞需要有时可以超出语法规则的范畴。</p><blockquote><p>P165</p></blockquote><p>“打扫卫生”和“救火”，逻辑上好像不通（“打扫”的不是“卫生”，“救”的也不是“火”），这时也就只能以习惯为准，不能说是错句。</p><h2 id="语法和语法研究的“万花筒”"><a href="#语法和语法研究的“万花筒”" class="headerlink" title="语法和语法研究的“万花筒”"></a>语法和语法研究的“万花筒”</h2><blockquote><p>P166</p></blockquote><ul><li>语法规律：客观上存在的语法，即人们说话时直觉和习惯上所遵守的某种语感。</li><li>语法规则：主观认识的语法，即语言学家对于人们组词造句的语感或习惯进行研究后做出的归纳和说明。</li></ul><blockquote><p>P166</p></blockquote><p>讲语法也<strong>必须</strong>限定某种目的，不可能不分对谁都讲一样的话，比如给中学生讲课说的语法和给机器翻译用的语法各自的要求就肯定不同。</p><blockquote><p>P170</p></blockquote><ul><li>组合规则：从线性的不同位置看都是相互怎么样搭配起来的关系。<ul><li>哪个成分在前哪个成分在后的问题就由“组合规则”来管。</li></ul></li><li>聚合规则：从线性的某个位置看都是相互能不能替换出现的关系。<ul><li>什么样的成分能替换出现在某个组合位置上的问题就由“聚合规则”来管。</li></ul></li></ul><h2 id="“形式”和“意义”：这一面和那一面"><a href="#“形式”和“意义”：这一面和那一面" class="headerlink" title="“形式”和“意义”：这一面和那一面"></a>“形式”和“意义”：这一面和那一面</h2><h3 id="形式和意义是语法这张纸的两面"><a href="#形式和意义是语法这张纸的两面" class="headerlink" title="形式和意义是语法这张纸的两面"></a>形式和意义是语法这张纸的两面</h3><blockquote><p>P175</p></blockquote><p>语法管的形式不是指语言中所有的形式，而只是能体现语法意义的那些形式；语法管的意义也不是指语言中的所有的意义，而只是语法形式所体现的那些意义。</p><ul><li>语法形式：凡是能够体现一类意义或者共同作用的形式，就是语法形式。<ul><li>例：疑问句的语音变化形式、词尾“-s”“-ed”的词形变化形式</li></ul></li><li>语法意义：凡是通过一类形式或共同功能所获得的意义，就是语法意义，反之只属于个别词语或者个别句子的意义就不是语法意义。<ul><li>例：主谓关系的意义、</li></ul></li></ul><h3 id="语法意义可以归纳为哪几个主要的范畴"><a href="#语法意义可以归纳为哪几个主要的范畴" class="headerlink" title="语法意义可以归纳为哪几个主要的范畴"></a>语法意义可以归纳为哪几个主要的范畴</h3><blockquote><p>P183 - P186</p></blockquote><ul><li>“性”范畴是在某些语言中表示人或事物有关性属的一组特征。<ul><li>阳性、中性、阴性</li></ul></li><li>“数”范畴是表示事物数量的一组特征。<ul><li>单数、复数</li></ul></li><li>“格”范畴是表示名词与其他词的语法结构关系的一组特征。<ul><li>主格、宾格、与格、属格</li></ul></li><li>“有定和无定”范畴是表示名字指称特性的一组特征。<ul><li>定冠词、不定冠词</li></ul></li><li>“时”范畴是表示动词所反映的动作发生的时间和说话的时间的关系的一组特征。<ul><li>现在时、过去时、将来时</li></ul></li><li>“体”范畴是表示动词所反映的动作行为进行的状况的一组特征。<ul><li>进行体、完成体、未完成体</li></ul></li><li>“态”范畴是表示动词与主语名词之间施受关系的一组特征。<ul><li>主动态、被动态</li></ul></li><li>“人称”范畴是表示动词与主语名词之间的一致关系的一组特征。<ul><li>单数复数的三个人称发生变化</li></ul></li></ul><h2 id="“聚合”和“组合”：竖着看和横着看"><a href="#“聚合”和“组合”：竖着看和横着看" class="headerlink" title="“聚合”和“组合”：竖着看和横着看"></a>“聚合”和“组合”：竖着看和横着看</h2><blockquote><p>P188</p></blockquote><p>实际上语法单位中最重要的是“词”和“句子”这两个单位。词和句子是既能联系低一级单位又能联系同一级单位的语法单位。</p><blockquote><p>P191</p></blockquote><ul><li>根据词形变化来确定词类</li><li>根据词的意义来确定词类</li><li>根据词的聚合位置来确定词类：从整体上说，汉语的词类划分似乎只能用这种标准</li></ul><blockquote><p>P191</p></blockquote><p>英语一般认为只有九种词类，但汉语差不多有十五种</p><blockquote><p>P197</p></blockquote><ul><li>体词性词组：整个词组的功能相当于体词的词组<ul><li>“微型汽车”</li></ul></li><li>谓词性词组：整个词组的功能相当于谓词的词组<ul><li>“慢慢走”</li></ul></li><li>向心词组：整个词组的功能相当于词组中心语功能的词组<ul><li>“木头房子”、“仔细看”</li></ul></li><li>离心词组：整个词组的功能不等于词组中任何成分的功能的词组<ul><li>“吃的”</li></ul></li></ul><h1 id="第七讲-语言的表达内容——语义"><a href="#第七讲-语言的表达内容——语义" class="headerlink" title="第七讲 语言的表达内容——语义"></a>第七讲 语言的表达内容——语义</h1><h2 id="语义是语言形式表达的内容"><a href="#语义是语言形式表达的内容" class="headerlink" title="语义是语言形式表达的内容"></a>语义是语言形式表达的内容</h2><h3 id="什么是“语义”"><a href="#什么是“语义”" class="headerlink" title="什么是“语义”"></a>什么是“语义”</h3><blockquote><p>P207</p></blockquote><ul><li>语汇意义（词语意义）：由语汇形式表达的语义</li><li>语法意义（范畴意义）：由语法形式表达的语义</li></ul><blockquote><p>P208</p></blockquote><p>一般和稳定的意义是语言形式本身通常所表达的意义，而个别和临时的意义则是语言形式在特定的交际场合和知识背景等音素的作用下才表达出来的意义。</p><p>概括起来说，“语义”主要就是指“语言形式所表达的意义”。而语言的意义又大致可以分为“语汇意义（词语意义）”和“语法意义（范畴意义）”、“语段意义（言内之意）”和“非理性意义（情感意义）”等。广义地说，这些意义当然都是“语义”。</p><h3 id="“语义”有什么样的性质和特点"><a href="#“语义”有什么样的性质和特点" class="headerlink" title="“语义”有什么样的性质和特点"></a>“语义”有什么样的性质和特点</h3><blockquote><p>P209</p></blockquote><p>语言形式所表达的意义都是概括的、一般的。表达概括意义的词语和句子等语言形式能同个别的对象联系在一起，主要是依靠了上下文和交际环境对语言形式所指的限定作用。</p><blockquote><p>P210</p></blockquote><p>“模糊”是指词义所反映的对象只有一个大致的范围，而没有明确的界限。</p><blockquote><p>P212</p></blockquote><p>不同的民族对客观事物的认识会有所不同，对客观事物的概括和分类也会存在着差异，因而不同语言的语义也会有所不同，这就是语义的民族特点。</p><h3 id="在语言系统中如何处理语义问题"><a href="#在语言系统中如何处理语义问题" class="headerlink" title="在语言系统中如何处理语义问题"></a>在语言系统中如何处理语义问题</h3><blockquote><p>P213</p></blockquote><p>在语言研究中，词语的意义，词语搭配的意义，整个句子的意义，才是最重要的；而词语的类别，词组的构造，句子的结构形式，并不太重要，至少形式的地位要低于意义。语言研究中的这种观点可以叫做 <strong>“语义中心说”</strong> 或者 <strong>“语义—句法说”</strong> 。</p><blockquote><p>P214</p></blockquote><p>在语言研究中，词语的类别，词组的构造，句子的结构形式，才是最重要的；而词语的意义，词语搭配的意义，整个句子的意义并不太重要，至少意义的地位要低于形式。语言研究种的这种观点可以叫作 <strong>“句法自治说”</strong> 或者 <strong>“句法—语义说”</strong> 。</p><h2 id="词义构成的各个要素"><a href="#词义构成的各个要素" class="headerlink" title="词义构成的各个要素"></a>词义构成的各个要素</h2><h3 id="“义类”：词语中两种不同类型的意义"><a href="#“义类”：词语中两种不同类型的意义" class="headerlink" title="“义类”：词语中两种不同类型的意义"></a>“义类”：词语中两种不同类型的意义</h3><blockquote><p>P216</p></blockquote><ul><li>“理性意义”：表达人们对主客观世界的事物和现象的反映</li><li>“非理性意义”：表达说话人的主观情感、态度以及语体风格方面的内容。</li></ul><blockquote><p>P217</p></blockquote><p>词义的非理性意义</p><ul><li>词义的感情色彩：褒义色彩、贬义色彩</li><li>词义的语体色彩：口语、书面语</li><li>词义的形象色彩：有内部的组成成分所引起的对事物视觉形象或听觉形象的联想。</li></ul><h3 id="根据词义的异同给词语归类"><a href="#根据词义的异同给词语归类" class="headerlink" title="根据词义的异同给词语归类"></a>根据词义的异同给词语归类</h3><blockquote><p>P225</p></blockquote><p>词有单义发展为多义是有原因的。一方面是因为客观事物之间往往有各种各样的联系，人们在使用词语时就有可能根据客观对象之间的某种联系，用指称甲类对象的词去指称乙类对象，从而产生出与原来的词语意义有联系的新的意义。另一方面当然还因为语言中语音形式总是有限的，而随着社会的发展和人的认识的深化，语言要表达的意义总是不断增加，用数量有限的语音形式去表达数量庞大且不断增加的意义，就必然会出现一个语音形式表达多个意义的现象。</p><blockquote><p>P226</p></blockquote><p>相关性的词义引申方式是“借代”，相似性的词义引申方式是“比喻”。</p><blockquote><p>P227</p></blockquote><p>词语总是在一定的上下文中使用的，特定的上下文会使多义词指表现出一个意义。因为言语交际总是在一定环境中发生的，特定的交际环境也可以使多义词只表现出一个意义。</p><h2 id="句子的意义和句子的语义分析"><a href="#句子的意义和句子的语义分析" class="headerlink" title="句子的意义和句子的语义分析"></a>句子的意义和句子的语义分析</h2><blockquote><p>P245 - P246</p></blockquote><ul><li>口头歧义：“明天期中（终）考试”</li><li>书面歧义<ul><li>语汇歧义：词语形式相同而意义不同噪声的歧义。“他<strong>原来</strong>住这里”</li><li>组合歧义<ul><li>语法结构歧义：语言片段序列相同而结构不同造成的歧义。“两个外语学院的学生”</li><li>语义结构歧义：“鸡不吃了”、“反对的是少数人”</li></ul></li></ul></li></ul><h1 id="第八讲-语言的运用特点——语用"><a href="#第八讲-语言的运用特点——语用" class="headerlink" title="第八讲 语言的运用特点——语用"></a>第八讲 语言的运用特点——语用</h1><h2 id="语境与语句意义和话语结构"><a href="#语境与语句意义和话语结构" class="headerlink" title="语境与语句意义和话语结构"></a>语境与语句意义和话语结构</h2><blockquote><p>P253 - P254</p></blockquote><p>大致说来，语境可以分为两个大类：一类就是指话语中内部的语境，包括书面语（单独语流）中的前言后语或者口语（对话语流）中的你一言我一语：这可以叫作语言语境或狭义的语境。另一类则是指话语外部各种影响话语表达和理解的因素，包括说话的时间、地点、人物、场景等外部环境，甚至也包括社会文化背景等环境：这可以叫作非语言语境或广义的语境。</p><blockquote><p>P256</p></blockquote><p>并不跟语境中实际对象联系，而是跟上句或本句中的某个成分（先行语）所指相同。这种所指关系不叫指示，而叫作“照应”。</p><h2 id="会话准则和会话含义"><a href="#会话准则和会话含义" class="headerlink" title="会话准则和会话含义"></a>会话准则和会话含义</h2><blockquote><p>P263 - P264</p></blockquote><ul><li>合作原则<ul><li>真实原则：要说真话，不说假话和无根据的话。</li><li>适量原则：提供的信息要适量，不多也不少。</li><li>相关准则：要说跟话题有关的话，不说无关的话。</li><li>方式准则：说话要清楚明了，简洁有条理和没有歧义。</li></ul></li><li>礼貌原则<ul><li>得体和慷慨：尽量减少表达中有损于他人和有利于自己的观点。</li><li>赞誉和谦逊：尽量少赞誉自己多赞誉对方。</li><li>一致和同情：尽量缩小与对方的分歧和对立，增加相互的一致性和共同点。</li></ul></li></ul><blockquote><p>P267</p></blockquote><p>语言学家列文森提出了会话含义的一个推导模式：如果说话人 S 说的话 P 要表达会话含义 Q，那么 S 应知道会话双方都会遵守合作原则以及都知道违反合作原则能导致含义 Q，因此说话人才故意说 P，而听话人也就可以从 P 推知 Q。</p><h1 id="第九讲-语言的发展和变化"><a href="#第九讲-语言的发展和变化" class="headerlink" title="第九讲 语言的发展和变化"></a>第九讲 语言的发展和变化</h1><blockquote><p>P292</p></blockquote><p>在一个语言社会中不同的人说话可能会使用不同的语言特征而呈现出不同的特点，这些不同的语言变体又同说话人的社会特征相关联，同一定的社群联系在一起。</p><h1 id="第十讲-语言的规划和规范"><a href="#第十讲-语言的规划和规范" class="headerlink" title="第十讲 语言的规划和规范"></a>第十讲 语言的规划和规范</h1><blockquote><p>P334</p></blockquote><p>事实上物极必反，由于这种网络文字中毕竟有一些是不符合语言文字发展规律的，当全社会大多数人最终都不接受或不愿意用这种语言表达方式时，这种网络文字也很可能只不过是昙花一现的东西。</p>]]></content>
    
    
    <categories>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2021s12-机器翻译的自动评估</title>
    <link href="/posts/2021s12/"/>
    <url>/posts/2021s12/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>（把每双周周报在BLOG上整理一下，以防止自己做的PPT最后都想不起来是在说什么。）</p></blockquote><p>这次周报的题目非常的别扭，可能是我翻译的不太好。英文的话应该是 Automatic Evaluation of Machine Translation ，也就是对机器翻译的结果给出一个自动的评价或者评分，用来对比哪一个机器翻译模型的性能更好。当然，自动也是题目中非常重要的一部分。基于人工的评价自然无法应付现在大规模的评估需求。</p><span id="more"></span><p>机器翻译是指实现一种语言到另一种语言的自动翻译。<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="宗成庆. 统计自然语言处理[M]. 清华大学出版社, 2013.">[1]</span></a></sup> 该系统的输入是语言 A 的一个文本段落。输出是语言 B 的段落。这两个段落都是不定长的文本序列。对于同一句话可能存在多种合理的翻译，也可能存在明显不合理的翻译。例如下图中的最后一个翻译，把 docter 错翻译成博士，显然是不符合原义的。</p><blockquote><p><del>（阿米娅：博士，您还有许多事情需要处理。现在还不能休息哦。）</del></p></blockquote><p><img src="./traneg.png" alt="翻译例子"></p><p>那么在这么多翻译中，如何评价它们，得出一个更好的翻译呢？这就需要对翻译进行评估，或者称评价。对于如何做好翻译，有大家耳熟能详的严复提出的提出译事三大难：<strong>信</strong>、<strong>达</strong>、<strong>雅</strong>。在 Hovy 等人的论文<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hovy E H . Toward Finely Differentiated Evaluation Metrics for Machine Translation[J]. proceedings of the eagles workshop on standards & evaluation.pisa italy.international standards for language engineering, 1999.">[2]</span></a></sup>中也提出了类似的观点：好的翻译要追求<strong>充分性</strong>（adequacy）、<strong>准确性</strong>（fidelity）和<strong>流畅性</strong>（fluency）。例如在上面的句子中，除了我的翻译无法做到正确之外，有道的翻译则更加流畅，可以认为在这句话上的翻译更好。</p><p>上述需要人类参与、使用人类对语言的理解的评估可以成为<strong>人工评估</strong>，相对的如果可以计算地得出评估结果，这种可以称为<strong>自动评估</strong>。显然，人工评估不仅费时费力，而且为了弥补人类的不客观性，需要采纳大量人类的结果，显然无法为现今大规模的翻译模型测试提供帮助。由此对自动评估的期望越来越高。对于一个自动评估方案来讲，其通常需要引入由人类提供的<strong>正确答案</strong>（Ground Truth / Reference）。对模型输出的<strong>候选结果</strong>（Candidate）与正确答案进行可重复性的计算，为每个候选结果提供一个表示其翻译质量的分数。自动评估不仅成本低、速度快，而且可以重复，为翻译模型间相互比较提供了可能。</p><h1 id="传统自动评估方法"><a href="#传统自动评估方法" class="headerlink" title="传统自动评估方法"></a>传统自动评估方法</h1><h2 id="BLEU（The-Bilingual-Evaluation-Understudy）"><a href="#BLEU（The-Bilingual-Evaluation-Understudy）" class="headerlink" title="BLEU（The Bilingual Evaluation Understudy）"></a>BLEU（The Bilingual Evaluation Understudy）</h2><p>BLEU<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Papineni S . Blue ; A method for Automatic Evaluation of Machine Translation[C]// Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2002.">[3]</span></a></sup> 是比较经典的自动评估方法。这个词和蓝色（BLUE）比较像，但看在原论文特意把这四个字母写成蓝色，我觉得读音应该一致。此外经过搜索，BLEU在法语里就是蓝色的意思。</p><p><img src="./bleu_tran.png" alt="bleu的翻译"></p><p>BLEU主要分为两部分：n元准确度修改版（Modified $n$-gram precision）、简短惩罚系数（Brevity Penalty）。下面以一个候选句c和多个参考句R为输入。</p><h3 id="修改的n元准确度"><a href="#修改的n元准确度" class="headerlink" title="修改的n元准确度"></a>修改的n元准确度</h3><script type="math/tex; mode=display">p_n = \frac{\sum_{n\text{-gram}_\text{uniq} \in c} \text{Count}_\text{clip}(n\text{-gram}_\text{uniq}, R)}{\sum_{n\text{-gram}_\text{uniq}' \in c} \text{Count}(n\text{-gram}_\text{uniq}', c)}</script><p>该公式计算的是准确率（相对于召回率），统计的是<strong>候选句$c$</strong>中出现在<strong>句子$R$</strong>中的$n$-gram数量占<strong>候选句$c$</strong>中所有$n$-gram数量的比例。但与普通的准确率不同的是，BLEU对分子部分进行裁切，使同一个n-gram的匹配数量得到限制。先看公式。</p><script type="math/tex; mode=display">\text{Count}_\text{clip}(n\text{-gram}, R) = \min(\text{Count}(n\text{-gram}, c), \max_{r \in R}(\text{Count}(n\text{-gram}, r)))</script><p>其中：</p><script type="math/tex; mode=display">\text{Count}(n\text{-gram}, w)= \sum_{n\text{-gram}'\in w} I(n\text{-gram}, n\text{-gram}')</script><script type="math/tex; mode=display">I(a,b) = \begin{cases}1 \quad \text{if} \ a = b \\0 \quad \text{else}\end{cases}</script><p>该公式说明的是每一个$n$-gram，都不能超过参考句中匹配数量最大的一项。这样说很抽象，以下面的例子作为参考。</p><p><img src="./bleueg1.png" alt="n元准确度的例子"></p><p>我们计算这个例子的1元准确度。可以发现 the 在参考句 1 和参考句 2 中分别出现了 2 次和 1 次。那么 $1$-gram 在其中的最大匹配数为 2. 因此最终的1元准确率为 $\frac{2}{7}$.</p><script type="math/tex; mode=display">\text{Count}_\text{clip}(\text{[the]}, R) = \min(7, \max(2, 1)) = 2</script><script type="math/tex; mode=display">p_1 = \frac{2}{7}</script><h3 id="简短惩罚系数"><a href="#简短惩罚系数" class="headerlink" title="简短惩罚系数"></a>简短惩罚系数</h3><p>上述准确度的不足显然是：如果<strong>候选句$c$</strong>过短，且其中$n$-gram完全匹配，那么这个候选句的准确度为满分。为了对这样的候选句进行惩罚，引入简短惩罚系数。</p><script type="math/tex; mode=display">\text{BP}(c, R) = \begin{cases}1  & \text{len}(c) > \text{len}(r^*) \\\exp(1 - \frac{\text{len}(r^*)}{\text{len(c)}}) & \text{len}(c) \leq \text{len}(r^*)\end{cases}</script><p>其中：</p><script type="math/tex; mode=display">r^* = \arg \min_{r \in R} | \text{len}(r) - \text{len}(c)|</script><p>关于候选句的参考，有两种方案。上面公式给出的是 IBM 方案，即选择与候选句长度差距最小的一个参考句。当候选句小于参考句时，惩罚系数会逐渐降低。</p><p>最终 BLEU 评分将两部分相乘得到。</p><script type="math/tex; mode=display">\text{BLEU}(c, R) = \text{BP}(c, R) \cdot \exp \left( \sum_{n=1}^N w_n \log p_n\right)</script><p>其中：</p><script type="math/tex; mode=display">w_n = \frac{1}{N}</script><p>右侧的求和式意为对多个不同大小的$n$-gram取平均。</p><h3 id="BLEU的缺点"><a href="#BLEU的缺点" class="headerlink" title="BLEU的缺点"></a>BLEU的缺点</h3><ul><li>缺少对<strong>召回率</strong>的考量。虽然用惩罚系数的方式进行了弥补，但采用准确率和召回率的调和平均值分数显然更具优势。</li><li>对仅含有一个参考句的数据集效果较差。若只含有一个参考句，则削弱了 BLEU 的性能。</li><li>缺少对同义词的考量。BLEU 仅通过完全匹配的方式给出得分，包容度更低。</li></ul><h2 id="METEOR-（Metric-for-Evaluation-of-Translation-with-Explicit-Ordering）"><a href="#METEOR-（Metric-for-Evaluation-of-Translation-with-Explicit-Ordering）" class="headerlink" title="METEOR （Metric for Evaluation of Translation with Explicit Ordering）"></a>METEOR （Metric for Evaluation of Translation with Explicit Ordering）</h2><p>METEOR<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Satanjeev B . METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments[J]. ACL-2005, 2005:228-231.">[4]</span></a></sup> 是一个基于对齐的一种评分方法，在一定程度上弥补了 BLEU 的不足。但该评分方法较为复杂，还需要引用外界知识。毕竟要考量同义词的话，外界知识是必不可少的。下面以一个候选句$c$和一个参考句$r$为输入。</p><h3 id="对齐（Alignment）"><a href="#对齐（Alignment）" class="headerlink" title="对齐（Alignment）"></a>对齐（Alignment）</h3><p>对齐，或者说连线？意在将候选句与参考句中的n-gram对应起来，对应的越多则越接近。</p><p>对齐也分为两步：<strong>映射</strong>和<strong>选择</strong>。</p><h4 id="映射（Mapping）"><a href="#映射（Mapping）" class="headerlink" title="映射（Mapping）"></a>映射（Mapping）</h4><ul><li>精确模式（exact）：要求词与词之间完全对应，不允许词形变化。例如 computer 只能与 computer 形成映射。</li><li>词根模式（porter stem）：两个词的词根相同也允许建立映射，需要使用一些外部的词根化工具。例如 computer - compters.</li><li>同义词模式（WN synonymy）：两个词的意思相同即可，需要使用一些外部的同义词词表。例如：computer - PC.</li></ul><p>选用上述的某一个规则，在候选句与参考句中间构成映射。映射可以不完全覆盖候选句或参考句中的所有n-gram。但实际上仅满足上述规则的的话，会存在多个满足条件的映射。因此需要对映射进行选择。</p><h4 id="选择（Select）"><a href="#选择（Select）" class="headerlink" title="选择（Select）"></a>选择（Select）</h4><ul><li>选择已匹配条目最多的映射（the largest subset）</li><li>选择交叉最少的映射（the least number of crosses）</li></ul><p>根据上述的全部规则，就可以选择出最佳映射了。关于交叉这一概念，论文中进行了形式化表达。我们可以简单理解为两条映射所对应的词出现顺序相反。若画成词与词链接的图，可以发现两条线段有一个交点。</p><p><img src="./aligneg.png" alt="选择的例子"></p><p>我们以上图<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wikipedia">[5]</span></a></sup>为例，r为参考句，c为候选句。左图的交叉数为7，右图为11. 因此选择左图的映射方法。</p><p>原论文中，以上的两步将会循环进行三次。其中映射规则分别使用精确模式、词根模式、同义词模式。每一轮尝试对<strong>未被映射的词</strong>构建映射。这三种映射规则逐渐宽松，也更容易将所有存在可能的词映射起来，为下一步的计算提供可能。</p><h3 id="计算（Compute）"><a href="#计算（Compute）" class="headerlink" title="计算（Compute）"></a>计算（Compute）</h3><p>计算部分也分为两部分：F均值与惩罚系数。其中的惩罚系数比较复杂，而 F均值较好理解。</p><h4 id="F均值（Fmean）"><a href="#F均值（Fmean）" class="headerlink" title="F均值（Fmean）"></a>F均值（Fmean）</h4><p>F均值是在判断分类任务中很常用的指标，综合考量了准确率与召回率。这里使用了<strong>召回率</strong>权重很高的F均值。</p><script type="math/tex; mode=display">F_\text{mean} = \frac{10PR}{R+ 9P}</script><p>其中：</p><script type="math/tex; mode=display">P= \frac{\sum_{n\text{-gram} \in c} I(n\text{-gram}\text{ is aligned})}{\sum_{n\text{-gram} \in c}1}</script><script type="math/tex; mode=display">R= \frac{\sum_{n\text{-gram} \in c} I(n\text{-gram}\text{ is aligned})}{\sum_{n\text{-gram} \in r}1}</script><p>准确率的分母为候选句的n-gram数量，召回率的分母是参考句的n-gram数量。</p><h4 id="惩罚系数（Penalty）"><a href="#惩罚系数（Penalty）" class="headerlink" title="惩罚系数（Penalty）"></a>惩罚系数（Penalty）</h4><p>这里的惩罚系数是为了评价前面对齐的情况。即使候选句中所有单词都来自于参考句，F评分为1，但顺序与参考句完全不一致，也无法获得高分数。</p><script type="math/tex; mode=display">\text{Penalty} = 0.5 \times \left( \frac{\text{chunks}}{n\text{-gram_matched}} \right) ^3</script><p>其中分子位置的$\text{chunks}$表示在候选句$c$和参考句$r$中都连续的块数。分母部分是最终匹配的n-gram数。将两部分结合到一起就为METEOR分数。用下面的例子<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wikipedia contributors.METEOR[DB/OL].https://en.wikipedia.org/w/index.php?title=METEOR&oldid=962913935,2020-6-16.">[6]</span></a></sup>解释惩罚系数的作用。</p><p><img src="./meteoreg.png" alt="METEOR示例"></p><p>比较示例1与示例2，虽然它们的F均值均为1，但示例2对齐的更好，所以块数较少，惩罚系数更小。</p><p>最终的METEOR分数为下式。</p><script type="math/tex; mode=display">\text{METEOR}(c, r) = F_\text{mean} \times (1- \text{Penalty})</script><h1 id="基于预训练的自动评估方法"><a href="#基于预训练的自动评估方法" class="headerlink" title="基于预训练的自动评估方法"></a>基于预训练的自动评估方法</h1><p>还有一类方法就是基于预训练的自动评估方法，其思路主要利用BERT的综合分析文本上下文的能力，从而完成对翻译的自动评估。但多少有一种用规则内的东西证明规则内的东西，多少是有极限的感觉。</p><h2 id="BERTScore"><a href="#BERTScore" class="headerlink" title="BERTScore"></a>BERTScore</h2><p>BERTScore<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang T ,  Kishore V ,  Wu F , et al. BERTScore: Evaluating Text Generation with BERT[J].  2019.">[7]</span></a></sup> 顾名思义是一个基于 BERT 的自动评估方法。其出发点为让候选句和参考句中的每一个词，分别携带其上下文信息。由此得到的词向量再进行相似度计算，得到最终分数。从方法上讲，属于基于<strong>词向量</strong>的自动评估方法。只不过这里的词向量信息更加丰富。</p><p><img src="./bertscore.png" alt="BERTScore"></p><p>BERTScore中也使用了F均值，其召回率和准确率定义如下。</p><script type="math/tex; mode=display">R_\text{BERT} = \frac{1}{|r|} \sum_{r_i \in r} \max_{c_j \in c} \boldsymbol{r}^\text{T}_i \boldsymbol{c}_j</script><script type="math/tex; mode=display">P_\text{BERT} = \frac{1}{|c|} \sum_{c_j \in c} \max_{r_i \in r} \boldsymbol{r}^\text{T}_i \boldsymbol{c}_j</script><h2 id="BLEURT（Bilingual-Evaluation-Understudy-with-Representations-from-Transformers）"><a href="#BLEURT（Bilingual-Evaluation-Understudy-with-Representations-from-Transformers）" class="headerlink" title="BLEURT（Bilingual Evaluation Understudy with Representations from Transformers）"></a>BLEURT（Bilingual Evaluation Understudy with Representations from Transformers）</h2><p>这个模型<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sellam T ,  Das D ,  Parikh A P . BLEURT: Learning Robust Metrics for Text Generation[J]. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.">[8]</span></a></sup>在结构上更加简单粗暴，但该方法的重点在于训练这个 BERT 模型。该模型将候选句和参考句直接输入进 BERT，在 [CLS] 位置的输出上添加一个全连接层得到分数。</p><p><img src="./bleurt.png" alt="BLEURT的训练任务"></p><p>训练任务如下：</p><ul><li>BLEU：输入为Wikipedia的句子对，直接拟合BLEU的分数。</li><li>ROUGE：输入为Wikipedia的句子对，直接拟合ROUGE的分数。</li><li>BERTScore：输入为Wikipedia的句子对，直接拟合BERTScore的分数。<del>(用BERT拟合BERT)</del></li><li>回译似然度：概括来说就是产生回译数据，将Transformers的损失作为评分，然后拟合这个分数。<del>(用BERT拟合Transformers)</del></li><li>包含关系：使用包含关系数据集训练分类任务。</li><li>回译标签：判断数据是否是由回译产生的。</li></ul><p>总的来说这篇文章定义了一堆关于翻译度量的预训练任务。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>翻译的自动评估原本以为会是一个比较偏向语言学和数学的领域，但也好像理所当然的走到了预训练模型这条路上。虽然说这些复杂的自动评估方法性能很好，但在各种机器翻译论文中 BLEU 依旧在使用。可能还是因为 BLEU 虽然性能一般，但也比较方便。外加机器翻译论文通常使用多个指标同时比较，某一个指标的性能有倾向性，多个指标放在一起也就没有太多影响了。</p><p>END</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">宗成庆. 统计自然语言处理[M]. 清华大学出版社, 2013.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hovy E H . Toward Finely Differentiated Evaluation Metrics for Machine Translation[J]. proceedings of the eagles workshop on standards &amp; evaluation.pisa italy.international standards for language engineering, 1999.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Papineni S . Blue ; A method for Automatic Evaluation of Machine Translation[C]// Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2002.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Satanjeev B . METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments[J]. ACL-2005, 2005:228-231.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wikipedia<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wikipedia contributors.METEOR[DB/OL].https://en.wikipedia.org/w/index.php?title=METEOR&amp;oldid=962913935,2020-6-16.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang T ,  Kishore V ,  Wu F , et al. BERTScore: Evaluating Text Generation with BERT[J].  2019.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sellam T ,  Das D ,  Parikh A P . BLEURT: Learning Robust Metrics for Text Generation[J]. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.<a href="#fnref:8" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
      <category>周报</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Note of CG4MulRCaS</title>
    <link href="/posts/CG4MulRCaS/"/>
    <url>/posts/CG4MulRCaS/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>本文是 ACL 2019 <strong>Cognitive Graph for Multi-Hop Reading Comprehension at Scale</strong> 的阅读笔记，说是笔记，应该会大量翻译原文的内容。为了强迫自己把某些不知道什么原因每次看到一半就不想看了以至于需要重新看的论文看完，会简单写一个笔记。</p><p>顺带一提，本文中的脚注会按照原论文进行编号。为加以区别，我自己的脚注会从 51 开始编号。</p><span id="more"></span><h1 id="Author-amp-Abstract"><a href="#Author-amp-Abstract" class="headerlink" title="Author &amp; Abstract"></a>Author &amp; Abstract</h1><blockquote><ul><li><p>Author:  MingDing, ChangZhou, QibinChen ,HongxiaYang, JieTang</p><p>Department of Computer Science and Technology, Tsinghua University</p><p>DAMO Academy, Alibaba Group </p></li><li><p>Link: <a href="https://arxiv.org/abs/1905.05460">arXiv: 1905.05460</a></p></li></ul></blockquote><p>我们在互联网规模的文档的多跳问答 (Multi-hop question answering) 方面，提出了一个新的 CogQA 框架。基于认知科学的双过程理论，该框架在<strong>隐含抽取模块</strong> (System 1) 和<strong>具体抽取模块</strong> (System 2) 的相互作用的迭代过程中，逐渐建立一个 <em>认知图</em> 。除了提供准确的答案，我们的框架进一步提供可解释的推理路线。具体的说，我们的实现<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="在 [https://github.com/THUDM/CogQA](https://github.com/THUDM/CogQA) 上获取代码">[1]</span></a></sup>基于 BERT 和 图神经网络 (GNN) 有效地解决了在 HotpotQA 全维基数据集中，百万计的多跳推理问题，获得了排行榜中最高的联合 $F_1$ 分数：34.9，最高竞争者的分数是23.6。<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://hotpotqa.github.io](https://hotpotqa.github.io) 2019年3月4日">[2]</span></a></sup></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><blockquote><p>这里就写一些要点了 = = 全篇翻译可能也没有太大用处。</p></blockquote><ul><li>在单文章的问答方面，深度学习模型取得了超过人类水平的成绩。</li><li>但我们还面临以下三大挑战：推理能力、可解释性、可扩展性<ul><li>推理能力：多跳问答</li><li>可解释性：HotpotQA 要求 <em>无序</em>、<em>句子级别</em> 的可解释性，而人类可以做到<em>有序</em>、<em>实体级别</em> 的可解释性</li><li>可扩展性：与人类通过大容量内存中的知识进行推理的能力相比，该框架是单段问题解答和可伸缩信息检索之间的简单折衷。（长难句翻译？）</li></ul></li><li>参考人类的认知过程：<em>双过程理论</em> ，构建两个系统。</li><li>我们构建的 CogQA 也分为两个系统：<ul><li>System 1 在多个段落中抽取与问题相关的<strong>实体</strong>与<strong>候选答案</strong>，并对其语义信息进行编码。</li><li>System 2 对实体图进行推理，收集线索并指导 System 1 更好地提取下一跳的实体。</li></ul></li><li>两系统迭代工作，直到<strong>所有</strong>的可能答案被发现。</li></ul><h1 id="2-Cognitive-Graph-QA-Framework"><a href="#2-Cognitive-Graph-QA-Framework" class="headerlink" title="2 Cognitive Graph QA Framework"></a>2 Cognitive Graph QA Framework</h1><blockquote><p>这里加上那个算法有点没看懂，所以就翻译全文了。</p><p>（u1s1，谷歌翻译得比我好多了。）</p><p>（u2s2， 伪代码是什么鬼语言 = =）</p></blockquote><p>人类的推理能力严格依靠信息的关系结构。我们直观地采用<strong>有向图</strong>结构，在多条问答的认知过程中一步步的推理和探索。在我们的阅读理解设置中，认知图 $\mathcal{G}$ 的每一个节点与一个<strong>实体</strong>或一个<strong>可能的答案</strong> $x$ 相关，也可以互换的表示成节点 $x$<sup id="fnref:51"><a href="#fn:51" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="我觉得这里的 $x$ 即指的是**实体**，又指的是**可能的答案**。大概是这两个东西共用了一个符号。">[51]</span></a></sup>。  抽取模块 System 1 阅读实体 $x$ 的介绍性段落 $\mathrm{para}[x]$ 并在段落中抽取 <em>候选答案</em> 与有用的 <em>下一跳实体</em>  。 然后，用这些新节点扩展 $\mathcal{G}$ ，为推理模块 System 2 提供显式结构。在这篇文章中，我们假设 System 2 由通过计算节点的隐状态 $\boldsymbol{\mathrm{X}}$ 基于深度学习的学习，而不是基于规则的推理。因此 System 1 也需要在抽取文本片段时，将 $\mathrm{para}[x]$ 归纳成语法向量作为初始隐状态。然后 System 2 基于图结构更新 $\boldsymbol{\mathrm{X}}$ ，作为下游预测的推理结果。</p><p>由于认知图中有明确的推理路径，因此具有可解释性。<sup id="fnref:52"><a href="#fn:52" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="via 谷歌翻译">[52]</span></a></sup> 不仅对于简单的路径，认知图也可以清楚地显示联合或循环的推理过程，其中新的前节点可能带来关于答案的新的 $\mathrm{clues}$。 在我们的框架中，$\mathrm{clues}$ 是一个形式灵活的概念，它指的是来自前节点的信息，用来指导 System 1 更好的抽取文本片段。除了<strong>新增加的节点</strong>，由于有新的线索，那些具有<strong>新的入边的节点</strong>也需要重新访问。我们把它们都称作 <em>前节点</em>。</p><p>可扩展性意味着问答的时间消耗不会随着文章数量的增加显著地增长。我们的框架可以自然地缩放，因为唯一参照所有文章的操作是通过他们的标题索引获得一些特定的段落。对于多跳的问题，传统的检索抽取框架可能会牺牲<sup id="fnref:53"><a href="#fn:53" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="sacrifice ">[53]</span></a></sup>后续模型的潜力，因为距离问题跳跃多次的段落可能与该问题共享更少的相同词和更少的语法关系，导致检索失败。然而，在我们的框架中，通过迭代扩展 $\mathrm{clues}$ 可以发现这些文章。</p><p><strong>算法 1 </strong>描述了我们的框架 CogQA 的流程。在初始化之后，将开始一个图扩展与推理的迭代过程。在每一步我们访问一个前节点 $x$ ，System 1 在 $\mathrm{clues}$ 和问题 $Q$ 的指导下阅读 $\mathrm{para}[x]$ ，抽取文本片段并生成语法向量 $\mathrm{sem}[x, Q, \mathrm{clues}]$。同时，System 2 更新隐状态 $\boldsymbol{\mathrm{X}}$ 并为后节点 $y$ 准备 $\mathrm{clues}[y, \mathcal{G}]$ 。最终预测将基于 $\boldsymbol{\mathrm{X}}$。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">算法1：认知图问答<br>Input：<br>System 1 模型 S1, System 2 模型 S2,<br>问题 Q, 预测器 F, 维基数据集 W<br>Initialize 认知图 G with 被提到的实体 in Q and mark them &quot;frontier nodes&quot;<br>repeat<br>pop 一个节点 x from &quot;frontier nodes&quot;<br>collect clues[x, G] from x 的前节点<br>    // eg. clues可以是提到 x 的句子<br>    fetch para[x, G] in W if any<br>    generate sem[x, Q, clues] with S1 // 初始化 X[x]<br>    if x is &quot;hop node&quot; then<br>    find &quot;hop spans&quot; and &quot;answer spans&quot; in para[x] with S1<br>    for y in &quot;hop spans&quot; do<br>    if y not in G and y in W then<br>    create &quot;hop node&quot; for y<br>    if y in G and edge(x, y) not in G<br>    add edge(x, y) to G<br>    mark y as &quot;frontier nodes&quot;<br>    end<br>    for y in &quot;answer spans&quot; do<br>    add new &quot;answer node&quot; for y and edge(x, y) to G<br>    end<br>    end<br>    update 隐状态 X with S2<br>until G 中没有 &quot;frontier nodes&quot; or G 足够大<br>return argmax F（X[x]）<br></code></pre></td></tr></table></figure><h1 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3 Implementation"></a>3 Implementation</h1><blockquote><p>这部分主要 算法 + 长难句的谷歌翻译 吧。</p></blockquote><ul><li>使用 BERT 作为 System 1</li><li>使用 GNN 作为 System 2</li><li>$\mathrm{clues}[x, \mathcal{G}]$ 是 $x$ 前节点对应的段落中 抽取出 $x$ 的句子。<sup id="fnref:54"><a href="#fn:54" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="$\mathrm{clues}[x, \mathcal{G}]$ are sentences in paragraphs of $x$'s predecessor nodes, from which $x$ is extracted.">[54]</span></a></sup> 并使用<strong>原始句子</strong>作为 $\mathrm{clues}$ 而不是隐状态。</li></ul><p><img src="./figure2.png" alt="图2" title="图2"></p><h2 id="3-1-System-1"><a href="#3-1-System-1" class="headerlink" title="3.1 System 1"></a>3.1 System 1</h2><p>输入模式：</p><script type="math/tex; mode=display">\underbrace{\mathrm{[CLS]} \  \mathrm{Question} \ \mathrm{[SEP]} \ \mathrm{clues}[x, \mathcal{G}] \ \mathrm{[SEP]}}_{\text{Sentence}\  A} \ \underbrace{\mathrm{para}[x]}_{\text{Sentence B}}</script><p>输出模式：</p><script type="math/tex; mode=display">\boldsymbol{\mathrm{T}} \in \mathbb{R}^{L \times H}</script><p>其中 $L$ 是输入<strong>序列</strong>的长度，$H$是隐状态的维度。</p><p>但对于答案节点 $x$ 而言，$\mathrm{para}[x]$可能不存在。因此我们只使用 “Sentence A” 计算 $\mathrm{sem}[x, Q, \mathrm{clues}]$. 之后当我们抽取距离问题一跳的节点初始化 $\mathcal{G}$ 时，我们不计算语义向量，输入中只存在 $\mathrm{Question}$ 部分。<sup id="fnref:55"><a href="#fn:55" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="And when extracting 1-hop nodes from question to initialize $\mathcal{G}$ , we do not calculate semantic vectors and only the $\mathrm{Question}$ part exists in the input. 我也没看懂这句话它想表达什么。">[55]</span></a></sup></p><h3 id="Span-Extraction"><a href="#Span-Extraction" class="headerlink" title="Span Extraction"></a>Span Extraction</h3><p><strong>答案</strong>与<strong>下一跳实体</strong>有不同的属性。答案抽取很大程度上依赖于问题指出的字符。下一跳实体经常是 其描述与问题中的声明 相匹配的实体。因此计算两者的文本片段时，使用不用的可训练参数。</p><p>答案文本片段抽取：</p><script type="math/tex; mode=display">\mathrm{P}_{\text{ans}}^{\text{start}} = \frac{e^{\boldsymbol{\mathrm{S}}_\text{ans} \cdot \boldsymbol{\mathrm{T}}} }{\sum_j e^{\boldsymbol{\mathrm{S}}_\text{ans} \cdot \boldsymbol{\mathrm{T}}_j}} = \mathrm{Softmax}(\boldsymbol{\mathrm{S}}_\text{ans} \cdot \boldsymbol{\mathrm{T}})</script><script type="math/tex; mode=display">\mathrm{end}_k = \underset{\mathrm{start}_k \leq j \leq \mathrm{start_k} + \mathrm{maxL} }{\arg \max} \mathrm{P}_{\text{ans}}^{\text{end}}[j]</script><p> 为了识别不相关的段落，使用负采样训练 System 1 生成负阈值。在前 $K$ 个文本片段中，起始概率小于负阈值的会被丢弃。这里使用 $\mathrm{P}_{\text{ans}}^{\text{start}}[0]$ 作为阈值。</p><h3 id="Semantics-Generation"><a href="#Semantics-Generation" class="headerlink" title="Semantics Generation"></a>Semantics Generation</h3><p>使用<strong>第三层</strong>到<strong>最后一层</strong> 0 位置的隐状态作为$\mathrm{sem}[x, Q, \mathrm{clues}]$</p><h2 id="3-2-System-2"><a href="#3-2-System-2" class="headerlink" title="3.2 System 2"></a>3.2 System 2</h2><ul><li>Function 1 要为前节点准备 $\mathrm{clues}[x, \mathcal{G}]$ ，这里我们使用提及 $x$ 的原始句子。</li><li><p>Function 2 更新隐状态 $\boldsymbol{\mathrm{X}} \in \mathbb{R}^{n \times H}$。</p><p>GNN 更新公式：</p><script type="math/tex; mode=display">\begin{aligned}\mathrm{\Delta} &= \sigma((AD^{-1})^{T} \sigma(\boldsymbol{\mathrm{X}}W_1)) \\\boldsymbol{\mathrm{X}}' &= \sigma(\boldsymbol{\mathrm{X}}W_2 + \Delta) \end{aligned}</script><p>其中，$W_1, W_2 \in \mathbb{R}^{H \times H}$ ，$A$ 是图 $\mathcal{G}$ 的邻接矩阵。详情可能需要看这篇论文。<a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a>（GCN可能要单独写一篇。）</p></li></ul><p>在实验中，我们观察到这种“异步更新” 与 在 $\mathcal{G}$ 最终确定之后，通过多个步骤一起更新所有节点的 $\boldsymbol{\mathrm{X}}$，性能上没有表现出明显差异，后者实践中更为有效和被采用。（长难句翻译）</p><h2 id="3-3-Predictor"><a href="#3-3-Predictor" class="headerlink" title="3.3 Predictor"></a>3.3 Predictor</h2><ul><li>HotpotQA 分三部分：<em>特殊</em> 问题、<em>替代</em> 问题、<em>一般</em> 问题<sup id="fnref:56"><a href="#fn:56" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="我觉得这个翻译不太行。原文： *special* question, *alternative* question and *general* question">[56]</span></a></sup></li></ul><p>特殊问题是最普遍的情况。使用两层全连接网络作为预测器 $\mathcal{F}$</p><script type="math/tex; mode=display">\text{answer} = \underset{\text{answer node}\  x}{\arg \max} \mathcal{F}(\boldsymbol{\mathrm{X}}[x])</script><p>替代问题与一般问题d都是比较两个特定实体，给出实体名字或者 yes or no。因此<strong>另</strong>使用单独的两个全连接网络作为二分类器。</p><h2 id="3-4-Training"><a href="#3-4-Training" class="headerlink" title="3.4 Training"></a>3.4 Training</h2><p>我们的模型使用<strong>负采样监督范式</strong>下进行训练。抽取获得文章片段。</p><script type="math/tex; mode=display">\mathcal{D}[x, Q] = \lbrace (y_1, \text{start}_1,\text{end}_1), \cdots, (y_n, \text{start}_n, \text{end}_n) \rbrace</script><h3 id="3-4-1-Task-1-Span-Extraction"><a href="#3-4-1-Task-1-Span-Extraction" class="headerlink" title="3.4.1 Task #1: Span Extraction"></a>3.4.1 Task #1: Span Extraction</h3><p>损失函数如下定义：</p><script type="math/tex; mode=display">\mathcal{L}_\text{ans}^\text{start} = - \sum_i \boldsymbol{\mathrm{gt}}_\text{ans}^\text{start}[i] \cdot \log P_\text{ans}^\text{start}[i]</script><p>其中，ground truth $\boldsymbol{\mathrm{gt}}_\text{ans}^\text{start}$ 如下定义。</p><ul><li>对于 answer span $(y, \text{start}, \text{end})$，使用 one hot 即 $\boldsymbol{\mathrm{gt}}_\text{ans}^\text{start}[\mathrm{start}] = 1$ <sup id="fnref:57"><a href="#fn:57" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="为什么说 answer span 只有一个？莫非是数据集定义的？">[57]</span></a></sup></li><li>对于 next-hop span，可能有多个 span ，因此概率均匀分布在所有出现的位置上，即$\boldsymbol{\mathrm{gt}}_\text{ans}^\text{start}[\text{start}_i] = 1 / k$</li><li>对于负跳节点（应该就是不应该连接上的点？）使用$\boldsymbol{\mathrm{gt}}_\text{ans}^\text{start}[0] = 1$</li></ul><h3 id="3-4-2-Task-2-Answer-Node-Prediction"><a href="#3-4-2-Task-2-Answer-Node-Prediction" class="headerlink" title="3.4.2 Task #2: Answer Node Prediction"></a>3.4.2 Task #2: Answer Node Prediction</h3><p>为了推理能力，我们的模型必须学会在实体图中辨别正确答案节点。对于在训练集中的每一个问题，我们对这个任务构造一个训练样例。每一个训练样例是 <em>gold-only graph</em> 和负节点的组合。其中 <em>gold-only graph</em> 是所有正确推理路径的组合。负节点包括在任务一中使用的负跳节点和两个负答案节点。一个负答案节点由一个从随机选择的跳节点中随机抽取的文本片段构成。</p><p>对于特殊问题，我们首先对每一个节点计算 <em>最终答案概率</em> ，通过在 $\mathcal{F}$ 的输出中施加 softmax. 损失 $\mathcal{L}$  定义为 概率 与 答案 one-hot 向量的交叉熵。</p><script type="math/tex; mode=display">\mathcal{L} = - \log \left ( \mathrm{softmax}(\mathcal{F}(\bold{X})[\text{ans}]) \right)</script><p>对于替代问题与一般问题，我们用同样的方法使用二进制交叉熵进行优化。这个任务的损失不仅反向传播优化预测器和 System 2，而且 fine-tune System 1 通过 语义向量 $\mathrm{sem} [x, Q, \mathrm{clues}]$</p><h1 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4 Experiment"></a>4 Experiment</h1><h2 id="4-1-Dataset"><a href="#4-1-Dataset" class="headerlink" title="4.1 Dataset"></a>4.1 Dataset</h2><p>我们使用 full-wiki setting of HotpotQA 构建我们的实验。 根据 Wikipedia 文档的第一段收集了 112,779 个来自与群众的问题，其中84%需要多跳推理。数据分成了 90564 个问题的训练集，7405 个问题的开发集 和 7405 个问题的测试集。所有在开发集和测试集中的问题都是 困难的多跳 问题。</p><p>在训练集中，对于每一个问题，提供一个答案和两个有用实体的段落。并标注出多个支持事实，即包含对推理重要的信息的句子。同时为训练也提供了 8 个没有用的 负段落。在验证中，只提供问题，除答案之外还要给出推理支持事实。</p><p>为了在训练中构建认知图，在 gold-only 认知图中的边通过基于 levenshtein 距离的模糊匹配与支持事实相关联。对每一个在 $\mathrm{para}[x]$ 的支持事实，如果任何标记为 $y$ 的 黄金实体 或者 答案，与支持事实中的文本片段模糊匹配，则添加边 $(x,y)$.</p><h2 id="4-2-Experimental-Details"><a href="#4-2-Experimental-Details" class="headerlink" title="4.2 Experimental Details"></a>4.2 Experimental Details</h2><p>我们使用预训练的 BERT-base 模型作为 System 1. 隐状态大小 $H$ 为 768， 在 GNN 和 预测器中保持不变。在我们的模型中，所有的激活函数为 $GeLU$。我们在 Task #1 上 训练一个 epoch，然后 将 Task #1 与 Task #2 合并训练一个 epoch。训练超参数如下：</p><div class="table-container"><table><thead><tr><th>模型</th><th>任务</th><th>批大小</th><th>学习率</th><th>权重消散</th></tr></thead><tbody><tr><td>BERT</td><td>#1,#2</td><td>10</td><td>$10^{-4}, 4\times10^{-5}$</td><td>0.01</td></tr><tr><td>GNN</td><td>#2</td><td>graph</td><td>$10^{-4}$</td><td>0</td></tr></tbody></table></div><p>BERT 和 GNN 用两个不同的 Adam 优化器进行优化， $\beta_1 = 0.9, \beta_2 = 0.999$。 预测器与 GNN 共享同一个优化器。BERT 的学习率在前 10% step中进行热身，之后线性消散至0.</p><p>为了选出支持事实，我们认为在图中任何节点 $\text{clues}$ 中的句子当作 支持事实。在 $\mathcal{G}$ 的初始化中，这些 1跳文本片段 存在于问题中，并且能通过与在训练集中的支持事实进行模糊匹配被发现。在我们的框架中，被抽取的1跳实体能提高其他模型的 retrieval phase，这促使我们将1跳实体抽取分离开，与其他基于 BERT 的模型，以实现重用。</p><h2 id="4-3-Baseline"><a href="#4-3-Baseline" class="headerlink" title="4.3 Baseline"></a>4.3 Baseline</h2><p>= =</p><h2 id="4-4-Results"><a href="#4-4-Results" class="headerlink" title="4.4 Results"></a>4.4 Results</h2><p>使用两种度量：EM 和 F1。联合 EM 当答案字符串和支持事实都严格正确时 为1. 联合准确率与召回率是 答案与支持事实 各自的准确率与召回率的乘积，之后计算联合 F1。这些度量在整个测试集上进行平均。实验结果展示了我们的模型在多方面的优越性。</p><h3 id="Overall-Performance"><a href="#Overall-Performance" class="headerlink" title="Overall Performance"></a>Overall Performance</h3><ul><li>认知图结构 比 检索抽取更有效</li></ul><h3 id="Logical-Rigor"><a href="#Logical-Rigor" class="headerlink" title="Logical Rigor"></a>Logical Rigor</h3><p>Logical Rigor： 逻辑严谨</p><p>使用</p><script type="math/tex; mode=display">\frac{\mathrm{JointEM}}{\mathrm{AnsEM}}</script><p>度量严谨性。</p><h3 id="Multi-hop-Reasoning"><a href="#Multi-hop-Reasoning" class="headerlink" title="Multi-hop Reasoning"></a>Multi-hop Reasoning</h3><p>在 替代问题 和 一般问题 中没有进步。</p><h3 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h3><ul><li>BERT 不是提升的主要因素。</li></ul><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>反正很强就是了 = =</p><h1 id="Something-else"><a href="#Something-else" class="headerlink" title="Something else"></a>Something else</h1><p>零零散散看了好久才看完hhhhhhh，期间帮忙整理了高数和通原的知识点，也希望她能加油吧hhhhh</p><p>其实这篇没怎么看懂，部分实现也不是很具体。（可能还需要补充一些其他的知识。。）</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">在 <a href="https://github.com/THUDM/CogQA">https://github.com/THUDM/CogQA</a> 上获取代码<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://hotpotqa.github.io">https://hotpotqa.github.io</a> 2019年3月4日<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:51"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">51.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">我觉得这里的 $x$ 即指的是<strong>实体</strong>，又指的是<strong>可能的答案</strong>。大概是这两个东西共用了一个符号。<a href="#fnref:51" rev="footnote"> ↩</a></span></li><li id="fn:52"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">52.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">via 谷歌翻译<a href="#fnref:52" rev="footnote"> ↩</a></span></li><li id="fn:53"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">53.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">sacrifice<a href="#fnref:53" rev="footnote"> ↩</a></span></li><li id="fn:54"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">54.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">$\mathrm{clues}[x, \mathcal{G}]$ are sentences in paragraphs of $x$'s predecessor nodes, from which $x$ is extracted.<a href="#fnref:54" rev="footnote"> ↩</a></span></li><li id="fn:55"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">55.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">And when extracting 1-hop nodes from question to initialize $\mathcal{G}$ , we do not calculate semantic vectors and only the $\mathrm{Question}$ part exists in the input. 我也没看懂这句话它想表达什么。<a href="#fnref:55" rev="footnote"> ↩</a></span></li><li id="fn:56"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">56.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">我觉得这个翻译不太行。原文： <em>special</em> question, <em>alternative</em> question and <em>general</em> question<a href="#fnref:56" rev="footnote"> ↩</a></span></li><li id="fn:57"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">57.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">为什么说 answer span 只有一个？莫非是数据集定义的？<a href="#fnref:57" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    
    <categories>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>距离整理笔记</title>
    <link href="/posts/metrics/"/>
    <url>/posts/metrics/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>本篇是在日常看论文的时候所整理的关于<strong>距离</strong>或者<strong>度量</strong>的内容。主要会记录一些度量的方法，和这种度量方法所适用的范围。</p><span id="more"></span><h1 id="Version"><a href="#Version" class="headerlink" title="Version"></a>Version</h1><ul><li>ver0.1 2019/11/6 创建文档</li></ul><h1 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离"></a>编辑距离</h1><h2 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h2><p>设$x$，$y$为两个长度为$n$的二进制向量，则汉明距离为</p><script type="math/tex; mode=display">\begin{aligned}d(x, y) = \sum_{i=1}^n I(x_i, y_i) \\I(a, b) = \begin{cases}1, & a = b \\0, & \text{else} \\\end{cases}\end{aligned}</script><p>汉明距离满足<strong>距离公理</strong>：非负性、对称性、三角不等式。</p><h2 id="Levenshtein距离"><a href="#Levenshtein距离" class="headerlink" title="Levenshtein距离"></a>Levenshtein距离</h2><p>设$x$，$y$是两个字符串。那么Levenshtein距离定义为：将$s_1$转换成$s_2$的最小<strong>编辑操作</strong>数。通常，这样的编辑操作包括：</p><ol><li>将一个字符插入字符串</li><li>从字符串中删除一个字符</li><li>将字符串中的一个字符替换成另外一个字符</li></ol><p>关于该编辑距离是否可以满足距离的三定义（非负性，对称性，三角不等式）只查到已被证明，但没有查询到相关的证明过程。在维基百科的 <a href="https://en.wikipedia.org/wiki/Edit_distance">编辑距离</a> 词条中，简单的说明了该编辑距离满足距离定义。简略翻译如下：</p><blockquote><p>具有非负成本(non-negative cost)的编辑距离满足距离公理(the axioms of a metric)。当满足以下条件时，该编辑距离生成一个距离空间(metric space)：</p><ul><li>每一个编辑操作都具有正成本。</li><li>对于每一个编辑操作，都有相同成本的逆操作(inverse operation)。</li></ul><p>具有以上这些属性，将满足如下的距离公理：</p><ul><li>$d(a,b) = 0$当且仅当$a = b$，因为每一个字符串都可以通过0个操作转换成自身。</li><li>$d(a,b) &gt; 0$当$a \neq b$，因为这将至少需要一个操作。</li><li>$d(a,b) = d(b,a)$，每一个操作的成本与其逆操作的成本是相同的。</li><li>$d(a,c) \leq d(a,b) + d(b,c)$，三角不等式。</li></ul><p>Levenshtein distance 和 LCS distance 具有单元成本并满足以上条件，因此满足距离公理。一些编辑距离的变体即使不是真正的(proper)距离，也被在一些文献中使用。</p></blockquote><p>该距离由递推式定义：</p><script type="math/tex; mode=display">d(x[0:i], y[0:j]) = \begin{cases}\max (i, j) & \text{if} \min(i, j) = 0\min \begin{cases} d(x[0:i-1], y[0:j]) +1 \\d(x[0:i],y[0:j-1]) +1 \\d(x[0:i-1], y[0:j-1]) + 1 _{(x_i \neq y_j)}  \end{cases}& \text{otherwise}\end{cases}</script><h2 id="Max-Match-M-2-Evaluation"><a href="#Max-Match-M-2-Evaluation" class="headerlink" title="Max-Match($M^2$) Evaluation"></a>Max-Match($M^2$) Evaluation</h2><p><a href="https://www.aclweb.org/anthology/N12-1067/">Paper: Better Evaluation for Grammatical Error Correction</a></p><p>我们这里提到的评估，有以下的限制与使用环境。</p><ul><li>我们对两个句子$C$，$S$进行评估，并使用“黄金标准”$G$决定性评价句子。</li><li>评估的最小粒度为单词（词条）。</li><li>我们希望这个评估可以对包含$G$的$C$给予<strong>较高</strong>的分数，对接近$S$的$C$给予<strong>较低</strong>的分数。</li></ul><p>文章中使用了一种称之为<strong>编辑晶格（Edit lattice）</strong>的方法辅助计算该调整过的距离。首先，我们计算$C$和$S$在单词（词条）粒度上的编辑距离。在计算编辑距离时会使用表格进行动态规划。之后将这个表格转化成一个有向无环图。便于理解，我们按照下面的步骤进行生成。</p><ol><li>首先我们将表格中的最优编辑（编辑距离最小的编辑方式）画成如图所示的有向无环图。</li><li>我们定义了合并过程中的不变单词的最大数量$u$，这里我们让$u=2$，这也是文章中的默认配置。</li><li>我们开始对编辑操作进行合并：对任意两个结点进行连线，并满足第二条的<strong>不变单词的最大数量</strong>的限制，从而得到完整的有向无环图。</li><li>我们对所有的单位操作赋予权值$1$，对于合并的操作赋予被合并的权值之和。此外，我们将黄金标准中的编辑操作赋予权值$-(u+1) \times |E|$，其中$|E|$是这张图中的边的总数。也就是含有黄金标准的路径将<strong>一定</strong>会获得一个负数的权值。这样，我们就得到了一个<strong>编辑晶格</strong>。</li></ol><p>之后计算$M^2$距离时，仅需要先计算$C$，$S$的最小权值路径所对应的编辑操作$e$。对于所有的测试句子的编辑操作组成一个集合$\lbrace e_1, \cdots, e_n \rbrace$，与其对应的黄金标准集合为$\lbrace g_1, \cdots , g_n \rbrace$。之后计算两个集合的准确率、召回率和$F_1$分数即可。</p><script type="math/tex; mode=display">\begin{aligned}P &= \frac{\sum_{i=1}^n | e_i \cap g_i |}{\sum_{i=1}^n | e_i | } \\R &= \frac{\sum_{i=1}^n | e_i \cap g_i |}{\sum_{i=1}^n |g_i|} \\F_1 &=  2 \times \frac{P \times R}{P + R}\end{aligned}</script><p>这里的$|e_i \cap g_i |$表示的是$e_i$操作是否与$g_i$操作match。</p><script type="math/tex; mode=display">\mathrm{match}(e,g) \Leftrightarrow (e.a = g.a) \land (e.b = g,b) \land (e.C \in g.C)</script><h1 id="二元素度量"><a href="#二元素度量" class="headerlink" title="二元素度量"></a>二元素度量</h1><h2 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h2><p><a href="https://www.aclweb.org/anthology/P02-1040.pdf">Paper: BLEU: a Method for Automatic Evaluation of Machine Translation</a></p><p>BLEU是为<strong>机器翻译</strong>准备的评估方法。以单词（词条）为单位，计算候选句$C$与参考句$R$的距离。原论文可以计算多个候选句与多个参考句之间的距离，这里我们将其简化为一个候选句与多个参考句的距离（当然也可以一对一）。</p><p>公式直接列举在下面了，感觉下面的定义足够的详细。</p><script type="math/tex; mode=display">\mathrm{BLEU}(c;R) = \mathrm{BP}(c;R) \cdot \exp \left( \sum_{n=1}^{N} w_N \log p_n\right)</script><script type="math/tex; mode=display">\mathrm{BP}(c;R) = \begin{cases}1 \quad &\text{if}\ \mathrm{len}(c) > \mathrm{len}(r^\star)\\\exp(1-\frac{\mathrm{len}(r^\star)}{\mathrm{len}(c)}) &\text{if}\  \mathrm{len}(c) \leq \mathrm{len}(r^\star)\end{cases}</script><script type="math/tex; mode=display">r^\star = \arg \min_{r \in R} \left| \mathrm{len}(r) - \mathrm{len}(c)\right|</script><script type="math/tex; mode=display">w_N = \frac{1}{N}</script><script type="math/tex; mode=display">p_n = \frac{\sum_{n\text{-gram} \in c} \mathrm{Count_{clip}}(n\text{-gram})}{\sum_{n\text{-gram}' \in c} \mathrm{Count}(n\text{-gram}')}</script><script type="math/tex; mode=display">\mathrm{Count_{clip}}(n\text{-gram}) = \min(\mathrm{Count}(n\text{-gram},c),\max_{r \in R}(\mathrm{Count(n\text{-gram},r)}))</script><script type="math/tex; mode=display">\mathrm{Count}(n\text{-gram}, w) = \sum_{n\text{-gram}' \in w} I(n\text{-gram}, n\text{-gram}')</script><script type="math/tex; mode=display">I(a,b) = \begin{cases}1 & \text{if}\ a = b \\0 & \text{else}\end{cases}</script><p>简单概括下，主要分为以下步骤。</p><ul><li>计算截断数量$\mathrm{Count}_\mathrm{clip}$</li><li>计算准确度$p_n$</li><li>计算长度惩罚系数$\mathrm{BP}(c;R)$</li><li>最终得出$\mathrm{BLEU}(c;R)$</li></ul><h1 id="三元素度量"><a href="#三元素度量" class="headerlink" title="三元素度量"></a>三元素度量</h1><h2 id="I-measure-Evaluation"><a href="#I-measure-Evaluation" class="headerlink" title="I-measure Evaluation"></a>I-measure Evaluation</h2><p><a href="https://www.aclweb.org/anthology/N15-1060/">Paper: Towards a standard evaluation method for grammatical error detection and correction</a></p><p>我们这里提到的评估，有以下的限制与使用环境。</p><ul><li>我们对三个句子$C$，$S$，$R$进行评估。</li><li>评估的最小粒度为单词（词条）。</li><li>我们希望这个评估可以对接近$R$的$C$给予<strong>较高</strong>的分数，对接近$S$的$C$给予<strong>较低</strong>的分数。</li></ul><p>文章的核心在于使用了<strong>SP对齐（the Sum of Pairs alignment）的方式</strong>，将三个句子同时对齐。同样是基于Levenshtein距离的方法，我们对插入删除操作（gap）和替换操作（mismatch）分别赋予不同的代价（cost），分别为$c<em>\text{gap}$，$c</em>\text{mis}$，得到一种总代价最小的对齐方案。</p><p><em>该算法该天再补。</em></p><p>之后的部分是计算PRF1。首先，我们使用改进版的<em>WAS</em>方法定义TP、TN、FP、FN。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Written（Source）</th><th style="text-align:center">Annotated（Reference）</th><th style="text-align:center">System（Candidate）</th></tr></thead><tbody><tr><td style="text-align:center">TN</td><td style="text-align:center">X</td><td style="text-align:center">X</td><td style="text-align:center">X</td></tr><tr><td style="text-align:center">FP</td><td style="text-align:center">X</td><td style="text-align:center">X</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">FN</td><td style="text-align:center">X</td><td style="text-align:center">Y</td><td style="text-align:center">X</td></tr><tr><td style="text-align:center">TP</td><td style="text-align:center">X</td><td style="text-align:center">Y</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">FPN</td><td style="text-align:center">X</td><td style="text-align:center">Y</td><td style="text-align:center">Z</td></tr></tbody></table></div><p>简单解释，我们将W与A是否匹配分成两类：不匹配（即需要更改）【P】、匹配（即不需要更改）【N】。由此我们可以将S的预测分为TP、TN、FP、FN四类。因为在本次计算中，我们还会遇到最后一种情况，此时我们将其定义为FPN类。这样，我们得出加权准确性指标。</p><script type="math/tex; mode=display">\mathrm{WAcc} = \frac{w \cdot \mathrm{TP} + \mathrm{TN}}{w \cdot \mathrm{TP} + \mathrm{TN} + w \cdot (\mathrm{FP} - \frac{\mathrm{FPN}}{2}) + (\mathrm{FN} - \frac{\mathrm{FPN}}{2})}</script><p>我们希望给予$\mathrm{TP}$多于$\mathrm{TN}$的奖励（即正确的更改好于不更改）；我们希望基于$\mathrm{FP}$多于$\mathrm{FN}$的惩罚（即错误的更改不如不更改），因此这里$w \geq 1$。通常$w = 2$。</p><p>最后为了对比在不同数据集下的模型情况，我们构建了每个数据集格子的基线标准，即：将Reference直接作为Candidate，计算加权准确性指标$\mathrm{WAcc}_{\text{base}}$，然后我们得到I-measure</p><script type="math/tex; mode=display">I = \begin{cases}\lfloor \mathrm{WAcc}_{\text{sys}} \rfloor & \text{if}\  \mathrm{WAcc}_{\text{sys}} = \mathrm{WAcc}_{\text{base}} \\\frac{\mathrm{WAcc}_{\text{sys}} - \mathrm{WAcc}_{\text{base}}}{1 - \mathrm{WAcc}_{\text{base}}} & \text{if}\ \mathrm{WAcc}_{\text{sys}} > \mathrm{WAcc}_{\text{base}} \\\frac{\mathrm{WAcc}_\text{sys}}{\mathrm{WAcc}_\text{base}} - 1 & \text{otherwise}\end{cases}</script><h2 id="GLEU"><a href="#GLEU" class="headerlink" title="GLEU"></a>GLEU</h2><p><a href="https://www.aclweb.org/anthology/P15-2097/">Paper: Ground Truth for Grammatical Error Correction Metrics</a></p><p><a href="https://arxiv.org/abs/1605.02592">Paper: GLEU Without Tuning</a></p><p>本文将主要介绍升级版的GLEU，他们称之为$\mathrm{GLEU^+}$。更新版的GLEU+相对于原始版本的GLEU，不仅去掉了一个需要特殊训练的参数$\lambda$，计算公式也更加合理易懂，所以我们将主要介绍这个。</p><p>我们这里提到的评估，有以下的限制与使用环境。</p><ul><li>我们对三个句子$C$，$S$，$R$进行评估。</li><li>评估的最小粒度为单词（词条）。</li><li>我们希望这个评估可以对接近$R$的$C$给予<strong>较高</strong>的分数，对接近$S$的$C$给予<strong>较低</strong>的分数。</li></ul><p>该算法大体与BLEU一致，为了适应语法错误更正这个任务(GEC)，我们要修改截断计数。这里我们大体沿用BLEU部分的式子，只对$p_n$进行修改：</p><script type="math/tex; mode=display">\mathrm{GLEU}(c,r,s) = \mathrm{BP}(c; r) \cdot \exp \left( \sum_{n=1}^N w_N \log p^\star_n\right)</script><script type="math/tex; mode=display">p^\star_n = \frac{\sum_{n\text{-gram} \in c \cap r } \mathrm{Count}_{c, r} (n\text{-gram}) - \sum_{n\text{-gram} \in c \cap s} \max \left[ 0, \mathrm{Count}_{c, s}(n\text{-gram}) - \mathrm{Count}_{c, r}(n\text{-gram}) \right]  }{\sum_{n\text{-gram}' \in c} \mathrm{Count}_c (n\text{-gram}')}</script><script type="math/tex; mode=display">\mathrm{Count}_{A,B}(n\text{-gram}) = \min \left[ \mathrm{Count}_A (n \text{-gram}), \mathrm{Count}_B (n \text{-gram}) \right]</script><script type="math/tex; mode=display">\mathrm{Count}_A (n\text{-gram}) = \sum_{n\text{-gram}' \in A} I(n\text{-gram}, n\text{-gram}')</script><h1 id="向量度量"><a href="#向量度量" class="headerlink" title="向量度量"></a>向量度量</h1><h2 id="p-范数"><a href="#p-范数" class="headerlink" title="$p$-范数"></a>$p$-范数</h2><p>各个领域都很常用的范数，用来度量向量应该是比较成熟的理论了。</p><script type="math/tex; mode=display">\Vert x \Vert_p = \left( \vert x_1 \vert^p + \vert x_2 \vert^p + \cdots + \vert x_n \vert^p \right)^{1/p}</script><h3 id="1-范数"><a href="#1-范数" class="headerlink" title="$1$-范数"></a>$1$-范数</h3><script type="math/tex; mode=display">\Vert x \Vert_1 = \vert x_1 \vert + \vert x_2 \vert + \cdots + \vert x_n \vert</script><p>1-范数有比较好的鲁棒性，根据罚函数的理论，其在大的$x_i$上，罚函数上升较慢。</p><h3 id="2-范数"><a href="#2-范数" class="headerlink" title="$2$-范数"></a>$2$-范数</h3><script type="math/tex; mode=display">\Vert x \Vert_2 = \left( \vert x_1 \vert^2 + \vert x_2 \vert^2 + \cdots + \vert x_n \vert^2 \right)^{1/2}</script><p>这个就是大家都很常用的欧氏距离了。也是最小二乘法所使用的范数。</p><h3 id="infty-范数"><a href="#infty-范数" class="headerlink" title="$\infty$-范数"></a>$\infty$-范数</h3><script type="math/tex; mode=display">\Vert x \Vert_\infty = \max \left( \vert x_1 \vert , \vert x_2 \vert , \cdots , \vert x_n \vert \right)</script><p> 总之也能有用的上的地方吧。（比如说对数据的上下界有极为严格的定义，之类的。）</p><h1 id="概率分布度量"><a href="#概率分布度量" class="headerlink" title="概率分布度量"></a>概率分布度量</h1><p>如果要认为这个也是向量度量的话，也不是不可以。这里就将对概率化的向量的度量单独列出来了。</p><h2 id="相对熵（Kullback-Leibler散度）"><a href="#相对熵（Kullback-Leibler散度）" class="headerlink" title="相对熵（Kullback-Leibler散度）"></a>相对熵（Kullback-Leibler散度）</h2><p>源自信息论理论中的一个度量。用来度量两个概率分布的差异。</p><p><strong>注意：这是一个非对称的度量，换句话说，这个度量不符合标准的距离定义。</strong></p><script type="math/tex; mode=display">\mathrm{KL}(p \Vert q) = \sum p(x) \log \frac{p(x)}{q(x)}</script><h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>很不幸的是，交叉熵也不具有对称性。通常我们认为某一个分布是已知的，我们用q去逼近p，在这种情况下，相对熵可以表示为：</p><script type="math/tex; mode=display">\begin{aligned}\mathrm{KL}(p \Vert q ) &= \sum p(x) \log p(x) - \sum p(x) \log q(x) \\&= -H(p) + H(p \Vert q)\end{aligned}</script><p>我们称$H(p \Vert q)$为交叉熵，定义为：</p><script type="math/tex; mode=display">H(p \Vert q) = - \sum p(x) \log q(x)</script><p>这里使用了可能和大多数博客不一样的符号。为了区别联合熵$H(p,q)$，因此没有使用逗号。为了区别条件熵$H(p \vert q)$，因此没有使用单竖线。为了看起来保持了相对熵不对称的性质，因此使用了双竖线。</p>]]></content>
    
    
    <categories>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>动画OP与ED手法观察</title>
    <link href="/posts/oped-view/"/>
    <url>/posts/oped-view/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><div class="note note-secondary">            <p><strong>发布于</strong>：檐枫动漫社<br><strong>作者</strong>：Santoin<br><strong>日期</strong>：2019年10月26日<br><strong>原文链接</strong>：<a href="https://mp.weixin.qq.com/s/lvbCVQXobIQ8t6hfp5LIhw">【投稿】动画OP与ED手法观察 - 檐枫动漫社</a></p>          </div><p>みなさんこんばんは。</p><p>第一次尝试写一点稍微有一点深度的文章。虽说最近看的新番越来越少，不过还是发现了一点有趣的事情，就记录下来了。大部分内容都来自于自己的理解，如果有什么错误也请多多指教。</p><p>对于一部番剧而言，如果说作画、剧情、配音是能否将一部作品称为优秀的评判标准的话。那 OP 和 ED ，就是吸引观众看番的比较重要的一部分了。</p><p>如何在短短的 90 秒内，充分展现作品的主题、背景，或者渲染好故事的气氛，做好足够的铺垫，一首合适的曲子以及一个合适的动画就显得比较重要。这篇文章主要会记录一些自己看到的一些OP或者ED的摄影手法，或者称作：视频制作手法。</p><span id="more"></span><h1 id="动画OP与ED手法观察"><a href="#动画OP与ED手法观察" class="headerlink" title="动画OP与ED手法观察"></a>动画OP与ED手法观察</h1><p>当然，与现实世界常用的摄影手法相比，动画由于离现实更远，从而有更加广阔的创造空间，可以利用更丰富的想象力去创造一些不可能在现实中存在的画面。不过可以发现的是，有越来越多的现实世界的手法已经或多或少的运用在动画的 OP 与 ED 中。</p><h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><p>让笔者突然有了想写一篇这样的文章，就来源于下面这个作品。不确定这种特效有没有什么名字，我们姑且称之为<strong>光影特效</strong>吧。（虽然说一般大家都会用《光影XX》作为作品的名字。）</p><p><img src="./sarazanmai-ed.jpg" alt="さらざんまい - ED"></p><p>说起来这种特效也简单，在现实环境中的添加一些不存在的<strong>高光图形</strong>。在制作过程中稍微注意光环境的改变给其他物品带来的影响。在《さらざんまい》这部作品的ED中，将这种光影效果与二渲三（姑且这么称呼吧）融合在一起。带来一种既在真实世界又仿佛有了一点不真实的感觉。</p><p><img src="./miku.jpg" alt="笔者之前做过的一张类似风格的图"></p><p>第二个让我想起写这样一篇文章的作品就是最近热播的《女子高中生的虚度日常》。当然，这个OP也有了各种各样的翻版。</p><p><img src="./jioshimuda.jpg" alt="女子高生の無駄づかい - OP"></p><p>但是定格动画在动画的应用其实很多。倒不如说定格动画就是低帧数的动画嘛。但这里所用的定格动画反倒在仿照着真实世界的拍摄方法所绘制的。</p><p><img src="./BV114411U72C.jpg" alt="BV114411U72C"></p><p><a href="https://www.bilibili.com/video/BV114411U72C">https://www.bilibili.com/video/BV114411U72C</a></p><p>当然，这种效果拍摄起来也是很费力气的。不过这种意外的不真实感在现实世界广受好评。在动画中就又添了一番乐趣。</p><h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><p>当然，还有一些其他方法，说不上什么借鉴于真实世界的摄影手法。不过却有一些在与动画不相关的领域中也会使用的方法。</p><p><img src="./raamensuki.jpg" alt="ラーメン大好き小泉さん - OP"></p><p>这种在换场过程中，背景会像是重新构建世界一样拼贴出来的效果，在一些MG动画中，也是比较常用的手法。至少，这种转场的确比较方便。</p><p><img src="./raamensuki2.jpg" alt="ラーメン大好き小泉さん - OP"></p><p>《小泉同学》的OP还是很有MG风格的，这一段文字的转换就更加明显。准确的把握文字与其他元素的<strong>运动与变换</strong>是MG动画的核心。</p><p><img src="./happysugarlife.jpg" alt="ハッピーシュガーライフ - OP"></p><p>一部形式奇特的讲述爱情的动画。当然，就这部动画的主题而言，OP 很准确的营造了这种特殊的气氛。有人把这种风格叫<em>超现实主义</em>或者<em>达达主义</em>。不过这种对真实存在的物品进行奇怪的调色，塑造的看似甜蜜的诡异气氛十分符合这一作品。</p><h2 id="3"><a href="#3" class="headerlink" title="3"></a>3</h2><p>设计这种东西，每个人的想法和看法都不同，最后的成品也都不一样，也就不存在什么“通用手法”、“模板”，之类的东西。也许和以往的一小点不一样，就足以吸引眼球。</p><p><img src="./newgame2.jpg" alt="NEW GAME!! - OP"></p><p><img src="./slowstart.jpg" alt="SLOW START - OP"></p><p>这里的一小点不一样就是突然丰富的动画。但不得不说，能驾驭这样的画面可见作画功底了得。也对之后使用这种风格的作品带来了更大的挑战。如果让动画流畅又不做作是一大难题。</p><p><img src="./watashitenshi.jpg" alt="私に天使が舞い降りた！- OP - OP"></p><p><img src="./blends.jpg" alt="Blend·S - OP"></p><p>当然，感觉越来越多的作品把注意力放在经常会挡住画面的 Staff 表。将 Staff 巧妙地融入到画面中就成了这些作品的特点。不过，总觉得这样让NCOP吸引力略微下降了x</p><h2 id="4"><a href="#4" class="headerlink" title="4"></a>4</h2><p>不过话说回来，虽然有各种各样可以拿来使用的风格模式可以让现有的动画增加那么一点点亮点。不过，扎实的作画、精致的舞蹈、抑或是极富想象力的画面，也能给观众带来深刻的印象。无论是“螺旋上天”还是“唱歌的打印机”，能表现出属于作品本身的特点与风格，才是一个优秀的 OP 或者 ED 的评价标准。</p><p><img src="./meidora.jpg" alt="小林さんちのメイドラゴン - OP"></p><p><img src="./evergarden.jpg" alt="ヴァイオレット・エヴァーガーデン - OP"></p><p>随着时代的变化，技术也在不断进步。即使凭借当年的画质与技术，《凉宫》的团舞或者《幸运星》的水手服给一代人留下了深刻的印象。而未来凝聚在这短短的90秒中的创作者的智慧，又会迸发出怎样的魅力呢，就暂且拭目以待。</p><p>大好きだよ、みんな！</p>]]></content>
    
    
    <categories>
      
      <category>吐槽</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>人工神经网络的基本知识</title>
    <link href="/posts/nn0/"/>
    <url>/posts/nn0/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>这个原本是通信网理论与应用这门课的一个调研型的大作业。（不过这个和通信网有什么关系）就当是笔者从零开始学习深度学习吧。</p><p>本篇BLOG含有大量的公式，用来阐明最基本的神经网络在优化问题的基本算法。此外根据查到的各种资料，本篇也会简单说明基于神经网络而发展的典型网络结构以及使用这些网络结构的领域。虽说含有大量的公式，不过用到的原理也只是最基本的高等数学。为了比较美观的展现公式和算法，还会使用一些基本的线性代数运算。</p><p>不过，毕竟初来乍到，可能有很多漏洞和疏忽之处，恳请理解指正。</p><span id="more"></span><p id="symbol"></p><h1 id="符号声明"><a href="#符号声明" class="headerlink" title="符号声明"></a>符号声明</h1><div class="table-container"><table><thead><tr><th>符号</th><th>作用 </th></tr></thead><tbody><tr><td>$x_k^{(L)}$</td><td>第$L$层的第$k$个神经元的值</td></tr><tr><td>$w_{kn}^{(L)}$</td><td>第$L$层的第$k$个神经元与第$L-1$层的第$n$个神经元的连接权重    </td></tr><tr><td>$\theta_k^{(L)}$</td><td>第$L$层的第$k$个神经元的输入偏置</td></tr><tr><td>$z_k^{(L)}$</td><td>第$L$层的第$k$个神经元通过激活函数之前的值</td></tr><tr><td>$E$</td><td>误差</td></tr><tr><td>$\hat{x}_k^{(L)}$</td><td>第$L$层的第$k$个神经元的估计值（通常指最后一层）</td></tr><tr><td>$N^{(L)}$</td><td>第$L$层的神经元的个数</td></tr></tbody></table></div><h1 id="参考与推荐阅读"><a href="#参考与推荐阅读" class="headerlink" title="参考与推荐阅读"></a>参考与推荐阅读</h1><p>本篇大部分内容参考自周志华的《机器学习》（西瓜书）和3Blue1Brown关于神经网络的视频。私心推荐一下3Blue1Brown这位作者的视频。</p><p>3Blue1Brown: <a href="https://youtu.be/aircAruvnKk">YouTube</a> <a href="https://space.bilibili.com/88461692">Bilibili</a></p><h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><blockquote><p>神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。</p></blockquote><p>上述概念来源于周志华在《机器学习》这本书中的对原论文的翻译。神经网络是由多个进行特定运算的单元组成的网络。</p><h2 id="m-p神经元模型"><a href="#m-p神经元模型" class="headerlink" title="m-p神经元模型"></a>m-p神经元模型</h2><p>上文提到论文中的神经元模型是含有<strong>时间</strong>这个参数的。在下面这张图中，忽略时间的影响得到神经元的结构。</p><p><img src="./mp_model.jpg" alt="M-P 神经元模型" title="M-P 神经元模型"></p><p>M-P神经元的数学模型为：</p><script type="math/tex; mode=display">\begin{equation}    y = f(\sum_{k = 1}^{n} w_k x_k + \theta)\label{eq1}\end{equation}</script><p>关于这张图上的模型，仍需几点特殊说明：</p><ul><li>对于一个神经元而言，这里$x$表示输入，$y$表示输出。而在多级神经网络中，$y$将表示下一级神经元的一个输入。</li><li>为了便于理解，在这里我们将输入和输出人为规定成0和1之间的数。在实际应用中不存在这样的限制。</li></ul><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>激活函数的出发点是将各个神经元的值限制在0到1之间，于是有<strong>单位阶跃函数</strong>和<strong>Sigmoid函数</strong>。随着这个限制的消失，追求更高收敛速度和性能的激活函数得以出现。例如<strong>ReLU函数</strong>、<strong>Softplus函数</strong>。</p><p>单位阶跃函数：</p><script type="math/tex; mode=display">\begin{equation}    H(x) = \begin{cases}    1, & x \geq 0 \\    0, & x < 0 \end{cases} \end{equation}</script><p>Sigmoid函数：</p><script type="math/tex; mode=display"> \begin{equation}    \mathrm{Sigmoid}(x) = \frac{1}{1 + \mathrm{e}^{-x}}\end{equation}</script><p>ReLU函数：</p><script type="math/tex; mode=display">                      \begin{equation}                      \mathrm{ReLU}(x) = \begin{cases}                      x, & x > 0 \\                      0, & x \leq 0                      \end{cases}                      \end{equation}</script><p>Softplus函数：</p><script type="math/tex; mode=display">\begin{equation}                      \mathrm{Softplus}(x) = \ln{(1 + \mathrm{e}^{x})}                      \end{equation}</script><p><img src="./activfuc.jpg" alt="激活函数" title="激活函数"></p><h2 id="多层前馈神经网络"><a href="#多层前馈神经网络" class="headerlink" title="多层前馈神经网络"></a>多层前馈神经网络</h2><p>这部分的公式会开始复杂起来，可以参考<a href="#symbol">符号声明</a>回顾各个符号的含义。</p><p>通常来说，一个基本的神经网络不会只有一层，而是由很多层构成的。我们把每个圆圈都当作可以存放数值的一个神经元，上一层神经元的数值会通过连线影响到下一层的数值。大概就像下面的这张图一样。通常我们把不是输入输出层的神经元称作<strong>隐含层</strong>。而<strong>输入层</strong>的值由输入向量完全决定，所以该层是所有层中最特殊的一层。我们把隐含层和输出层的神经元称作<strong>功能神经元</strong>。</p><p><img src="./mlp.png" alt="多层感知机" title="多层感知机"></p><p>我们参考公式\eqref{eq1}，更换符号和角标使得更加符合多层感知机的应用情况。</p><script type="math/tex; mode=display">\begin{equation}                      x_{k}^{(L)} = f(\sum_{n=0}^{N^{(L-1)}} w_{kn}^{(L-1)} x_n^{(L-1)} + \theta_k^{(L)})                      \end{equation}</script><p>由此我们可以写出从第$L-1$层的各神经元的值计算第$L$层各神经元的值的公式。</p><script type="math/tex; mode=display">\begin{equation}\left[ \begin{matrix}        x_0^{(L)} \\        x_1^{(L)} \\        \vdots \\        x_k^{(L)}        \end{matrix}\right] = f \left(                \left[ \begin{matrix}                      w_{0,0}^{(L-1)} & w_{0,1}^{(L-1)} & \cdots & w_{0,n}^{(L-1)}\\                      w_{1,0}^{(L-1)} & w_{1,1}^{(L-1)} & \cdots & w_{1,n}^{(L-1)}\\                      \vdots & \vdots & \ddots & \vdots \\                      w_{k,0}^{(L-1)} & w_{k,1}^{(L-1)} & \cdots & w_{k,n}^{(L-1)}\\                      \end{matrix}                \right]                \left[ \begin{matrix}                      x_0^{(L-1)} \\                      x_1^{(L-1)} \\                      \vdots \\                      x_n^{(L-1)}                        \end{matrix}                \right]                +                \left[ \begin{matrix}                      \theta_0^{(L)} \\                      \theta_1^{(L)} \\                      \vdots \\                      \theta_k^{(L)}                      \end{matrix}                \right]            \right)    \label{eq2}\end{equation}</script><p>简化后就是:</p><script type="math/tex; mode=display">                    \begin{equation}                      \boldsymbol{x}^{(L)} = \boldsymbol{f} \left(                      \mathbf{W}^{(L-1)} \boldsymbol{x}^{(L-1)} + \boldsymbol{\theta}^{(L)}                      \right)                      \end{equation}</script><p>这个公式看起来就要美观不少。此外，该形式也比较容易使用<strong>numpy</strong>这种支持矩阵运算的插件来实现。</p><h1 id="应用环境"><a href="#应用环境" class="headerlink" title="应用环境"></a>应用环境</h1><p>我们通过一些简单的例子，简单阐述一下这种神经网络结构可以用来做什么样的事情，以及网络复杂度与所解决问题的复杂度的相关性。</p><h2 id="简单的逻辑运算（与或非）"><a href="#简单的逻辑运算（与或非）" class="headerlink" title="简单的逻辑运算（与或非）"></a>简单的逻辑运算（与或非）</h2><p>先看这个非常简单的例子。我们把这样的 “最小神经网络” 称作感知机：只有两个输入神经元和一个功能神经元。我们可以发现这样的感知机就可以搭建与或非门了。通过设定权值$w$和偏置$\theta$就可以调整输出结果。（有了与或非是不是就可以用来搭计算机了x）</p><p><img src="./sp.png" alt="感知机的简单逻辑实现" title="感知机的简单逻辑实现"></p><h2 id="较复杂的逻辑运算（异或）"><a href="#较复杂的逻辑运算（异或）" class="headerlink" title="较复杂的逻辑运算（异或）"></a>较复杂的逻辑运算（异或）</h2><p>但在工程中，异或门使用的情况要远多于与或非门的使用情况。但实际上，异或问题无法通过上图的感知机模型实现。原因是简单的感知机在解决分类问题时，只能在二维平面画一条直线。而异或问题显然不能在二维平面内用一条线分开。</p><p><img src="./classify.png" alt="四种问题的空间分布情况" title="四种问题的空间分布情况"></p><p>看起来异或问题已经超出了简单的感知机的能力。那如何用神经网络实现异或问题呢？emm，再加一层。</p><p><img src="./xorq.png" alt="异或问题的双层感知机" title="异或问题的双层感知机"></p><p>由此得知，通常来讲，问题越复杂，必要的神经元的个数和层数越多。</p><h2 id="再复杂一点的问题（简单的二分类问题）"><a href="#再复杂一点的问题（简单的二分类问题）" class="headerlink" title="再复杂一点的问题（简单的二分类问题）"></a>再复杂一点的问题（简单的二分类问题）</h2><p><img src="./2class.png" alt="简单的二分类问题" title="简单的二分类问题"></p><p>上面这个网络用来解决一个简单的二分类问题。即判断五个0~1的数字之和大于2.5还是小于2.5。（等于2.5？概率为零怕什么（不是））由于2.5这个结果分在任意一组均可，在之后描述该问题时，暂忽略该值。上面这个网络虽然可能不是最精简的网络，但可以完成这个分类问题，最简规模也应比异或问题要大很多。</p><h2 id="更复杂一点的问题（MNIST数据集识别）"><a href="#更复杂一点的问题（MNIST数据集识别）" class="headerlink" title="更复杂一点的问题（MNIST数据集识别）"></a>更复杂一点的问题（MNIST数据集识别）</h2><p>这个就是很出名的有关于手写数字的数据集了。虽然目前有更好的解决办法，但这种普通的神经网络也可以解决这个问题。这个数据集中每一个数字都是由28*28像素构成的。图中这个网络结构由3Blue1Brown提出，不是最简单的方案但也可以完成数字分类的工作。但显然，这个更复杂的问题的解决方案，也需要更复杂的模型才可以解决。</p><blockquote><p>MNIST数据集：<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></p></blockquote><p><img src="./mnist.png" alt="MNIST分类问题" title="MNIST分类问题"></p><h1 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h1><p>这部分会详细解释一下神经网络解决分类问题的基本流程，以及神经网络优化问题的数学模型以及解法。这部分会有大量的公式，但基本原理来自于高等数学<strong>导数的链式法则</strong>。</p><h2 id="神经网络优化问题的基本流程"><a href="#神经网络优化问题的基本流程" class="headerlink" title="神经网络优化问题的基本流程"></a>神经网络优化问题的基本流程</h2><p>感知机虽然只有一个功能神经元，不过也算是某种意义上的神经网络。先以感知机实现的<strong>与问题</strong>为例介绍下神经网络的优化问题是什么样子的。</p><p><img src="./PER.png" alt="感知机" title="感知机"></p><p>我们把感知机的模型再放在这里便于查看。然后定义模型:</p><script type="math/tex; mode=display">\begin{equation}                      x_1 \wedge x_2 \quad y = f \left( w_0 x_0 + w_1 x_1 + \theta \right) \quad f(x) = \mathrm{sgn}(x)                      \end{equation}</script><p>我们的将按照下面的步骤完成这个优化问题：</p><ol><li>初始化$w_0$、$w_1$、$\theta$</li><li>输入一组训练数据$x_0$、$x_1$得到输出$\hat{y}$</li><li>根据该训练数据的标签计算，得到误差$E(y, \hat{y})$</li><li>根据误差$E$调整$w_0$、$w_1$、$\theta$</li></ol><h3 id="1-初始化-w-0-、-w-1-、-theta"><a href="#1-初始化-w-0-、-w-1-、-theta" class="headerlink" title="1.初始化$w_0$、$w_1$、$\theta$"></a>1.初始化$w_0$、$w_1$、$\theta$</h3><p>虽然说初始化对最终收敛得到的结果应该会有些影响，不过通常我们使用随机数据进行初始化。</p><script type="math/tex; mode=display">w_0 = 0.3898 \quad w_1 = 0.4950 \quad \theta = 0.8115</script><h3 id="2-输入一组训练数据-x-0-、-x-1-得到输出-hat-y"><a href="#2-输入一组训练数据-x-0-、-x-1-得到输出-hat-y" class="headerlink" title="2.输入一组训练数据$x_0$、$x_1$得到输出$\hat{y}$"></a>2.输入一组训练数据$x_0$、$x_1$得到输出$\hat{y}$</h3><p>我们先选一组测试数据，比如：</p><script type="math/tex; mode=display">(x_0, x_1), y = (1, 0), 0</script><p>然后根据公式\eqref{eq1}计算得到估计值$\hat{y}$。</p><script type="math/tex; mode=display">\begin{aligned}                                    \hat{y} & = \mathrm{sgn} (0.3898 x_0 + 0.4950 x_1 + 0.8115) \\                                    \hat{y} & = \mathrm{sgn} (0.3898 \times 1 + 0.4950 \times 0 + 0.8115) \\                                    & = \mathrm{sgn} (1.2013) \\                                    & = 1                                    \end{aligned}</script><h3 id="3-根据该训练数据的标签计算，得到误差-E-y-hat-y"><a href="#3-根据该训练数据的标签计算，得到误差-E-y-hat-y" class="headerlink" title="3.根据该训练数据的标签计算，得到误差$E(y, \hat{y})$"></a>3.根据该训练数据的标签计算，得到误差$E(y, \hat{y})$</h3><p>接下来我们评价一下这个网络的输出结果。emm，显然与预期结果不符。这种不符合的程度，我们通过定义误差函数计算出实际结果与预期结果的差距。</p><script type="math/tex; mode=display"> E(y, \hat{y}) = y - \hat{y} = 0 - 1 = -1</script><h3 id="4-根据误差-E-调整-w-0-、-w-1-、-theta"><a href="#4-根据误差-E-调整-w-0-、-w-1-、-theta" class="headerlink" title="4.根据误差$E$调整$w_0$、$w_1$、$\theta$"></a>4.根据误差$E$调整$w_0$、$w_1$、$\theta$</h3><p>之后，我们根据这个误差，通过一定的方法更新我们的三个参数$w_0$、$w_1$、$\theta$。通常，我们会控制一次学习改变的程度，从而使系统慢慢接近想要的结果。这里我们使用下面这种方法进行调整。</p><script type="math/tex; mode=display">w_i \leftarrow w_i + \eta E x_i \quad \eta                                    \in (0, 1)</script><p>我们把这里的$\eta$称作学习率。因此我们按照这个公式完成对数据的调整。（关于$\theta$的调整方式我们在后文详述。）</p><script type="math/tex; mode=display">\begin{aligned}    w_0 & \leftarrow 0.3898 + 0.1 \times (-1) \times 1 & = 0.2898 \\    w_1 & \leftarrow 0.4950 + 0.1 \times (-1) \times 0 & = 0.4950 \\    \theta & \leftarrow 0.8115 + 0.1 \times (-1) & = 0.7115\end{aligned}</script><p>调整之后，我们就可以再返回步骤2，导入新的训练数据，对各个变量进行调整。下面这张图是笔者按照这种方式训练出来的结果，看起来还是很完美的实现了与门的功能。</p><script type="math/tex; mode=display">y = \mathrm{sgn} ( 0.2898 x_0 + 0.1950 x_1 -                                    0.3885)</script><p><img src="./andrst.png" alt="与门的训练结果1" title="与门的训练结果1"><br><img src="./andrst2.png" alt="与门的训练结果2" title="与门的训练结果2"></p><h2 id="神经网络优化问题的数学建模与算法"><a href="#神经网络优化问题的数学建模与算法" class="headerlink" title="神经网络优化问题的数学建模与算法"></a>神经网络优化问题的数学建模与算法</h2><p>介绍完基本的流程，接下来的部分会详细的的介绍神经网络优化问题的数学模型，并如何对这个模型进行求解。当然，毕竟这是一个非常复杂的问题，可能无法准确的找到最优解，不过对于工业需求而言，总能在不断尝试中获得一个符合要求的结果。</p><p>我们以前面提到的简单的二分类问题为例，来进行下面的推导。首先，我们把前面这个问题的网络结构放在这里。</p><p><img src="./2class.png" alt="简单的二分类问题" title="简单的二分类问题"></p><p>我们希望在所有的训练例中，输出结果与预期结果尽可能的一致，也就是误差尽可能的小。所以这个问题的目标函数为：</p><script type="math/tex; mode=display">\begin{equation}\min_{w, \theta}{E}\label{eq:3}\end{equation}</script><p>当然，定义$E$的方法各有不同，这里我们使用最常见的均方误差，所以公式\eqref{eq:3}改写成：</p><script type="math/tex; mode=display">\begin{equation}\min_{w, \theta}{E} = \frac{1}{N} \sum_{m=0}^{N - 1}{ \left[ \frac{1}{2} \sum_{k}{\left( \hat{y}_k - y_k \right)^2 } \right]_m }\end{equation}</script><p>这里的$m$指的是某一个训练例，$N$表示训练例的个数。</p><p>那接下来的问题就是，如何调整$w$，$\theta$，使误差$E$最小。看起来这是一个<strong>无约束最小值问题</strong>。因此，这里可以求助于数学模型的相关知识，使用<strong>梯度下降法</strong>。</p><blockquote><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度下降法是一个比较常见的用来解决无约束最小值问题的算法，<strong>但该算法无法保证所得结果为全局最小值</strong>。我们用下面这个简单的二次函数图像来说明梯度下降法。<br><img src="./gd1.png" alt="某个二次函数" title="某个二次函数"><br>图中的曲线上的颜色表示该点出梯度的绝对值。梯度用来表征该点数值变化的最大值以及方向。因此利用梯度的这个性质，我们每次以梯度为参考，不断的向变化最大的方向<strong>下降</strong>，梯度的绝对值越大，下降的速度越大。当然这些都是比较定性的描述，下面我们用公式来描述这个算法。</p><p>首先确定目标函数:</p><script type="math/tex; mode=display">\min_{x}{y} = x^2</script><p>随机一个初始值，然后按照下面的策略不断更新参数$x$，使目标函数$y$达到最小。</p><script type="math/tex; mode=display">x \leftarrow x + \Delta x \quad \Delta x =  - \eta \nabla y = - \eta 2 x</script><div class="table-container"><table><thead><tr><th>序号</th><th>$x$</th><th>$y$</th><th>$\delta x$  </th></tr></thead><tbody><tr><td>1</td><td>$-3$</td><td>$9$</td><td>$0.6$</td></tr><tr><td>2</td><td>$-2.4$</td><td>$5.76$</td><td>$0.48$ </td></tr><tr><td>3</td><td>$-1.92$</td><td>$3.686$</td><td>$0.384$</td></tr><tr><td>4</td><td>$-1.536$</td><td>$2.359$</td><td>$0.3072$</td></tr><tr><td>5</td><td>$-1.229$</td><td>$1.510$</td><td>$0.2458$</td></tr><tr><td>6</td><td>$-0.983$</td><td>$0.966$</td><td>$0.1966$</td></tr><tr><td>7</td><td>$-0.786$</td><td>$0.618$</td><td>$0.1258$</td></tr><tr><td>8</td><td>$-0.629$</td><td>$0.396$</td><td>$0.1006$</td></tr><tr><td>9</td><td>$\cdots$</td><td>$\cdots$</td><td>$\cdots$</td></tr></tbody></table></div><p>根据表格内容发现，仅用了8次迭代就将结果的误差下降到$0.4$左右，可见这个算法还是有一定的效果的。但是，前文我们提到<strong>该算法无法保证所得结果为全局最小值</strong>，我们以下面的例子说明这一点。</p><p><img src="./gd2.png" alt="某个四次函数" title="某个四次函数"></p><script type="math/tex; mode=display">y = 0.25x^4 -0.1 x^3 - 3 x^2 - x + 0.5</script><p>我们仍以$x = -3$作为初始值，迭代100次，结果如下：</p><script type="math/tex; mode=display">x = -4.728182381361199 \quad y = -2.3879650505567143</script><p>显然，结果落入了局部最小值，而不是全局最小值。因此也提出了很多<strong>改善</strong>这一问题的方法，例如<em>模拟退火算法</em>等等，但本质上无法解决落入局部极小的问题。不过在工业上，局部极小的结果往往也足以满足需求，因此目前的优化算法都大体基于梯度下降法。</p></blockquote><p>由此，我们根据梯度下降算法，完成对神经网络里面各种参数的优化。反正求梯度就是求偏导嘛。首先我们列出各个参数与误差的函数关系。</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}    E & = \frac{1}{2} \sum_{k}(\hat{x}^{(L)}_k - x^{(L)}_k)^2 \\    \hat{x}^{(L)}_k & = \sigma(z^{(L)}_k) \\    z^{(L)}_k & = w_{kn} x^{(L-1)}_n + \theta^{(L)}_k\end{aligned}\end{equation}</script><p>然后开始求导。</p><script type="math/tex; mode=display">                      \begin{equation}                      \begin{aligned}                      \frac{\partial E}{\partial w_{kn}} &= \frac{\partial E}{\partial x^{(L)}_k} \cdot \frac{\partial                      x^{(L)}_k }{\partial z^{(L)}_k} \cdot \frac{\partial z^{(L)}_k}{\partial w_{kn}} \\                      \frac{\partial E}{\partial \theta^{(L)}_{k}} &= \frac{\partial E}{\partial x^{(L)}_k} \cdot                      \frac{\partial x^{(L)}_k }{\partial z^{(L)}_k} \cdot \frac{\partial z^{(L)}_k}{\partial                      \theta^{(L)}_{kn}} \\                      \frac{\partial E}{\partial x^{(L-1)}_{n}} &= \sum_{k}{\frac{\partial E}{\partial x^{(L)}_k} \cdot                      \frac{\partial x^{(L)}_k }{\partial z^{(L)}_k} \cdot \frac{\partial z^{(L)}_k}{\partial                      x^{(L-1)}_{n}} }                      \end{aligned}                      \end{equation}</script><p>观察上面的三个公式，发现其实就是普通的导数链式法则。但之所以拆解成这样的形式，是因为这样的形式可以被很方便的计算出来。</p><script type="math/tex; mode=display">                      \begin{equation}                      \begin{aligned}                      \frac{\partial E}{\partial w_{kn}} & = \frac{\partial E}{\partial x^{(L)}_k} \cdot \frac{\partial                      x^{(L)}_k }{\partial z^{(L)}_k} \cdot \frac{\partial z^{(L)}_k}{\partial w_{kn}} & =                      \frac{\partial E}{\partial x^{(L)}_k} \cdot \sigma'(z^{(L)}_k) \cdot x^{(L-1)}_{n} \\                      \frac{\partial E}{\partial \theta^{(L)}_{k}} & = \frac{\partial E}{\partial x^{(L)}_k} \cdot                      \frac{\partial x^{(L)}_k }{\partial z^{(L)}_k} \cdot \frac{\partial z^{(L)}_k}{\partial                      \theta^{(L)}_{kn}} & = \frac{\partial E}{\partial x^{(L)}_k} \cdot \sigma'(z^{(L)}_k) \cdot 1 \\                      \frac{\partial E}{\partial x^{(L-1)}_{n}} & = \sum_{k} \frac{\partial E}{\partial x^{(L)}_k} \cdot                      \frac{\partial x^{(L)}_k }{\partial z^{(L)}_k} \cdot \frac{\partial z^{(L)}_k}{\partial                      x^{(L-1)}_{n}} & = \sum_{k} \frac{\partial E}{\partial x^{(L)}_k} \cdot \sigma'(z^{(L)}_k) \cdot                      w_{kn} \\                      \end{aligned}                      \label{eq:4}                      \end{equation}</script><p>最后一步，将公式\eqref{eq:4}中的第三个公式代入第一个公式，就完成了第$L-1$与$L-2$层的误差传递。从而修改每一层的每一个参数，大功告成。</p><p>笔者根据以上的算法，完成了简单的二分类问题。基于Python的源码可以参见这个链接。</p><blockquote><p><a href="https://github.com/Tackoil/mlp_example">https://github.com/Tackoil/mlp_example</a> </p></blockquote><p><img src="./eb.png" alt="简单二分类问题的实验结果" title="简单二分类问题的实验结果"></p><p>不过，这些算法最后都被TensorFlow或者pyTorch等等模块内置了，使用的时候只需要一句话就完成所有参数的调整，还是很方便的。</p><h1 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h1><p>可喜可贺，最复杂的部分已经介绍完了，后面是一些现今针对这个简单的前馈神经网络的问题进行调整，得到一些新的性能更好的网络。</p><h2 id="多层前馈神经网络的不足"><a href="#多层前馈神经网络的不足" class="headerlink" title="多层前馈神经网络的不足"></a>多层前馈神经网络的不足</h2><p>虽然这个模型可以解决很多很多复杂的无法提取特征的分类问题，但在应用过程中，发现还是有很多不足，也提出了很多针对某一个问题对该模型进行改进。</p><h3 id="运算量大"><a href="#运算量大" class="headerlink" title="运算量大"></a>运算量大</h3><p>以我们刚刚得到的二分类问题这个模型为例，发现该模型一共使用了$92$个变量。需要计算$78$次浮点数乘法。这远比计算$4$次浮点数加法要复杂得多。再加上上万的训练数据，无疑是一个巨大的负担。</p><p>针对这一问题，人们提出了很多模型简化的方案，提高系统的效率。目前最常见的方案称作<strong>权值共享</strong>。当然不能随意选择哪些部分权值相等，通常人们将具有一定相关性的节点选择相同的权值。这也是<strong>卷积神经网络</strong>的一个出发点。</p><h3 id="输入节点固定"><a href="#输入节点固定" class="headerlink" title="输入节点固定"></a>输入节点固定</h3><p>可以发现，这个系统输入点数是固定的。训练好的系统无法对与输入长度不符的数据进行分类。另外，即使在末位补零与输入长度匹配效果也不是很理想。于是提出<strong>循环神经网络</strong>，将时间参数引入神经元模型，这样就可以输入不定长的数据，解决序列相关的问题。</p><h3 id="梯度弥散与梯度爆炸"><a href="#梯度弥散与梯度爆炸" class="headerlink" title="梯度弥散与梯度爆炸"></a>梯度弥散与梯度爆炸</h3><p>首先，我们简单计算一下各层对权重的偏导$\frac{\partial E}{\partial w}$</p><script type="math/tex; mode=display">                      \begin{equation}                      \begin{aligned}                      \frac{\partial E}{\partial w_{kn}} &= \frac{\partial E}{\partial x_k^{(L)}} \cdot \sigma'                      (z_k^{(L)}) \cdot x_n^{(L-1)} \\                      \frac{\partial E}{\partial w_{nj}} &= \sum_k \left( \frac{\partial E}{\partial x_k^{(L)}} \cdot                      \sigma' (z_k^{(L)}) \cdot w_{kn} \right) \cdot \sigma' (z_n^{(L-1)}) \cdot x_j^{(L-2)} \\                      \frac{\partial E}{\partial w_{ji}} &= \sum_n \left( \sum_k \left( \frac{\partial E}{\partial                      x_k^{(L)}} \cdot \sigma' (z_k^{(L)}) \cdot w_{kn} \right) \cdot \sigma' (z_n^{(L-1)}) \cdot w_{nj}                      \right) \cdot \sigma' (z_j^{(L-2)}) \cdot x_i^{(L-3)} \\                      \end{aligned}                      \end{equation}</script><p>可以发现，这个偏导数里面有激活函数导数的三次幂。可见当层数越多时，激活函数的导数对系统的影响越大。那我们来看一下$\mathrm{Sigmoid}(x)$的导数，可见其函数值最大值在$0.25$左右。随着系统层数的增加，梯度值会越来越小，导致首层的神经元基本无法得到优化。我们把这种现象称作<strong>梯度弥散</strong>。相对的，导数值基本大于$1$，从而使首层收到的梯度值异常的大，把这种称作<strong>梯度爆炸</strong>。</p><p><img src="./dv.png" alt="Sigmoid(x)的导数" title="Sigmoid(x)的导数"></p><h2 id="卷积神经网络与模型压缩"><a href="#卷积神经网络与模型压缩" class="headerlink" title="卷积神经网络与模型压缩"></a>卷积神经网络与模型压缩</h2><p>卷积神经网络主要使用了权值共享从而大步幅降低运算量。当然，也充分利用了图像数据的相关性，使相邻的一部分像素共享相同的权值。为了使权值共享带来的负效应尽可能的小，这个网络也对神经网络的运算做了适当的修改，但也基本符合之前的结论。该模型被广泛的使用在计算机视觉领域。（多半也是因为视觉相关的图像数据内相关性很高）</p><p>卷积神经网络主要有两种比较特殊的层。通常我们把他们称作<strong>卷积层</strong>与<strong>池化层</strong>。</p><p>卷积层所做的如下图所示，对原始数据进行窗口滑动，与权值矩阵对应位置相乘再相加，得到一个数据，作为输出结果。下面这个图比较生动的说明了卷积层的功能。也推荐这个网页关于卷积层的动态展示。（不过这哪里卷积了x）</p><blockquote><p><a href="http://cs231n.github.io/assets/conv-demo/index.html">http://cs231n.github.io/assets/conv-demo/index.html</a> </p></blockquote><p><img src="./conv.gif" alt="卷积层" title="卷积层"></p><p><img src="./pool.png" alt="池化层" title="池化层"></p><p>为了简化说明，我们把卷积层与池化层作为一个神经元。以这个著名的手写数字识别的模型为例，发现其功能神经元的第一层只使用了6个神经元，第二层使用了10个神经元。每两个神经元的连接由原来的乘法变成了卷积+池化。从而大幅度的减少系统的运算量。</p><p><img src="./cnn.png" alt="卷积神经网络与另一种表示方式" title="卷积神经网络与另一种表示方式"></p><p>但为了满足移动设备的需要，当前的模型仍然过于复杂。Google的相关团队 与 旷视的Face++团队分别提出了MobileNets和ShuffleNet进行进一步的模型压缩。</p><blockquote><p>MobileNets：<a href="https://arxiv.org/abs/1704.04861v1">https://arxiv.org/abs/1704.04861v1</a><br>ShuffleNet：<a href="https://arxiv.org/abs/1707.01083v2">https://arxiv.org/abs/1707.01083v2</a></p></blockquote><h2 id="循环神经网络与长短期记忆网络"><a href="#循环神经网络与长短期记忆网络" class="headerlink" title="循环神经网络与长短期记忆网络"></a>循环神经网络与长短期记忆网络</h2><p>为了解决输入数据无法改变长度问题，于是引入循环神经网络。一般来说，对于输入到循环神经网络的一个元素而言，其输入到输出只有三层。但网络隐含层的状态会随着循环次数不断变化。这样就被称作循环神经网络了。该网络经常使用在语音处理，自然语音处理等等。</p><p><img src="./rnn.png" alt="循环神经网络" title="循环神经网络"></p><p>不过，这个模型的梯度弥散问题非常严重。于是进一步引入每个神经元的遗忘系统，得到长短期记忆网络。因为变量增多，系统也看着更加复杂起来。</p><p><img src="./rnnlstm.png" alt="循环神经网络与长短期记忆网络" title="循环神经网络与长短期记忆网络"></p><h2 id="残差神经网络"><a href="#残差神经网络" class="headerlink" title="残差神经网络"></a>残差神经网络</h2><p>为了解决梯度弥散与梯度爆炸的问题，这个模型只做了一点点简单的修改，却得到了出乎意料的效果。详情可以参考这篇提出残差神经网络的论文。</p><blockquote><p>ResNet: <a href="https://arxiv.org/abs/1512.03385v1">https://arxiv.org/abs/1512.03385v1l</a> </p></blockquote><p><img src="./resnet.png" alt="残差神经网络" title="残差神经网络"></p><h1 id="Special-Thanks"><a href="#Special-Thanks" class="headerlink" title="Special Thanks"></a>Special Thanks</h1><p>在此感谢为这次调研提供帮助的各位。也感谢オレンジ提供的帮助与校对~~~</p><ul><li>Johntheuser</li><li>オレンジ</li><li>Tab</li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li>Kohonen, Teuvo . An introduction to neural computing. Neural Networks 1.1(1988):3-16</li><li>But what <em>is</em> a Neural Network? | Deep learning, Part 1 3Blue1Brown <a href="https://youtu.be/aircAruvnKk">https://youtu.be/aircAruvnKk</a> </li><li>THE MNIST DATABASE of handwritten digits Yann LeCun, Corinna Cortes, Christopher J.C. Burges <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> </li><li>使用Python将MNIST数据集转化为图片 name_s_Jimmy <a href="https://blog.csdn.net/qq_32166627/article/details/52640730">https://blog.csdn.net/qq_32166627/article/details/52640730</a></li><li>Lécun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324. Gradient-based</li><li>刘建平Pinard, 卷积神经网络(CNN)模型结构 <a href="https://www.cnblogs.com/pinard/p/6483207.html">https://www.cnblogs.com/pinard/p/6483207.html</a></li><li>蒋竺波, CNN入门讲解：什么是采样层（pooling） <a href="https://zhuanlan.zhihu.com/p/32299939">https://zhuanlan.zhihu.com/p/32299939</a></li><li>刘建平Pinard,循环神经网络(RNN)模型与前向反向传播算法 <a href="https://www.cnblogs.com/pinard/p/6509630.html">https://www.cnblogs.com/pinard/p/6509630.html</a></li><li>刘建平Pinard, LSTM模型与前向反向传播算法 <a href="https://www.cnblogs.com/pinard/p/6519110.html">https://www.cnblogs.com/pinard/p/6519110.html</a></li><li>Howard, A. G. , Zhu, M. , Chen, B. , Kalenichenko, D. , Wang, W. , &amp; Weyand, T. , et al.  (2017). Mobilenets: efficient convolutional neural networks for mobile vision applications.</li><li>Zhang, X. , Zhou, X. , Lin, M. , &amp; Sun, J. . (2017). Shufflenet: an extremely efficient convolutional neural network for mobile devices.</li><li>He, K. , Zhang, X. , Ren, S. , &amp; Sun, J. . (2015). Deep residual learning for image recognition.</li><li>周志华. (2016). 机器学习. 清华大学出版社.</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>

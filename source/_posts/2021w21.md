---
title: 2021w21-机器翻译的自动评估
date: 2021-06-02 19:45:02
categories: [学习, 周报]
---

***

> （把每双周周报在BLOG上整理一下，以防止自己做的PPT最后都想不起来是在说什么。）

&ensp;&ensp;&ensp;&ensp;这次周报的题目非常的别扭，可能是我翻译的不太好。英文的话应该是 Automatic Evaluation of Machine Translation ，也就是对机器翻译的结果给出一个自动的评价或者评分，用来对比哪一个机器翻译模型的性能更好。当然，自动也是题目中非常重要的一部分。基于人工的评价自然无法应付现在大规模的评估需求。

<!-- more -->

## 机器翻译及其评估

&ensp;&ensp;&ensp;&ensp;机器翻译是指实现一种语言到另一种语言的自动翻译。[^1] 该系统的输入是语言 A 的一个文本段落。输出是语言 B 的段落。这两个段落都是不定长的文本序列。对于同一句话可能存在多种合理的翻译，也可能存在明显不合理的翻译。例如下图中的最后一个翻译，把 docter 错翻译成博士，显然是不符合原义的。

> ~~（阿米娅：博士，您还有许多事情需要处理。现在还不能休息哦。）~~

![翻译例子](./traneg.png)

[^1]: 宗成庆. 统计自然语言处理[M]. 清华大学出版社, 2013.

&ensp;&ensp;&ensp;&ensp;那么在这么多翻译中，如何评价它们，得出一个更好的翻译呢？这就需要对翻译进行评估，或者称评价。对于如何做好翻译，有大家耳熟能详的严复提出的提出译事三大难：**信**、**达**、**雅**。在 Hovy 等人的论文[^2]中也提出了类似的观点：好的翻译要追求**充分性**（adequacy）、**准确性**（fidelity）和**流畅性**（fluency）。例如在上面的句子中，除了我的翻译无法做到正确之外，有道的翻译则更加流畅，可以认为在这句话上的翻译更好。

[^2]: Hovy E H . Toward Finely Differentiated Evaluation Metrics for Machine Translation[J]. proceedings of the eagles workshop on standards & evaluation.pisa italy.international standards for language engineering, 1999.

&ensp;&ensp;&ensp;&ensp;上述需要人类参与、使用人类对语言的理解的评估可以成为**人工评估**，相对的如果可以计算地得出评估结果，这种可以称为**自动评估**。显然，人工评估不仅费时费力，而且为了弥补人类的不客观性，需要采纳大量人类的结果，显然无法为现今大规模的翻译模型测试提供帮助。由此对自动评估的期望越来越高。对于一个自动评估方案来讲，其通常需要引入由人类提供的**正确答案**（Ground Truth / Reference）。对模型输出的**候选结果**（Candidate）与正确答案进行可重复性的计算，为每个候选结果提供一个表示其翻译质量的分数。自动评估不仅成本低、速度快，而且可以重复，为翻译模型间相互比较提供了可能。

## 传统自动评估方法

### BLEU（The Bilingual Evaluation Understudy）

&ensp;&ensp;&ensp;&ensp;BLEU[^3] 是比较经典的自动评估方法。这个词和蓝色（BLUE）比较像，但看在原论文特意把这四个字母写成蓝色，我觉得读音应该一致。此外经过搜索，BLEU在法语里就是蓝色的意思。

![bleu的翻译](./bleu_tran.png)

[^3]: BLEU

&ensp;&ensp;&ensp;&ensp;BLEU主要分为两部分：n元准确度修改版（Modified $n$-gram precision）、简短惩罚系数（Brevity Penalty）
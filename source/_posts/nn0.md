---
title: 人工神经网络的基本知识
date: 2019-09-12 16:03:42
categories: [机器学习]
mathjax: true
---

这个原本是通信网理论与应用这门课的一个调研型的大作业。（不过这个和通信网有什么关系）就当是笔者从零开始学习深度学习吧。

本篇BLOG含有大量的公式，用来阐明最基本的神经网络在优化问题的基本算法。此外根据查到的各种资料，本篇也会简单说明基于神经网络而发展的典型网络结构以及使用这些网络结构的领域。虽说含有大量的公式，不过用到的原理也只是最基本的高等数学。为了比较美观的展现公式和算法，还会使用一些基本的线性代数运算。

不过，毕竟初来乍到，可能有很多漏洞和疏忽之处，恳请理解指正。

<!-- more -->

<p id="symbol"></p>

## 符号声明

符号   | 作用 
----- | ---  
$x_k^{(L)}$ | 第$L$层的第$k$个神经元的值
$w_{kn}^{(L)}$ | 第$L$层的第$k$个神经元与第$L-1$层的第$n$个神经元的连接权重    
$\theta_k^{(L)}$ | 第$L$层的第$k$个神经元的输入偏置
$z_k^{(L)}$ | 第$L$层的第$k$个神经元通过激活函数之前的值
$E$ | 误差
$\hat{x}_k^{(L)}$ | 第$L$层的第$k$个神经元的估计值（通常指最后一层）
$N^{(L)}$ | 第$L$层的神经元的个数

## 参考与推荐阅读

本篇大部分内容参考自周志华的《机器学习》（西瓜书）和3Blue1Brown关于神经网络的视频。私心推荐一下3Blue1Brown这位作者的视频。

3Blue1Brown: [YouTube](https://youtu.be/aircAruvnKk) [Bilibili](https://space.bilibili.com/88461692)

## 基本概念

### 神经网络

> 神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。

上述概念来源于周志华在《机器学习》这本书中的对原论文的翻译。神经网络是由多个进行特定运算的单元组成的网络。

### m-p神经元模型

上文提到论文中的神经元模型是含有**时间**这个参数的。在下面这张图中，忽略时间的影响得到神经元的结构。

![M-P 神经元模型](./mp_model.jpg "M-P 神经元模型")

M-P神经元的数学模型为：

$$
\begin{equation}
    y = f(\sum_{k = 1}^{n} w_k x_k + \theta)
\label{eq1}
\end{equation}
$$

关于这张图上的模型，仍需几点特殊说明：

- 对于一个神经元而言，这里$x$表示输入，$y$表示输出。而在多级神经网络中，$y$将表示下一级神经元的一个输入。
- 为了便于理解，在这里我们将输入和输出人为规定成0和1之间的数。在实际应用中不存在这样的限制。

### 激活函数

激活函数的出发点是将各个神经元的值限制在0到1之间，于是有**单位阶跃函数**和**Sigmoid函数**。随着这个限制的消失，追求更高收敛速度和性能的激活函数得以出现。例如**ReLU函数**、**Softplus函数**。

单位阶跃函数：
$$
\begin{equation}
    H(x) = \begin{cases}
    1, & x \geq 0 \\\\
    0, & x < 0 \end{cases} 
\end{equation} 
$$

Sigmoid函数：
$$
 \begin{equation}
    \mathrm{Sigmoid}(x) = \frac{1}{1 + \mathrm{e}^{-x}}
\end{equation}
$$

ReLU函数：
$$
                      \begin{equation}
                      \mathrm{ReLU}(x) = \begin{cases}
                      x, & x > 0 \\\\
                      0, & x \leq 0
                      \end{cases}
                      \end{equation}
$$

Softplus函数：
$$
\begin{equation}
                      \mathrm{Softplus}(x) = \ln{(1 + \mathrm{e}^{x})}
                      \end{equation}
$$

![激活函数](./activfuc.jpg "激活函数")

### 多层前馈神经网络
这部分的公式会开始复杂起来，可以参考<a href="#symbol">符号声明</a>回顾各个符号的含义。

通常来说，一个基本的神经网络不会只有一层，而是由很多层构成的。我们把每个圆圈都当作可以存放数值的一个神经元，上一层神经元的数值会通过连线影响到下一层的数值。大概就像下面的这张图一样。通常我们把不是输入输出层的神经元称作**隐含层**。而**输入层**的值由输入向量完全决定，所以该层是所有层中最特殊的一层。我们把隐含层和输出层的神经元称作**功能神经元**。

![多层感知机](./mlp.png "多层感知机")

我们参考公式\eqref{eq1}，更换符号和角标使得更加符合多层感知机的应用情况。

$$
\begin{equation}
                      x_{k}^{(L)} = f(\sum_{n=0}^{N^{(L-1)}} w_{kn}^{(L-1)} x_n^{(L-1)} + \theta_k^{(L)})
                      \end{equation}
$$

由此我们可以写出从第$L-1$层的各神经元的值计算第$L$层各神经元的值的公式。

$$
\begin{equation}
\left[ \begin{matrix}
        x_0^{(L)} \\\\
        x_1^{(L)} \\\\
        \vdots \\\\
        x_k^{(L)}
        \end{matrix}
\right] = f \left(
                \left[ \begin{matrix}
                      w_{0,0}^{(L-1)} & w_{0,1}^{(L-1)} & \cdots & w_{0,n}^{(L-1)}\\\\
                      w_{1,0}^{(L-1)} & w_{1,1}^{(L-1)} & \cdots & w_{1,n}^{(L-1)}\\\\
                      \vdots & \vdots & \ddots & \vdots \\\\
                      w_{k,0}^{(L-1)} & w_{k,1}^{(L-1)} & \cdots & w_{k,n}^{(L-1)}\\\\
                      \end{matrix}
                \right]
                \left[ \begin{matrix}
                      x_0^{(L-1)} \\\\
                      x_1^{(L-1)} \\\\
                      \vdots \\\\
                      x_n^{(L-1)}
                        \end{matrix}
                \right]
                +
                \left[ \begin{matrix}
                      \theta_0^{(L)} \\\\
                      \theta_1^{(L)} \\\\
                      \vdots \\\\
                      \theta_k^{(L)}
                      \end{matrix}
                \right]
            \right)
    \label{eq2}
\end{equation}
$$

简化后就是:

$$
                    \begin{equation}
                      \boldsymbol{x}^{(L)} = \boldsymbol{f} \left(
                      \mathbf{W}^{(L-1)} \boldsymbol{x}^{(L-1)} + \boldsymbol{\theta}^{(L)}
                      \right)
                      \end{equation}
$$

这个公式看起来就要美观不少。此外，该形式也比较容易使用**numpy**这种支持矩阵运算的插件来实现。

## 应用环境

我们通过一些简单的例子，简单阐述一下这种神经网络结构可以用来做什么样的事情，以及网络复杂度与所解决问题的复杂度的相关性。

### 简单的逻辑运算（与或非）

先看这个非常简单的例子。我们把这样的 “最小神经网络” 称作感知机：只有两个输入神经元和一个功能神经元。我们可以发现这样的感知机就可以搭建与或非门了。通过设定权值$w$和偏置$\theta$就可以调整输出结果。（有了与或非是不是就可以用来搭计算机了x）

![感知机的简单逻辑实现](./sp.png "感知机的简单逻辑实现")

### 较复杂的逻辑运算（异或）

但在工程中，异或门使用的情况要远多于与或非门的使用情况。但实际上，异或问题无法通过上图的感知机模型实现。原因是简单的感知机在解决分类问题时，只能在二维平面画一条直线。而异或问题显然不能在二维平面内用一条线分开。

![四种问题的空间分布情况](./classify.png "四种问题的空间分布情况")

看起来异或问题已经超出了简单的感知机的能力。那如何用神经网络实现异或问题呢？emm，再加一层。

![异或问题的双层感知机](./xorq.png "异或问题的双层感知机")

由此得知，通常来讲，问题越复杂，必要的神经元的个数和层数越多。

### 再复杂一点的问题（简单的二分类问题）

![简单的二分类问题](./2class.png "简单的二分类问题")

上面这个网络用来解决一个简单的二分类问题。即判断五个0~1的数字之和大于2.5还是小于2.5。（等于2.5？概率为零怕什么（不是））由于2.5这个结果分在任意一组均可，在之后描述该问题时，暂忽略该值。上面这个网络虽然可能不是最精简的网络，但可以完成这个分类问题，最简规模也应比异或问题要大很多。

### 更复杂一点的问题（MNIST数据集识别）

这个就是很出名的有关于手写数字的数据集了。虽然目前有更好的解决办法，但这种普通的神经网络也可以解决这个问题。这个数据集中每一个数字都是由28*28像素构成的。图中这个网络结构由3Blue1Brown提出，不是最简单的方案但也可以完成数字分类的工作。但显然，这个更复杂的问题的解决方案，也需要更复杂的模型才可以解决。

MNIST数据集：[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)

![MNIST分类问题](./mnist.png "MNIST分类问题")

## 实现方法

这部分会详细解释一下神经网络解决分类问题的基本流程，以及神经网络优化问题的数学模型以及解法。这部分会有大量的公式，但基本原理来自于高等数学**导数的链式法则**。

### 神经网络优化问题的基本流程

感知机虽然只有一个功能神经元，不过也算是某种意义上的神经网络。先以感知机实现的**与问题**为例介绍下神经网络的优化问题是什么样子的。

![感知机](./PER.png "感知机")

我们把感知机的模型再放在这里便于查看。然后定义模型:

$$
\begin{equation}
                      x_1 \wedge x_2 \quad y = f \left( w_0 x_0 + w_1 x_1 + \theta \right) \quad f(x) = \mathrm{sgn}(x)
                      \end{equation}
$$

我们的将按照下面的步骤完成这个优化问题：

1. 初始化$w_0$、$w_1$、$\theta$
2. 输入一组训练数据$x_0$、$x_1$得到输出$\hat{y}$
3. 根据该训练数据的标签计算，得到误差$E(y, \hat{y})$
4. 根据误差$E$调整$w_0$、$w_1$、$\theta$

#### 1.初始化$w_0$、$w_1$、$\theta$

虽然说初始化对最终收敛得到的结果应该会有些影响，不过通常我们使用随机数据进行初始化。

$$w_0 = 0.3898 \quad w_1 = 0.4950 \quad \theta = 0.8115$$

#### 2.输入一组训练数据$x_0$、$x_1$得到输出$\hat{y}$

我们先选一组测试数据，比如：

$$(x_0, x_1), y = (1, 0), 0$$

然后根据公式\eqref{eq1}计算得到估计值$\hat{y}$。

$$
\begin{aligned}
                                    \hat{y} & = \mathrm{sgn} (0.3898 x_0 + 0.4950 x_1 + 0.8115) \\\\
                                    \hat{y} & = \mathrm{sgn} (0.3898 \times 1 + 0.4950 \times 0 + 0.8115) \\\\
                                    & = \mathrm{sgn} (1.2013) \\\\
                                    & = 1
                                    \end{aligned}
$$
